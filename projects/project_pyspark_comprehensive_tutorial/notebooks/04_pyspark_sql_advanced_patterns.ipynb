{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a9ec72e",
   "metadata": {},
   "source": [
    "# Module 4: SQL & DataFrame API Advanced Patterns\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "\n",
    "By the end of this module, you will master advanced PySpark patterns for complex analytics:\n",
    "\n",
    "### 🔍 **Window Functions & Analytics**\n",
    "- Row number, rank, and dense rank functions\n",
    "- Cumulative aggregations and moving averages\n",
    "- Lead/lag functions for time series analysis\n",
    "- Percentile and quartile calculations\n",
    "- Advanced windowing with custom frames\n",
    "\n",
    "### 📊 **Advanced SQL Patterns**\n",
    "- Common Table Expressions (CTEs) and subqueries\n",
    "- Complex JOIN patterns and optimization\n",
    "- CASE statements and conditional logic\n",
    "- Set operations (UNION, INTERSECT, EXCEPT)\n",
    "- SQL functions and user-defined functions (UDFs)\n",
    "\n",
    "### 🧮 **Statistical & Mathematical Operations**\n",
    "- Correlation and covariance analysis\n",
    "- Statistical distributions and sampling\n",
    "- Mathematical functions and calculations\n",
    "- Data profiling and quality assessment\n",
    "- Outlier detection and statistical tests\n",
    "\n",
    "### 🕐 **Time Series & Date Analysis**\n",
    "- Date arithmetic and calendar functions\n",
    "- Time-based aggregations and trends\n",
    "- Seasonal analysis and time windows\n",
    "- Event sequence analysis\n",
    "- Time zone handling and conversions\n",
    "\n",
    "### ⚡ **Performance & Optimization**\n",
    "- Query plan analysis and optimization\n",
    "- Broadcast joins and bucket joins\n",
    "- Partition pruning and predicate pushdown\n",
    "- Caching strategies for complex queries\n",
    "- Cost-based optimization understanding\n",
    "\n",
    "### 🏗️ **Advanced DataFrame Patterns**\n",
    "- Dynamic column operations\n",
    "- Complex data structures (arrays, maps, structs)\n",
    "- Data pivoting and unpivoting\n",
    "- Regular expressions and text processing\n",
    "- JSON and semi-structured data handling\n",
    "\n",
    "**Prerequisites:** Modules 1-3 (Foundation, I/O, Transformations)  \n",
    "**Estimated Time:** 2-3 hours  \n",
    "**Difficulty:** Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7a0509f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All advanced PySpark modules imported successfully!\n",
      "📊 PySpark SQL functions ready for advanced analytics\n",
      "🕐 Timestamp: 2025-08-25 21:13:21\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup and Imports\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, date, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Additional imports for advanced patterns\n",
    "from pyspark.sql.functions import (\n",
    "    row_number, rank, dense_rank, percent_rank,\n",
    "    lag, lead, first, last, nth_value,\n",
    "    cume_dist, ntile, stddev, stddev_pop, stddev_samp,\n",
    "    variance, var_pop, var_samp, corr, covar_pop, covar_samp,\n",
    "    regexp_extract, regexp_replace, split, explode, posexplode,\n",
    "    from_json, to_json, get_json_object, json_tuple,\n",
    "    monotonically_increasing_id, spark_partition_id\n",
    ")\n",
    "\n",
    "# Statistical functions\n",
    "from pyspark.sql.functions import (\n",
    "    percentile_approx, approx_count_distinct, \n",
    "    skewness, kurtosis, mode\n",
    ")\n",
    "\n",
    "print(\"✅ All advanced PySpark modules imported successfully!\")\n",
    "print(f\"📊 PySpark SQL functions ready for advanced analytics\")\n",
    "print(f\"🕐 Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99df0676",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/25 21:13:22 WARN Utils: Your hostname, Sanjeevas-iMac.local, resolves to a loopback address: 127.0.0.1; using 192.168.12.128 instead (on interface en1)\n",
      "25/08/25 21:13:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/25 21:13:22 WARN Utils: Your hostname, Sanjeevas-iMac.local, resolves to a loopback address: 127.0.0.1; using 192.168.12.128 instead (on interface en1)\n",
      "25/08/25 21:13:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/25 21:13:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/25 21:13:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/25 21:13:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/08/25 21:13:24 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/08/25 21:13:24 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/08/25 21:13:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/08/25 21:13:24 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/08/25 21:13:24 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Advanced Spark Session Created!\n",
      "📋 App Name: PySpark Module 4: Advanced SQL Patterns\n",
      "🎯 SQL Adaptive Query Execution: Enabled\n",
      "🧮 Cost-Based Optimization: Enabled\n",
      "🔄 Shuffle Partitions: 12\n",
      "🏠 Driver Memory: 4g\n",
      "⚡ Arrow Optimization: true\n",
      "\n",
      "🌐 Spark UI: http://192.168.12.128:4043\n",
      "📊 Ready for advanced SQL patterns and window functions!\n"
     ]
    }
   ],
   "source": [
    "# Create Spark Session with Advanced SQL Optimizations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Module 4: Advanced SQL Patterns\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.cbo.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.statistics.histogram.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set SQL shuffle partitions for optimal performance\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"12\")  # 2x CPU cores\n",
    "\n",
    "# Configure Spark context logging\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"🚀 Advanced Spark Session Created!\")\n",
    "print(f\"📋 App Name: {spark.sparkContext.appName}\")\n",
    "print(f\"🎯 SQL Adaptive Query Execution: Enabled\")\n",
    "print(f\"🧮 Cost-Based Optimization: Enabled\")\n",
    "print(f\"🔄 Shuffle Partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print(f\"🏠 Driver Memory: {spark.conf.get('spark.driver.memory')}\")\n",
    "print(f\"⚡ Arrow Optimization: {spark.conf.get('spark.sql.execution.arrow.pyspark.enabled')}\")\n",
    "\n",
    "# Display Spark UI URL\n",
    "print(f\"\\n🌐 Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "print(\"📊 Ready for advanced SQL patterns and window functions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f154d457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Sales Transaction Dataset Created:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   📈 Records: 20\n",
      "   📅 Date Range: Row(min(transaction_date)=datetime.date(2024, 1, 15), max(transaction_date)=datetime.date(2024, 8, 1))\n",
      "+--------------+-----------+-----------+------+----------------+-------+------+--------------+\n",
      "|transaction_id|customer_id|category   |amount|transaction_date|quarter|region|salesperson_id|\n",
      "+--------------+-----------+-----------+------+----------------+-------+------+--------------+\n",
      "|1             |101        |Electronics|1200.0|2024-01-15      |Q1     |West  |1             |\n",
      "|2             |102        |Books      |45.99 |2024-01-16      |Q1     |East  |1             |\n",
      "|3             |103        |Clothing   |89.99 |2024-01-18      |Q1     |North |2             |\n",
      "|4             |101        |Electronics|899.99|2024-02-10      |Q1     |West  |1             |\n",
      "|5             |104        |Home       |234.5 |2024-02-12      |Q1     |South |3             |\n",
      "+--------------+-----------+-----------+------+----------------+-------+------+--------------+\n",
      "only showing top 5 rows\n",
      "   📅 Date Range: Row(min(transaction_date)=datetime.date(2024, 1, 15), max(transaction_date)=datetime.date(2024, 8, 1))\n",
      "+--------------+-----------+-----------+------+----------------+-------+------+--------------+\n",
      "|transaction_id|customer_id|category   |amount|transaction_date|quarter|region|salesperson_id|\n",
      "+--------------+-----------+-----------+------+----------------+-------+------+--------------+\n",
      "|1             |101        |Electronics|1200.0|2024-01-15      |Q1     |West  |1             |\n",
      "|2             |102        |Books      |45.99 |2024-01-16      |Q1     |East  |1             |\n",
      "|3             |103        |Clothing   |89.99 |2024-01-18      |Q1     |North |2             |\n",
      "|4             |101        |Electronics|899.99|2024-02-10      |Q1     |West  |1             |\n",
      "|5             |104        |Home       |234.5 |2024-02-12      |Q1     |South |3             |\n",
      "+--------------+-----------+-----------+------+----------------+-------+------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Create Comprehensive Sample Data for Advanced SQL Patterns\n",
    "\n",
    "# Sales Transaction Data with Time Series\n",
    "sales_data = [\n",
    "    (1, 101, \"Electronics\", 1200.00, \"2024-01-15\", \"Q1\", \"West\", 1),\n",
    "    (2, 102, \"Books\", 45.99, \"2024-01-16\", \"Q1\", \"East\", 1),\n",
    "    (3, 103, \"Clothing\", 89.99, \"2024-01-18\", \"Q1\", \"North\", 2),\n",
    "    (4, 101, \"Electronics\", 899.99, \"2024-02-10\", \"Q1\", \"West\", 1),\n",
    "    (5, 104, \"Home\", 234.50, \"2024-02-12\", \"Q1\", \"South\", 3),\n",
    "    (6, 102, \"Books\", 67.89, \"2024-02-15\", \"Q1\", \"East\", 2),\n",
    "    (7, 105, \"Sports\", 156.75, \"2024-03-01\", \"Q1\", \"West\", 1),\n",
    "    (8, 103, \"Clothing\", 123.45, \"2024-03-05\", \"Q1\", \"North\", 3),\n",
    "    (9, 101, \"Electronics\", 1599.99, \"2024-04-01\", \"Q2\", \"West\", 2),\n",
    "    (10, 106, \"Beauty\", 78.99, \"2024-04-05\", \"Q2\", \"East\", 1),\n",
    "    (11, 104, \"Home\", 445.00, \"2024-04-10\", \"Q2\", \"South\", 2),\n",
    "    (12, 107, \"Automotive\", 2345.00, \"2024-05-01\", \"Q2\", \"North\", 1),\n",
    "    (13, 102, \"Books\", 34.99, \"2024-05-15\", \"Q2\", \"East\", 3),\n",
    "    (14, 105, \"Sports\", 267.50, \"2024-05-20\", \"Q2\", \"West\", 2),\n",
    "    (15, 108, \"Garden\", 189.99, \"2024-06-01\", \"Q2\", \"South\", 1),\n",
    "    (16, 103, \"Clothing\", 99.99, \"2024-06-10\", \"Q2\", \"North\", 1),\n",
    "    (17, 101, \"Electronics\", 2199.99, \"2024-07-01\", \"Q3\", \"West\", 3),\n",
    "    (18, 109, \"Toys\", 125.00, \"2024-07-05\", \"Q3\", \"East\", 2),\n",
    "    (19, 104, \"Home\", 567.89, \"2024-07-15\", \"Q3\", \"South\", 1),\n",
    "    (20, 110, \"Jewelry\", 899.99, \"2024-08-01\", \"Q3\", \"North\", 1)\n",
    "]\n",
    "\n",
    "sales_schema = StructType([\n",
    "    StructField(\"transaction_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"transaction_date\", StringType(), True),\n",
    "    StructField(\"quarter\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"salesperson_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, sales_schema)\n",
    "\n",
    "# Convert string date to proper date type\n",
    "sales_df = sales_df.withColumn(\"transaction_date\", to_date(col(\"transaction_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "print(\"📊 Sales Transaction Dataset Created:\")\n",
    "print(f\"   📈 Records: {sales_df.count()}\")\n",
    "print(f\"   📅 Date Range: {sales_df.agg(min('transaction_date'), max('transaction_date')).collect()[0]}\")\n",
    "sales_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b80b2706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👥 Customer Dataset Created:\n",
      "   📊 Records: 10\n",
      "+--------+-----+\n",
      "|    tier|count|\n",
      "+--------+-----+\n",
      "| Premium|    4|\n",
      "|Standard|    3|\n",
      "|    Gold|    3|\n",
      "+--------+-----+\n",
      "\n",
      "\n",
      "👨‍💼 Employee Dataset Created:\n",
      "   📊 Records: 5\n",
      "+--------+-----+\n",
      "|    tier|count|\n",
      "+--------+-----+\n",
      "| Premium|    4|\n",
      "|Standard|    3|\n",
      "|    Gold|    3|\n",
      "+--------+-----+\n",
      "\n",
      "\n",
      "👨‍💼 Employee Dataset Created:\n",
      "   📊 Records: 5\n",
      "+------+-----+\n",
      "| level|count|\n",
      "+------+-----+\n",
      "|Senior|    3|\n",
      "|Junior|    1|\n",
      "|   Mid|    1|\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "| level|count|\n",
      "+------+-----+\n",
      "|Senior|    3|\n",
      "|Junior|    1|\n",
      "|   Mid|    1|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Customer Demographics Dataset\n",
    "customer_data = [\n",
    "    (101, \"Alice Johnson\", \"alice.j@email.com\", \"Premium\", \"2023-01-15\", 32, \"Female\", \"Manager\"),\n",
    "    (102, \"Bob Smith\", \"bob.s@email.com\", \"Standard\", \"2023-03-22\", 45, \"Male\", \"Engineer\"),\n",
    "    (103, \"Carol Davis\", \"carol.d@email.com\", \"Premium\", \"2023-02-10\", 28, \"Female\", \"Designer\"),\n",
    "    (104, \"David Wilson\", \"david.w@email.com\", \"Gold\", \"2022-11-05\", 52, \"Male\", \"Director\"),\n",
    "    (105, \"Emma Brown\", \"emma.b@email.com\", \"Standard\", \"2023-06-18\", 36, \"Female\", \"Analyst\"),\n",
    "    (106, \"Frank Miller\", \"frank.m@email.com\", \"Premium\", \"2023-04-12\", 41, \"Male\", \"Consultant\"),\n",
    "    (107, \"Grace Lee\", \"grace.l@email.com\", \"Gold\", \"2022-09-30\", 29, \"Female\", \"Developer\"),\n",
    "    (108, \"Henry Taylor\", \"henry.t@email.com\", \"Standard\", \"2023-07-25\", 38, \"Male\", \"Manager\"),\n",
    "    (109, \"Ivy Chen\", \"ivy.c@email.com\", \"Premium\", \"2023-05-08\", 33, \"Female\", \"Researcher\"),\n",
    "    (110, \"Jack Anderson\", \"jack.a@email.com\", \"Gold\", \"2022-12-20\", 47, \"Male\", \"Executive\")\n",
    "]\n",
    "\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"tier\", StringType(), True),\n",
    "    StructField(\"signup_date\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"job_title\", StringType(), True)\n",
    "])\n",
    "\n",
    "customers_df = spark.createDataFrame(customer_data, customer_schema)\n",
    "customers_df = customers_df.withColumn(\"signup_date\", to_date(col(\"signup_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Salesperson/Employee Dataset\n",
    "employee_data = [\n",
    "    (1, \"Sarah Connor\", \"West\", \"Senior\", 85000, \"2022-01-15\", 145.2),\n",
    "    (2, \"John Matrix\", \"North\", \"Junior\", 52000, \"2023-03-10\", 98.7),\n",
    "    (3, \"Ellen Ripley\", \"South\", \"Senior\", 78000, \"2021-11-20\", 132.8),\n",
    "    (4, \"Kyle Reese\", \"East\", \"Mid\", 65000, \"2022-08-05\", 76.5),\n",
    "    (5, \"Dutch Schaefer\", \"West\", \"Senior\", 82000, \"2021-05-12\", 167.3)\n",
    "]\n",
    "\n",
    "employee_schema = StructType([\n",
    "    StructField(\"salesperson_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"level\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"hire_date\", StringType(), True),\n",
    "    StructField(\"performance_score\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "employees_df = spark.createDataFrame(employee_data, employee_schema)\n",
    "employees_df = employees_df.withColumn(\"hire_date\", to_date(col(\"hire_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "print(\"👥 Customer Dataset Created:\")\n",
    "print(f\"   📊 Records: {customers_df.count()}\")\n",
    "customers_df.groupBy(\"tier\").count().orderBy(\"count\", ascending=False).show()\n",
    "\n",
    "print(\"\\n👨‍💼 Employee Dataset Created:\")\n",
    "print(f\"   📊 Records: {employees_df.count()}\")\n",
    "employees_df.groupBy(\"level\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4390849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗄️ SQL Views Registered Successfully!\n",
      "\n",
      "📋 Available Views:\n",
      "+---------+---------+-----------+\n",
      "|namespace| viewName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|         |customers|       true|\n",
      "|         |employees|       true|\n",
      "|         |    sales|       true|\n",
      "+---------+---------+-----------+\n",
      "\n",
      "\n",
      "🔍 Quick Data Verification:\n",
      "+-----------+----------------+------------------+------------------+\n",
      "|total_sales|unique_customers|unique_salespeople|     total_revenue|\n",
      "+-----------+----------------+------------------+------------------+\n",
      "|         20|              10|                 3|11672.880000000001|\n",
      "+-----------+----------------+------------------+------------------+\n",
      "\n",
      "✅ All datasets ready for advanced SQL patterns!\n",
      "🎯 Ready to explore: Window Functions, CTEs, Advanced Joins, and Statistical Analysis\n",
      "+-----------+----------------+------------------+------------------+\n",
      "|total_sales|unique_customers|unique_salespeople|     total_revenue|\n",
      "+-----------+----------------+------------------+------------------+\n",
      "|         20|              10|                 3|11672.880000000001|\n",
      "+-----------+----------------+------------------+------------------+\n",
      "\n",
      "✅ All datasets ready for advanced SQL patterns!\n",
      "🎯 Ready to explore: Window Functions, CTEs, Advanced Joins, and Statistical Analysis\n"
     ]
    }
   ],
   "source": [
    "# Register DataFrames as SQL Views for Advanced SQL Patterns\n",
    "sales_df.createOrReplaceTempView(\"sales\")\n",
    "customers_df.createOrReplaceTempView(\"customers\") \n",
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "# Verify view registration\n",
    "print(\"🗄️ SQL Views Registered Successfully!\")\n",
    "print(\"\\n📋 Available Views:\")\n",
    "spark.sql(\"SHOW VIEWS\").show()\n",
    "\n",
    "# Quick verification with SQL\n",
    "print(\"\\n🔍 Quick Data Verification:\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_sales,\n",
    "        COUNT(DISTINCT customer_id) as unique_customers,\n",
    "        COUNT(DISTINCT salesperson_id) as unique_salespeople,\n",
    "        SUM(amount) as total_revenue\n",
    "    FROM sales\n",
    "\"\"\")\n",
    "result.show()\n",
    "\n",
    "print(\"✅ All datasets ready for advanced SQL patterns!\")\n",
    "print(\"🎯 Ready to explore: Window Functions, CTEs, Advanced Joins, and Statistical Analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8040b43c",
   "metadata": {},
   "source": [
    "## 🪟 Section 1: Window Functions & Analytics\n",
    "\n",
    "Window functions are powerful tools for performing calculations across a set of rows related to the current row. Unlike regular aggregations that reduce data, window functions maintain the original row count while adding analytical insights.\n",
    "\n",
    "### 🎯 **Key Concepts:**\n",
    "- **OVER Clause**: Defines the window specification\n",
    "- **PARTITION BY**: Groups rows for separate calculations\n",
    "- **ORDER BY**: Defines row ordering within partitions\n",
    "- **Frame Specification**: ROWS vs RANGE boundaries\n",
    "- **Ranking Functions**: ROW_NUMBER, RANK, DENSE_RANK\n",
    "- **Analytical Functions**: LAG, LEAD, FIRST_VALUE, LAST_VALUE\n",
    "- **Aggregate Functions**: SUM, AVG, COUNT over windows\n",
    "\n",
    "### 📊 **Use Cases:**\n",
    "- Running totals and moving averages\n",
    "- Ranking and percentile calculations\n",
    "- Time series analysis and comparisons\n",
    "- Data quality and outlier detection\n",
    "- Business analytics and KPI tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2311bf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 1.1 Basic Ranking Functions: ROW_NUMBER, RANK, DENSE_RANK\n",
      "\n",
      "📊 Ranking Results (Top 10 by Amount):\n",
      "+--------------+-----------+-----------+-------+----------------+------+--------------------+--------------+--------------------+----------------------+----------------+\n",
      "|transaction_id|customer_id|category   |amount |transaction_date|region|row_number_in_region|rank_in_region|dense_rank_in_region|row_number_by_category|rank_by_category|\n",
      "+--------------+-----------+-----------+-------+----------------+------+--------------------+--------------+--------------------+----------------------+----------------+\n",
      "|12            |107        |Automotive |2345.0 |2024-05-01      |North |1                   |1             |1                   |1                     |1               |\n",
      "|17            |101        |Electronics|2199.99|2024-07-01      |West  |1                   |1             |1                   |1                     |1               |\n",
      "|9             |101        |Electronics|1599.99|2024-04-01      |West  |2                   |2             |2                   |2                     |2               |\n",
      "|1             |101        |Electronics|1200.0 |2024-01-15      |West  |3                   |3             |3                   |3                     |3               |\n",
      "|4             |101        |Electronics|899.99 |2024-02-10      |West  |4                   |4             |4                   |4                     |4               |\n",
      "|20            |110        |Jewelry    |899.99 |2024-08-01      |North |2                   |2             |2                   |1                     |1               |\n",
      "|19            |104        |Home       |567.89 |2024-07-15      |South |1                   |1             |1                   |1                     |1               |\n",
      "|11            |104        |Home       |445.0  |2024-04-10      |South |2                   |2             |2                   |2                     |2               |\n",
      "|14            |105        |Sports     |267.5  |2024-05-20      |West  |5                   |5             |5                   |1                     |1               |\n",
      "|5             |104        |Home       |234.5  |2024-02-12      |South |3                   |3             |3                   |3                     |3               |\n",
      "+--------------+-----------+-----------+-------+----------------+------+--------------------+--------------+--------------------+----------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "🔍 Understanding Ranking Differences:\n",
      "   • ROW_NUMBER: Sequential numbers (1,2,3,4...)\n",
      "   • RANK: Gaps after ties (1,2,2,4...)\n",
      "   • DENSE_RANK: No gaps after ties (1,2,2,3...)\n",
      "+--------------+-----------+-----------+-------+----------------+------+--------------------+--------------+--------------------+----------------------+----------------+\n",
      "|transaction_id|customer_id|category   |amount |transaction_date|region|row_number_in_region|rank_in_region|dense_rank_in_region|row_number_by_category|rank_by_category|\n",
      "+--------------+-----------+-----------+-------+----------------+------+--------------------+--------------+--------------------+----------------------+----------------+\n",
      "|12            |107        |Automotive |2345.0 |2024-05-01      |North |1                   |1             |1                   |1                     |1               |\n",
      "|17            |101        |Electronics|2199.99|2024-07-01      |West  |1                   |1             |1                   |1                     |1               |\n",
      "|9             |101        |Electronics|1599.99|2024-04-01      |West  |2                   |2             |2                   |2                     |2               |\n",
      "|1             |101        |Electronics|1200.0 |2024-01-15      |West  |3                   |3             |3                   |3                     |3               |\n",
      "|4             |101        |Electronics|899.99 |2024-02-10      |West  |4                   |4             |4                   |4                     |4               |\n",
      "|20            |110        |Jewelry    |899.99 |2024-08-01      |North |2                   |2             |2                   |1                     |1               |\n",
      "|19            |104        |Home       |567.89 |2024-07-15      |South |1                   |1             |1                   |1                     |1               |\n",
      "|11            |104        |Home       |445.0  |2024-04-10      |South |2                   |2             |2                   |2                     |2               |\n",
      "|14            |105        |Sports     |267.5  |2024-05-20      |West  |5                   |5             |5                   |1                     |1               |\n",
      "|5             |104        |Home       |234.5  |2024-02-12      |South |3                   |3             |3                   |3                     |3               |\n",
      "+--------------+-----------+-----------+-------+----------------+------+--------------------+--------------+--------------------+----------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "🔍 Understanding Ranking Differences:\n",
      "   • ROW_NUMBER: Sequential numbers (1,2,3,4...)\n",
      "   • RANK: Gaps after ties (1,2,2,4...)\n",
      "   • DENSE_RANK: No gaps after ties (1,2,2,3...)\n"
     ]
    }
   ],
   "source": [
    "# Basic Ranking Window Functions\n",
    "print(\"🎯 1.1 Basic Ranking Functions: ROW_NUMBER, RANK, DENSE_RANK\")\n",
    "\n",
    "# Define window specifications - PERFORMANCE OPTIMIZED with partitioning\n",
    "window_by_region = Window.partitionBy(\"region\").orderBy(desc(\"amount\"))\n",
    "window_by_category = Window.partitionBy(\"category\").orderBy(desc(\"amount\"))\n",
    "\n",
    "# Apply ranking functions using DataFrame API - NO SINGLE PARTITION WARNING\n",
    "ranking_df = sales_df.select(\n",
    "    \"transaction_id\",\n",
    "    \"customer_id\", \n",
    "    \"category\",\n",
    "    \"amount\",\n",
    "    \"transaction_date\",\n",
    "    \"region\",\n",
    "    row_number().over(window_by_region).alias(\"row_number_in_region\"),\n",
    "    rank().over(window_by_region).alias(\"rank_in_region\"),\n",
    "    dense_rank().over(window_by_region).alias(\"dense_rank_in_region\"),\n",
    "    row_number().over(window_by_category).alias(\"row_number_by_category\"),\n",
    "    rank().over(window_by_category).alias(\"rank_by_category\")\n",
    ")\n",
    "\n",
    "print(\"\\n📊 Ranking Results (Top 10 by Amount):\")\n",
    "ranking_df.orderBy(desc(\"amount\")).show(10, truncate=False)\n",
    "\n",
    "# Explain the differences\n",
    "print(\"\\n🔍 Understanding Ranking Differences:\")\n",
    "print(\"   • ROW_NUMBER: Sequential numbers (1,2,3,4...)\")  \n",
    "print(\"   • RANK: Gaps after ties (1,2,2,4...)\")\n",
    "print(\"   • DENSE_RANK: No gaps after ties (1,2,2,3...)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8406b407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 1.2 Ranking Functions with SQL Syntax\n",
      "\n",
      "📊 SQL Ranking Results with Percentiles:\n",
      "+--------------+-----------+----------+------+----------------+------+--------------------+--------------+--------------------+----------------------+----------------+----------------------+---------------------------------+\n",
      "|transaction_id|customer_id|category  |amount|transaction_date|region|row_number_in_region|rank_in_region|dense_rank_in_region|row_number_by_category|rank_by_category|percent_rank_in_region|cumulative_distribution_in_region|\n",
      "+--------------+-----------+----------+------+----------------+------+--------------------+--------------+--------------------+----------------------+----------------+----------------------+---------------------------------+\n",
      "|18            |109        |Toys      |125.0 |2024-07-05      |East  |1                   |1             |1                   |1                     |1               |0.0                   |0.2                              |\n",
      "|10            |106        |Beauty    |78.99 |2024-04-05      |East  |2                   |2             |2                   |1                     |1               |0.25                  |0.4                              |\n",
      "|6             |102        |Books     |67.89 |2024-02-15      |East  |3                   |3             |3                   |1                     |1               |0.5                   |0.6                              |\n",
      "|2             |102        |Books     |45.99 |2024-01-16      |East  |4                   |4             |4                   |2                     |2               |0.75                  |0.8                              |\n",
      "|13            |102        |Books     |34.99 |2024-05-15      |East  |5                   |5             |5                   |3                     |3               |1.0                   |1.0                              |\n",
      "|12            |107        |Automotive|2345.0|2024-05-01      |North |1                   |1             |1                   |1                     |1               |0.0                   |0.2                              |\n",
      "|20            |110        |Jewelry   |899.99|2024-08-01      |North |2                   |2             |2                   |1                     |1               |0.25                  |0.4                              |\n",
      "|8             |103        |Clothing  |123.45|2024-03-05      |North |3                   |3             |3                   |1                     |1               |0.5                   |0.6                              |\n",
      "|16            |103        |Clothing  |99.99 |2024-06-10      |North |4                   |4             |4                   |2                     |2               |0.75                  |0.8                              |\n",
      "|3             |103        |Clothing  |89.99 |2024-01-18      |North |5                   |5             |5                   |3                     |3               |1.0                   |1.0                              |\n",
      "+--------------+-----------+----------+------+----------------+------+--------------------+--------------+--------------------+----------------------+----------------+----------------------+---------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "📈 Percentile Analysis:\n",
      "+--------------+-----------+----------+------+----------------+------+--------------------+--------------+--------------------+----------------------+----------------+----------------------+---------------------------------+\n",
      "|transaction_id|customer_id|category  |amount|transaction_date|region|row_number_in_region|rank_in_region|dense_rank_in_region|row_number_by_category|rank_by_category|percent_rank_in_region|cumulative_distribution_in_region|\n",
      "+--------------+-----------+----------+------+----------------+------+--------------------+--------------+--------------------+----------------------+----------------+----------------------+---------------------------------+\n",
      "|18            |109        |Toys      |125.0 |2024-07-05      |East  |1                   |1             |1                   |1                     |1               |0.0                   |0.2                              |\n",
      "|10            |106        |Beauty    |78.99 |2024-04-05      |East  |2                   |2             |2                   |1                     |1               |0.25                  |0.4                              |\n",
      "|6             |102        |Books     |67.89 |2024-02-15      |East  |3                   |3             |3                   |1                     |1               |0.5                   |0.6                              |\n",
      "|2             |102        |Books     |45.99 |2024-01-16      |East  |4                   |4             |4                   |2                     |2               |0.75                  |0.8                              |\n",
      "|13            |102        |Books     |34.99 |2024-05-15      |East  |5                   |5             |5                   |3                     |3               |1.0                   |1.0                              |\n",
      "|12            |107        |Automotive|2345.0|2024-05-01      |North |1                   |1             |1                   |1                     |1               |0.0                   |0.2                              |\n",
      "|20            |110        |Jewelry   |899.99|2024-08-01      |North |2                   |2             |2                   |1                     |1               |0.25                  |0.4                              |\n",
      "|8             |103        |Clothing  |123.45|2024-03-05      |North |3                   |3             |3                   |1                     |1               |0.5                   |0.6                              |\n",
      "|16            |103        |Clothing  |99.99 |2024-06-10      |North |4                   |4             |4                   |2                     |2               |0.75                  |0.8                              |\n",
      "|3             |103        |Clothing  |89.99 |2024-01-18      |North |5                   |5             |5                   |3                     |3               |1.0                   |1.0                              |\n",
      "+--------------+-----------+----------+------+----------------+------+--------------------+--------------+--------------------+----------------------+----------------+----------------------+---------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "📈 Percentile Analysis:\n",
      "+-----------+-----------------+----------+-------------+----------+----------+\n",
      "|category   |transaction_count|avg_amount|median_amount|p75_amount|p90_amount|\n",
      "+-----------+-----------------+----------+-------------+----------+----------+\n",
      "|Automotive |1                |2345.0    |2345.0       |2345.0    |2345.0    |\n",
      "|Electronics|4                |1474.99   |1200.0       |1599.99   |2199.99   |\n",
      "|Jewelry    |1                |899.99    |899.99       |899.99    |899.99    |\n",
      "|Home       |3                |415.8     |445.0        |567.89    |567.89    |\n",
      "|Sports     |2                |212.13    |156.75       |267.5     |267.5     |\n",
      "|Garden     |1                |189.99    |189.99       |189.99    |189.99    |\n",
      "|Toys       |1                |125.0     |125.0        |125.0     |125.0     |\n",
      "|Clothing   |3                |104.48    |99.99        |123.45    |123.45    |\n",
      "|Beauty     |1                |78.99     |78.99        |78.99     |78.99     |\n",
      "|Books      |3                |49.62     |45.99        |67.89     |67.89     |\n",
      "+-----------+-----------------+----------+-------------+----------+----------+\n",
      "\n",
      "+-----------+-----------------+----------+-------------+----------+----------+\n",
      "|category   |transaction_count|avg_amount|median_amount|p75_amount|p90_amount|\n",
      "+-----------+-----------------+----------+-------------+----------+----------+\n",
      "|Automotive |1                |2345.0    |2345.0       |2345.0    |2345.0    |\n",
      "|Electronics|4                |1474.99   |1200.0       |1599.99   |2199.99   |\n",
      "|Jewelry    |1                |899.99    |899.99       |899.99    |899.99    |\n",
      "|Home       |3                |415.8     |445.0        |567.89    |567.89    |\n",
      "|Sports     |2                |212.13    |156.75       |267.5     |267.5     |\n",
      "|Garden     |1                |189.99    |189.99       |189.99    |189.99    |\n",
      "|Toys       |1                |125.0     |125.0        |125.0     |125.0     |\n",
      "|Clothing   |3                |104.48    |99.99        |123.45    |123.45    |\n",
      "|Beauty     |1                |78.99     |78.99        |78.99     |78.99     |\n",
      "|Books      |3                |49.62     |45.99        |67.89     |67.89     |\n",
      "+-----------+-----------------+----------+-------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Same Operations Using SQL Syntax\n",
    "print(\"🎯 1.2 Ranking Functions with SQL Syntax\")\n",
    "\n",
    "sql_ranking = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        transaction_id,\n",
    "        customer_id,\n",
    "        category,\n",
    "        amount,\n",
    "        transaction_date,\n",
    "        region,\n",
    "        -- PERFORMANCE OPTIMIZED: Partitioned window functions\n",
    "        ROW_NUMBER() OVER (PARTITION BY region ORDER BY amount DESC) as row_number_in_region,\n",
    "        RANK() OVER (PARTITION BY region ORDER BY amount DESC) as rank_in_region,\n",
    "        DENSE_RANK() OVER (PARTITION BY region ORDER BY amount DESC) as dense_rank_in_region,\n",
    "        ROW_NUMBER() OVER (PARTITION BY category ORDER BY amount DESC) as row_number_by_category,\n",
    "        RANK() OVER (PARTITION BY category ORDER BY amount DESC) as rank_by_category,\n",
    "        -- Percentile ranking within region\n",
    "        PERCENT_RANK() OVER (PARTITION BY region ORDER BY amount DESC) as percent_rank_in_region,\n",
    "        CUME_DIST() OVER (PARTITION BY region ORDER BY amount DESC) as cumulative_distribution_in_region\n",
    "    FROM sales\n",
    "    ORDER BY region, rank_in_region\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n📊 SQL Ranking Results with Percentiles:\")\n",
    "sql_ranking.show(10, truncate=False)\n",
    "\n",
    "# Focus on percentile analysis\n",
    "print(\"\\n📈 Percentile Analysis:\")\n",
    "percentile_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        COUNT(*) as transaction_count,\n",
    "        ROUND(AVG(amount), 2) as avg_amount,\n",
    "        ROUND(PERCENTILE_APPROX(amount, 0.5), 2) as median_amount,\n",
    "        ROUND(PERCENTILE_APPROX(amount, 0.75), 2) as p75_amount,\n",
    "        ROUND(PERCENTILE_APPROX(amount, 0.9), 2) as p90_amount\n",
    "    FROM sales \n",
    "    GROUP BY category\n",
    "    ORDER BY avg_amount DESC\n",
    "\"\"\")\n",
    "percentile_analysis.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "703da1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 1.3 LAG and LEAD Functions for Time Series Analysis\n",
      "\n",
      "📊 Time Series Analysis Results:\n",
      "+-----------+----------------+-------+-----------+---------------+-------------+-----------+----------+------------------+---------------+\n",
      "|customer_id|transaction_date|amount |category   |previous_amount|previous_date|next_amount|next_date |amount_change     |days_since_last|\n",
      "+-----------+----------------+-------+-----------+---------------+-------------+-----------+----------+------------------+---------------+\n",
      "|101        |2024-01-15      |1200.0 |Electronics|NULL           |NULL         |899.99     |2024-02-10|NULL              |NULL           |\n",
      "|101        |2024-02-10      |899.99 |Electronics|1200.0         |2024-01-15   |1599.99    |2024-04-01|-300.01           |26             |\n",
      "|101        |2024-04-01      |1599.99|Electronics|899.99         |2024-02-10   |2199.99    |2024-07-01|700.0             |51             |\n",
      "|101        |2024-07-01      |2199.99|Electronics|1599.99        |2024-04-01   |NULL       |NULL      |599.9999999999998 |91             |\n",
      "|102        |2024-01-16      |45.99  |Books      |NULL           |NULL         |67.89      |2024-02-15|NULL              |NULL           |\n",
      "|102        |2024-02-15      |67.89  |Books      |45.99          |2024-01-16   |34.99      |2024-05-15|21.9              |30             |\n",
      "|102        |2024-05-15      |34.99  |Books      |67.89          |2024-02-15   |NULL       |NULL      |-32.9             |90             |\n",
      "|104        |2024-02-12      |234.5  |Home       |NULL           |NULL         |445.0      |2024-04-10|NULL              |NULL           |\n",
      "|104        |2024-04-10      |445.0  |Home       |234.5          |2024-02-12   |567.89     |2024-07-15|210.5             |58             |\n",
      "|104        |2024-07-15      |567.89 |Home       |445.0          |2024-04-10   |NULL       |NULL      |122.88999999999999|96             |\n",
      "+-----------+----------------+-------+-----------+---------------+-------------+-----------+----------+------------------+---------------+\n",
      "\n",
      "\n",
      "🎯 1.4 Advanced Time Series Patterns with SQL\n",
      "\n",
      "📈 Advanced Time Series Analysis:\n",
      "+-----------+----------------+-------+-----------+---------------+-------------+-----------+----------+------------------+---------------+\n",
      "|customer_id|transaction_date|amount |category   |previous_amount|previous_date|next_amount|next_date |amount_change     |days_since_last|\n",
      "+-----------+----------------+-------+-----------+---------------+-------------+-----------+----------+------------------+---------------+\n",
      "|101        |2024-01-15      |1200.0 |Electronics|NULL           |NULL         |899.99     |2024-02-10|NULL              |NULL           |\n",
      "|101        |2024-02-10      |899.99 |Electronics|1200.0         |2024-01-15   |1599.99    |2024-04-01|-300.01           |26             |\n",
      "|101        |2024-04-01      |1599.99|Electronics|899.99         |2024-02-10   |2199.99    |2024-07-01|700.0             |51             |\n",
      "|101        |2024-07-01      |2199.99|Electronics|1599.99        |2024-04-01   |NULL       |NULL      |599.9999999999998 |91             |\n",
      "|102        |2024-01-16      |45.99  |Books      |NULL           |NULL         |67.89      |2024-02-15|NULL              |NULL           |\n",
      "|102        |2024-02-15      |67.89  |Books      |45.99          |2024-01-16   |34.99      |2024-05-15|21.9              |30             |\n",
      "|102        |2024-05-15      |34.99  |Books      |67.89          |2024-02-15   |NULL       |NULL      |-32.9             |90             |\n",
      "|104        |2024-02-12      |234.5  |Home       |NULL           |NULL         |445.0      |2024-04-10|NULL              |NULL           |\n",
      "|104        |2024-04-10      |445.0  |Home       |234.5          |2024-02-12   |567.89     |2024-07-15|210.5             |58             |\n",
      "|104        |2024-07-15      |567.89 |Home       |445.0          |2024-04-10   |NULL       |NULL      |122.88999999999999|96             |\n",
      "+-----------+----------------+-------+-----------+---------------+-------------+-----------+----------+------------------+---------------+\n",
      "\n",
      "\n",
      "🎯 1.4 Advanced Time Series Patterns with SQL\n",
      "\n",
      "📈 Advanced Time Series Analysis:\n",
      "+-----------+----------------+-------+-----------+-----------+-------------+--------------+-------------+-----------------+------------------+\n",
      "|customer_id|transaction_date|amount |category   |prev_amount|amount_change|first_purchase|last_purchase|purchase_sequence|spending_trend    |\n",
      "+-----------+----------------+-------+-----------+-----------+-------------+--------------+-------------+-----------------+------------------+\n",
      "|101        |2024-01-15      |1200.0 |Electronics|NULL       |NULL         |1200.0        |2199.99      |1                |First Purchase    |\n",
      "|101        |2024-02-10      |899.99 |Electronics|1200.0     |-300.01      |1200.0        |2199.99      |2                |Decreased Spending|\n",
      "|101        |2024-04-01      |1599.99|Electronics|899.99     |700.0        |1200.0        |2199.99      |3                |Increased Spending|\n",
      "|101        |2024-07-01      |2199.99|Electronics|1599.99    |600.0        |1200.0        |2199.99      |4                |Increased Spending|\n",
      "|102        |2024-01-16      |45.99  |Books      |NULL       |NULL         |45.99         |34.99        |1                |First Purchase    |\n",
      "|102        |2024-02-15      |67.89  |Books      |45.99      |21.9         |45.99         |34.99        |2                |Increased Spending|\n",
      "|102        |2024-05-15      |34.99  |Books      |67.89      |-32.9        |45.99         |34.99        |3                |Decreased Spending|\n",
      "|104        |2024-02-12      |234.5  |Home       |NULL       |NULL         |234.5         |567.89       |1                |First Purchase    |\n",
      "|104        |2024-04-10      |445.0  |Home       |234.5      |210.5        |234.5         |567.89       |2                |Increased Spending|\n",
      "|104        |2024-07-15      |567.89 |Home       |445.0      |122.89       |234.5         |567.89       |3                |Increased Spending|\n",
      "+-----------+----------------+-------+-----------+-----------+-------------+--------------+-------------+-----------------+------------------+\n",
      "\n",
      "+-----------+----------------+-------+-----------+-----------+-------------+--------------+-------------+-----------------+------------------+\n",
      "|customer_id|transaction_date|amount |category   |prev_amount|amount_change|first_purchase|last_purchase|purchase_sequence|spending_trend    |\n",
      "+-----------+----------------+-------+-----------+-----------+-------------+--------------+-------------+-----------------+------------------+\n",
      "|101        |2024-01-15      |1200.0 |Electronics|NULL       |NULL         |1200.0        |2199.99      |1                |First Purchase    |\n",
      "|101        |2024-02-10      |899.99 |Electronics|1200.0     |-300.01      |1200.0        |2199.99      |2                |Decreased Spending|\n",
      "|101        |2024-04-01      |1599.99|Electronics|899.99     |700.0        |1200.0        |2199.99      |3                |Increased Spending|\n",
      "|101        |2024-07-01      |2199.99|Electronics|1599.99    |600.0        |1200.0        |2199.99      |4                |Increased Spending|\n",
      "|102        |2024-01-16      |45.99  |Books      |NULL       |NULL         |45.99         |34.99        |1                |First Purchase    |\n",
      "|102        |2024-02-15      |67.89  |Books      |45.99      |21.9         |45.99         |34.99        |2                |Increased Spending|\n",
      "|102        |2024-05-15      |34.99  |Books      |67.89      |-32.9        |45.99         |34.99        |3                |Decreased Spending|\n",
      "|104        |2024-02-12      |234.5  |Home       |NULL       |NULL         |234.5         |567.89       |1                |First Purchase    |\n",
      "|104        |2024-04-10      |445.0  |Home       |234.5      |210.5        |234.5         |567.89       |2                |Increased Spending|\n",
      "|104        |2024-07-15      |567.89 |Home       |445.0      |122.89       |234.5         |567.89       |3                |Increased Spending|\n",
      "+-----------+----------------+-------+-----------+-----------+-------------+--------------+-------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LAG and LEAD Functions for Time Series Analysis\n",
    "print(\"🎯 1.3 LAG and LEAD Functions for Time Series Analysis\")\n",
    "\n",
    "# Time-based window for each customer\n",
    "time_window = Window.partitionBy(\"customer_id\").orderBy(\"transaction_date\")\n",
    "\n",
    "# Apply lag and lead functions\n",
    "time_analysis_df = sales_df.select(\n",
    "    \"customer_id\",\n",
    "    \"transaction_date\", \n",
    "    \"amount\",\n",
    "    \"category\",\n",
    "    # Previous transaction\n",
    "    lag(\"amount\", 1).over(time_window).alias(\"previous_amount\"),\n",
    "    lag(\"transaction_date\", 1).over(time_window).alias(\"previous_date\"),\n",
    "    # Next transaction\n",
    "    lead(\"amount\", 1).over(time_window).alias(\"next_amount\"),\n",
    "    lead(\"transaction_date\", 1).over(time_window).alias(\"next_date\"),\n",
    "    # Calculate differences\n",
    "    (col(\"amount\") - lag(\"amount\", 1).over(time_window)).alias(\"amount_change\"),\n",
    "    datediff(col(\"transaction_date\"), lag(\"transaction_date\", 1).over(time_window)).alias(\"days_since_last\")\n",
    ").where(col(\"customer_id\").isin([101, 102, 104]))  # Focus on customers with multiple transactions\n",
    "\n",
    "print(\"\\n📊 Time Series Analysis Results:\")\n",
    "time_analysis_df.orderBy(\"customer_id\", \"transaction_date\").show(truncate=False)\n",
    "\n",
    "# SQL version with more advanced patterns\n",
    "print(\"\\n🎯 1.4 Advanced Time Series Patterns with SQL\")\n",
    "advanced_time_sql = spark.sql(\"\"\"\n",
    "    WITH customer_transactions AS (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            transaction_date,\n",
    "            amount,\n",
    "            category,\n",
    "            LAG(amount, 1) OVER (PARTITION BY customer_id ORDER BY transaction_date) as prev_amount,\n",
    "            LEAD(amount, 1) OVER (PARTITION BY customer_id ORDER BY transaction_date) as next_amount,\n",
    "            FIRST_VALUE(amount) OVER (PARTITION BY customer_id ORDER BY transaction_date \n",
    "                                    ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as first_purchase,\n",
    "            LAST_VALUE(amount) OVER (PARTITION BY customer_id ORDER BY transaction_date \n",
    "                                   ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING) as last_purchase,\n",
    "            ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY transaction_date) as purchase_sequence\n",
    "        FROM sales\n",
    "        WHERE customer_id IN (101, 102, 104)\n",
    "    )\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        transaction_date,\n",
    "        amount,\n",
    "        category,\n",
    "        prev_amount,\n",
    "        ROUND(amount - prev_amount, 2) as amount_change,\n",
    "        first_purchase,\n",
    "        last_purchase,\n",
    "        purchase_sequence,\n",
    "        CASE \n",
    "            WHEN prev_amount IS NULL THEN 'First Purchase'\n",
    "            WHEN amount > prev_amount THEN 'Increased Spending'\n",
    "            WHEN amount < prev_amount THEN 'Decreased Spending'  \n",
    "            ELSE 'Same Spending'\n",
    "        END as spending_trend\n",
    "    FROM customer_transactions\n",
    "    ORDER BY customer_id, transaction_date\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n📈 Advanced Time Series Analysis:\")\n",
    "advanced_time_sql.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f0d8520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 1.5 Running Totals and Moving Averages with Window Frames\n",
      "\n",
      "📊 Running Totals and Moving Averages:\n",
      "+--------------+-----------+----------------+-------+-----------+------+-----------------------+-------------------------+----------------------+----------------------+-------------+------------------------------+\n",
      "|transaction_id|customer_id|transaction_date|amount |category   |region|running_total_by_region|running_total_by_category|moving_avg_3_by_region|transactions_in_window|max_in_window|transaction_sequence_in_region|\n",
      "+--------------+-----------+----------------+-------+-----------+------+-----------------------+-------------------------+----------------------+----------------------+-------------+------------------------------+\n",
      "|1             |101        |2024-01-15      |1200.0 |Electronics|West  |1200.0                 |1200.0                   |1200.0                |1                     |1200.0       |1                             |\n",
      "|2             |102        |2024-01-16      |45.99  |Books      |East  |45.99                  |45.99                    |45.99                 |1                     |45.99        |1                             |\n",
      "|3             |103        |2024-01-18      |89.99  |Clothing   |North |89.99                  |89.99                    |89.99                 |1                     |89.99        |1                             |\n",
      "|4             |101        |2024-02-10      |899.99 |Electronics|West  |2099.99                |2099.99                  |1050.0                |2                     |1200.0       |2                             |\n",
      "|5             |104        |2024-02-12      |234.5  |Home       |South |234.5                  |234.5                    |234.5                 |1                     |234.5        |1                             |\n",
      "|6             |102        |2024-02-15      |67.89  |Books      |East  |113.88                 |113.88                   |56.94                 |2                     |67.89        |2                             |\n",
      "|7             |105        |2024-03-01      |156.75 |Sports     |West  |2256.74                |156.75                   |752.25                |3                     |1200.0       |3                             |\n",
      "|8             |103        |2024-03-05      |123.45 |Clothing   |North |213.44                 |213.44                   |106.72                |2                     |123.45       |2                             |\n",
      "|9             |101        |2024-04-01      |1599.99|Electronics|West  |3856.7299999999996     |3699.9799999999996       |885.58                |3                     |1599.99      |4                             |\n",
      "|10            |106        |2024-04-05      |78.99  |Beauty     |East  |192.87                 |78.99                    |64.29                 |3                     |78.99        |3                             |\n",
      "|11            |104        |2024-04-10      |445.0  |Home       |South |679.5                  |679.5                    |339.75                |2                     |445.0        |2                             |\n",
      "|12            |107        |2024-05-01      |2345.0 |Automotive |North |2558.44                |2345.0                   |852.81                |3                     |2345.0       |3                             |\n",
      "|13            |102        |2024-05-15      |34.99  |Books      |East  |227.86                 |148.87                   |60.62                 |3                     |78.99        |4                             |\n",
      "|14            |105        |2024-05-20      |267.5  |Sports     |West  |4124.23                |424.25                   |674.75                |3                     |1599.99      |5                             |\n",
      "|15            |108        |2024-06-01      |189.99 |Garden     |South |869.49                 |189.99                   |289.83                |3                     |445.0        |3                             |\n",
      "+--------------+-----------+----------------+-------+-----------+------+-----------------------+-------------------------+----------------------+----------------------+-------------+------------------------------+\n",
      "only showing top 15 rows\n",
      "+--------------+-----------+----------------+-------+-----------+------+-----------------------+-------------------------+----------------------+----------------------+-------------+------------------------------+\n",
      "|transaction_id|customer_id|transaction_date|amount |category   |region|running_total_by_region|running_total_by_category|moving_avg_3_by_region|transactions_in_window|max_in_window|transaction_sequence_in_region|\n",
      "+--------------+-----------+----------------+-------+-----------+------+-----------------------+-------------------------+----------------------+----------------------+-------------+------------------------------+\n",
      "|1             |101        |2024-01-15      |1200.0 |Electronics|West  |1200.0                 |1200.0                   |1200.0                |1                     |1200.0       |1                             |\n",
      "|2             |102        |2024-01-16      |45.99  |Books      |East  |45.99                  |45.99                    |45.99                 |1                     |45.99        |1                             |\n",
      "|3             |103        |2024-01-18      |89.99  |Clothing   |North |89.99                  |89.99                    |89.99                 |1                     |89.99        |1                             |\n",
      "|4             |101        |2024-02-10      |899.99 |Electronics|West  |2099.99                |2099.99                  |1050.0                |2                     |1200.0       |2                             |\n",
      "|5             |104        |2024-02-12      |234.5  |Home       |South |234.5                  |234.5                    |234.5                 |1                     |234.5        |1                             |\n",
      "|6             |102        |2024-02-15      |67.89  |Books      |East  |113.88                 |113.88                   |56.94                 |2                     |67.89        |2                             |\n",
      "|7             |105        |2024-03-01      |156.75 |Sports     |West  |2256.74                |156.75                   |752.25                |3                     |1200.0       |3                             |\n",
      "|8             |103        |2024-03-05      |123.45 |Clothing   |North |213.44                 |213.44                   |106.72                |2                     |123.45       |2                             |\n",
      "|9             |101        |2024-04-01      |1599.99|Electronics|West  |3856.7299999999996     |3699.9799999999996       |885.58                |3                     |1599.99      |4                             |\n",
      "|10            |106        |2024-04-05      |78.99  |Beauty     |East  |192.87                 |78.99                    |64.29                 |3                     |78.99        |3                             |\n",
      "|11            |104        |2024-04-10      |445.0  |Home       |South |679.5                  |679.5                    |339.75                |2                     |445.0        |2                             |\n",
      "|12            |107        |2024-05-01      |2345.0 |Automotive |North |2558.44                |2345.0                   |852.81                |3                     |2345.0       |3                             |\n",
      "|13            |102        |2024-05-15      |34.99  |Books      |East  |227.86                 |148.87                   |60.62                 |3                     |78.99        |4                             |\n",
      "|14            |105        |2024-05-20      |267.5  |Sports     |West  |4124.23                |424.25                   |674.75                |3                     |1599.99      |5                             |\n",
      "|15            |108        |2024-06-01      |189.99 |Garden     |South |869.49                 |189.99                   |289.83                |3                     |445.0        |3                             |\n",
      "+--------------+-----------+----------------+-------+-----------+------+-----------------------+-------------------------+----------------------+----------------------+-------------+------------------------------+\n",
      "only showing top 15 rows\n"
     ]
    }
   ],
   "source": [
    "# Running Totals and Moving Averages\n",
    "print(\"🎯 1.5 Running Totals and Moving Averages with Window Frames\")\n",
    "\n",
    "# Create comprehensive running calculations using SQL for clarity\n",
    "running_totals_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        transaction_id,\n",
    "        customer_id,\n",
    "        transaction_date,\n",
    "        amount,\n",
    "        category,\n",
    "        region,\n",
    "        -- PERFORMANCE OPTIMIZED: Regional and category-based windows only\n",
    "        SUM(amount) OVER (\n",
    "            PARTITION BY region \n",
    "            ORDER BY transaction_date \n",
    "            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "        ) as running_total_by_region,\n",
    "        \n",
    "        SUM(amount) OVER (\n",
    "            PARTITION BY category \n",
    "            ORDER BY transaction_date \n",
    "            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "        ) as running_total_by_category,\n",
    "        \n",
    "        -- Moving averages by region (3-transaction window)\n",
    "        ROUND(AVG(amount) OVER (\n",
    "            PARTITION BY region \n",
    "            ORDER BY transaction_date \n",
    "            ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n",
    "        ), 2) as moving_avg_3_by_region,\n",
    "        \n",
    "        -- Count of transactions in regional window\n",
    "        COUNT(*) OVER (\n",
    "            PARTITION BY region \n",
    "            ORDER BY transaction_date \n",
    "            ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n",
    "        ) as transactions_in_window,\n",
    "        \n",
    "        -- Maximum amount in last 3 transactions by region\n",
    "        MAX(amount) OVER (\n",
    "            PARTITION BY region \n",
    "            ORDER BY transaction_date \n",
    "            ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n",
    "        ) as max_in_window,\n",
    "        \n",
    "        -- Transaction rank within region by date\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY region \n",
    "            ORDER BY transaction_date\n",
    "        ) as transaction_sequence_in_region\n",
    "        \n",
    "    FROM sales \n",
    "    ORDER BY transaction_date, transaction_id\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n📊 Running Totals and Moving Averages:\")\n",
    "running_totals_sql.show(15, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98bf9b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 1.6 Quartiles and Percentiles using NTILE Function\n",
      "\n",
      "📊 Quartile Distribution:\n",
      "+--------------+-----------+-----------+-------+------+--------+------+--------+\n",
      "|transaction_id|customer_id|category   |amount |region|quartile|decile|quintile|\n",
      "+--------------+-----------+-----------+-------+------+--------+------+--------+\n",
      "|12            |107        |Automotive |2345.0 |North |1       |1     |1       |\n",
      "|17            |101        |Electronics|2199.99|West  |1       |1     |1       |\n",
      "|9             |101        |Electronics|1599.99|West  |1       |2     |1       |\n",
      "|1             |101        |Electronics|1200.0 |West  |2       |3     |2       |\n",
      "|20            |110        |Jewelry    |899.99 |North |1       |2     |2       |\n",
      "|4             |101        |Electronics|899.99 |West  |2       |4     |3       |\n",
      "|19            |104        |Home       |567.89 |South |1       |1     |1       |\n",
      "|11            |104        |Home       |445.0  |South |2       |2     |2       |\n",
      "|14            |105        |Sports     |267.5  |West  |3       |5     |4       |\n",
      "|5             |104        |Home       |234.5  |South |3       |3     |3       |\n",
      "+--------------+-----------+-----------+-------+------+--------+------+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "📈 Quartile Analysis:\n",
      "+------+--------+-----------------+----------+----------+----------+------------+--------------------------+\n",
      "|region|quartile|transaction_count|min_amount|max_amount|avg_amount|total_amount|categories                |\n",
      "+------+--------+-----------------+----------+----------+----------+------------+--------------------------+\n",
      "|East  |1       |2                |78.99     |125.0     |102.0     |203.99      |[Toys, Beauty]            |\n",
      "|East  |2       |1                |67.89     |67.89     |67.89     |67.89       |[Books]                   |\n",
      "|East  |3       |1                |45.99     |45.99     |45.99     |45.99       |[Books]                   |\n",
      "|East  |4       |1                |34.99     |34.99     |34.99     |34.99       |[Books]                   |\n",
      "|North |1       |2                |899.99    |2345.0    |1622.5    |3244.99     |[Automotive, Jewelry]     |\n",
      "|North |2       |1                |123.45    |123.45    |123.45    |123.45      |[Clothing]                |\n",
      "|North |3       |1                |99.99     |99.99     |99.99     |99.99       |[Clothing]                |\n",
      "|North |4       |1                |89.99     |89.99     |89.99     |89.99       |[Clothing]                |\n",
      "|South |1       |1                |567.89    |567.89    |567.89    |567.89      |[Home]                    |\n",
      "|South |2       |1                |445.0     |445.0     |445.0     |445.0       |[Home]                    |\n",
      "|South |3       |1                |234.5     |234.5     |234.5     |234.5       |[Home]                    |\n",
      "|South |4       |1                |189.99    |189.99    |189.99    |189.99      |[Garden]                  |\n",
      "|West  |1       |2                |1599.99   |2199.99   |1899.99   |3799.98     |[Electronics, Electronics]|\n",
      "|West  |2       |2                |899.99    |1200.0    |1050.0    |2099.99     |[Electronics, Electronics]|\n",
      "|West  |3       |1                |267.5     |267.5     |267.5     |267.5       |[Sports]                  |\n",
      "|West  |4       |1                |156.75    |156.75    |156.75    |156.75      |[Sports]                  |\n",
      "+------+--------+-----------------+----------+----------+----------+------------+--------------------------+\n",
      "\n",
      "\n",
      "🎯 Customer Value Segmentation:\n",
      "+------+--------+-----------------+----------+----------+----------+------------+--------------------------+\n",
      "|region|quartile|transaction_count|min_amount|max_amount|avg_amount|total_amount|categories                |\n",
      "+------+--------+-----------------+----------+----------+----------+------------+--------------------------+\n",
      "|East  |1       |2                |78.99     |125.0     |102.0     |203.99      |[Toys, Beauty]            |\n",
      "|East  |2       |1                |67.89     |67.89     |67.89     |67.89       |[Books]                   |\n",
      "|East  |3       |1                |45.99     |45.99     |45.99     |45.99       |[Books]                   |\n",
      "|East  |4       |1                |34.99     |34.99     |34.99     |34.99       |[Books]                   |\n",
      "|North |1       |2                |899.99    |2345.0    |1622.5    |3244.99     |[Automotive, Jewelry]     |\n",
      "|North |2       |1                |123.45    |123.45    |123.45    |123.45      |[Clothing]                |\n",
      "|North |3       |1                |99.99     |99.99     |99.99     |99.99       |[Clothing]                |\n",
      "|North |4       |1                |89.99     |89.99     |89.99     |89.99       |[Clothing]                |\n",
      "|South |1       |1                |567.89    |567.89    |567.89    |567.89      |[Home]                    |\n",
      "|South |2       |1                |445.0     |445.0     |445.0     |445.0       |[Home]                    |\n",
      "|South |3       |1                |234.5     |234.5     |234.5     |234.5       |[Home]                    |\n",
      "|South |4       |1                |189.99    |189.99    |189.99    |189.99      |[Garden]                  |\n",
      "|West  |1       |2                |1599.99   |2199.99   |1899.99   |3799.98     |[Electronics, Electronics]|\n",
      "|West  |2       |2                |899.99    |1200.0    |1050.0    |2099.99     |[Electronics, Electronics]|\n",
      "|West  |3       |1                |267.5     |267.5     |267.5     |267.5       |[Sports]                  |\n",
      "|West  |4       |1                |156.75    |156.75    |156.75    |156.75      |[Sports]                  |\n",
      "+------+--------+-----------------+----------+----------+----------+------------+--------------------------+\n",
      "\n",
      "\n",
      "🎯 Customer Value Segmentation:\n",
      "+-----------+------+-----------+-----------------+---------------+------------+\n",
      "|customer_id|region|total_spent|transaction_count|avg_transaction|segment     |\n",
      "+-----------+------+-----------+-----------------+---------------+------------+\n",
      "|102        |East  |148.87     |3                |49.62          |High Value  |\n",
      "|109        |East  |125.0      |1                |125.0          |Medium Value|\n",
      "|106        |East  |78.99      |1                |78.99          |Low Value   |\n",
      "|107        |North |2345.0     |1                |2345.0         |High Value  |\n",
      "|110        |North |899.99     |1                |899.99         |Medium Value|\n",
      "|103        |North |313.43     |3                |104.48         |Low Value   |\n",
      "|104        |South |1247.39    |3                |415.8          |High Value  |\n",
      "|108        |South |189.99     |1                |189.99         |Medium Value|\n",
      "|101        |West  |5899.97    |4                |1474.99        |High Value  |\n",
      "|105        |West  |424.25     |2                |212.13         |Medium Value|\n",
      "+-----------+------+-----------+-----------------+---------------+------------+\n",
      "\n",
      "+-----------+------+-----------+-----------------+---------------+------------+\n",
      "|customer_id|region|total_spent|transaction_count|avg_transaction|segment     |\n",
      "+-----------+------+-----------+-----------------+---------------+------------+\n",
      "|102        |East  |148.87     |3                |49.62          |High Value  |\n",
      "|109        |East  |125.0      |1                |125.0          |Medium Value|\n",
      "|106        |East  |78.99      |1                |78.99          |Low Value   |\n",
      "|107        |North |2345.0     |1                |2345.0         |High Value  |\n",
      "|110        |North |899.99     |1                |899.99         |Medium Value|\n",
      "|103        |North |313.43     |3                |104.48         |Low Value   |\n",
      "|104        |South |1247.39    |3                |415.8          |High Value  |\n",
      "|108        |South |189.99     |1                |189.99         |Medium Value|\n",
      "|101        |West  |5899.97    |4                |1474.99        |High Value  |\n",
      "|105        |West  |424.25     |2                |212.13         |Medium Value|\n",
      "+-----------+------+-----------+-----------------+---------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quartiles and Percentiles with NTILE\n",
    "print(\"🎯 1.6 Quartiles and Percentiles using NTILE Function\")\n",
    "\n",
    "# Using DataFrame API for ntile\n",
    "from pyspark.sql.functions import ntile\n",
    "\n",
    "# PERFORMANCE OPTIMIZED: Partitioned window functions\n",
    "ntile_window_by_region = Window.partitionBy(\"region\").orderBy(desc(\"amount\"))\n",
    "quartile_df = sales_df.select(\n",
    "    \"transaction_id\",\n",
    "    \"customer_id\", \n",
    "    \"category\",\n",
    "    \"amount\",\n",
    "    \"region\",\n",
    "    ntile(4).over(ntile_window_by_region).alias(\"quartile\"),\n",
    "    ntile(10).over(ntile_window_by_region).alias(\"decile\"),\n",
    "    ntile(5).over(ntile_window_by_region).alias(\"quintile\")\n",
    ")\n",
    "\n",
    "print(\"\\n📊 Quartile Distribution:\")\n",
    "quartile_df.orderBy(desc(\"amount\")).show(10, truncate=False)\n",
    "\n",
    "# Analyze quartile characteristics\n",
    "print(\"\\n📈 Quartile Analysis:\")\n",
    "quartile_analysis = spark.sql(\"\"\"\n",
    "    WITH quartiled_sales AS (\n",
    "        SELECT \n",
    "            *,\n",
    "            NTILE(4) OVER (PARTITION BY region ORDER BY amount DESC) as quartile\n",
    "        FROM sales\n",
    "    )\n",
    "    SELECT \n",
    "        region,\n",
    "        quartile,\n",
    "        COUNT(*) as transaction_count,\n",
    "        ROUND(MIN(amount), 2) as min_amount,\n",
    "        ROUND(MAX(amount), 2) as max_amount,\n",
    "        ROUND(AVG(amount), 2) as avg_amount,\n",
    "        ROUND(SUM(amount), 2) as total_amount,\n",
    "        COLLECT_LIST(category) as categories\n",
    "    FROM quartiled_sales\n",
    "    GROUP BY region, quartile\n",
    "    ORDER BY region, quartile\n",
    "\"\"\")\n",
    "\n",
    "quartile_analysis.show(truncate=False)\n",
    "\n",
    "# Customer value segmentation\n",
    "print(\"\\n🎯 Customer Value Segmentation:\")\n",
    "customer_segmentation = spark.sql(\"\"\"\n",
    "    WITH customer_totals AS (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            region,\n",
    "            SUM(amount) as total_spent,\n",
    "            COUNT(*) as transaction_count,\n",
    "            AVG(amount) as avg_transaction\n",
    "        FROM sales\n",
    "        GROUP BY customer_id, region\n",
    "    ),\n",
    "    segmented_customers AS (\n",
    "        SELECT \n",
    "            *,\n",
    "            NTILE(3) OVER (PARTITION BY region ORDER BY total_spent DESC) as value_segment\n",
    "        FROM customer_totals\n",
    "    )\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        region,\n",
    "        ROUND(total_spent, 2) as total_spent,\n",
    "        transaction_count,\n",
    "        ROUND(avg_transaction, 2) as avg_transaction,\n",
    "        CASE value_segment\n",
    "            WHEN 1 THEN 'High Value'\n",
    "            WHEN 2 THEN 'Medium Value' \n",
    "            WHEN 3 THEN 'Low Value'\n",
    "        END as segment\n",
    "    FROM segmented_customers\n",
    "    ORDER BY region, total_spent DESC\n",
    "\"\"\")\n",
    "\n",
    "customer_segmentation.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe8ea62",
   "metadata": {},
   "source": [
    "## 🗃️ Section 2: Advanced SQL Patterns\n",
    "\n",
    "Advanced SQL patterns enable complex data analysis through sophisticated query structures. These patterns are essential for building robust analytics pipelines and handling complex business logic.\n",
    "\n",
    "### 🎯 **Key Patterns:**\n",
    "- **Common Table Expressions (CTEs)**: Recursive and non-recursive\n",
    "- **Subqueries**: Correlated and non-correlated patterns\n",
    "- **Complex JOINs**: Self-joins, multiple joins, and join optimization\n",
    "- **CASE Statements**: Complex conditional logic\n",
    "- **Set Operations**: UNION, INTERSECT, EXCEPT operations\n",
    "- **Analytical Functions**: Advanced aggregations and calculations\n",
    "\n",
    "### 📊 **Business Applications:**\n",
    "- Hierarchical data analysis and reporting\n",
    "- Complex business rule implementation  \n",
    "- Data quality assessment and validation\n",
    "- Multi-dimensional analytics and pivoting\n",
    "- Performance optimization for large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9756c2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 2.1 Common Table Expressions (CTEs) for Complex Analysis\n",
      "\n",
      "📊 Comprehensive Customer Analysis using CTEs:\n",
      "+-----------+-------------+------------+-------------+-----------------+-----------------+-----------+---------------+--------------------+----------------------+--------------+-------------+------+-----------------------+\n",
      "|customer_id|name         |loyalty_tier|spending_tier|purchase_behavior|transaction_count|total_spent|avg_transaction|categories_purchased|customer_lifetime_days|first_purchase|last_purchase|region|pct_of_regional_revenue|\n",
      "+-----------+-------------+------------+-------------+-----------------+-----------------+-----------+---------------+--------------------+----------------------+--------------+-------------+------+-----------------------+\n",
      "|101        |Alice Johnson|Premium     |VIP          |Single-Category  |4                |5899.97    |1474.99        |1                   |168                   |2024-01-15    |2024-07-01   |West  |93.29                  |\n",
      "|107        |Grace Lee    |Gold        |VIP          |Single-Category  |1                |2345.0     |2345.0         |1                   |0                     |2024-05-01    |2024-05-01   |North |65.9                   |\n",
      "|104        |David Wilson |Gold        |Premium      |Single-Category  |3                |1247.39    |415.8          |1                   |154                   |2024-02-12    |2024-07-15   |South |86.78                  |\n",
      "|110        |Jack Anderson|Gold        |Standard     |Single-Category  |1                |899.99     |899.99         |1                   |0                     |2024-08-01    |2024-08-01   |North |25.29                  |\n",
      "|105        |Emma Brown   |Standard    |Basic        |Single-Category  |2                |424.25     |212.13         |1                   |80                    |2024-03-01    |2024-05-20   |West  |6.71                   |\n",
      "|103        |Carol Davis  |Premium     |Basic        |Single-Category  |3                |313.43     |104.48         |1                   |144                   |2024-01-18    |2024-06-10   |North |8.81                   |\n",
      "|108        |Henry Taylor |Standard    |Basic        |Single-Category  |1                |189.99     |189.99         |1                   |0                     |2024-06-01    |2024-06-01   |South |13.22                  |\n",
      "|102        |Bob Smith    |Standard    |Basic        |Single-Category  |3                |148.87     |49.62          |1                   |120                   |2024-01-16    |2024-05-15   |East  |42.19                  |\n",
      "|109        |Ivy Chen     |Premium     |Basic        |Single-Category  |1                |125.0      |125.0          |1                   |0                     |2024-07-05    |2024-07-05   |East  |35.42                  |\n",
      "|106        |Frank Miller |Premium     |Basic        |Single-Category  |1                |78.99      |78.99          |1                   |0                     |2024-04-05    |2024-04-05   |East  |22.39                  |\n",
      "+-----------+-------------+------------+-------------+-----------------+-----------------+-----------+---------------+--------------------+----------------------+--------------+-------------+------+-----------------------+\n",
      "\n",
      "+-----------+-------------+------------+-------------+-----------------+-----------------+-----------+---------------+--------------------+----------------------+--------------+-------------+------+-----------------------+\n",
      "|customer_id|name         |loyalty_tier|spending_tier|purchase_behavior|transaction_count|total_spent|avg_transaction|categories_purchased|customer_lifetime_days|first_purchase|last_purchase|region|pct_of_regional_revenue|\n",
      "+-----------+-------------+------------+-------------+-----------------+-----------------+-----------+---------------+--------------------+----------------------+--------------+-------------+------+-----------------------+\n",
      "|101        |Alice Johnson|Premium     |VIP          |Single-Category  |4                |5899.97    |1474.99        |1                   |168                   |2024-01-15    |2024-07-01   |West  |93.29                  |\n",
      "|107        |Grace Lee    |Gold        |VIP          |Single-Category  |1                |2345.0     |2345.0         |1                   |0                     |2024-05-01    |2024-05-01   |North |65.9                   |\n",
      "|104        |David Wilson |Gold        |Premium      |Single-Category  |3                |1247.39    |415.8          |1                   |154                   |2024-02-12    |2024-07-15   |South |86.78                  |\n",
      "|110        |Jack Anderson|Gold        |Standard     |Single-Category  |1                |899.99     |899.99         |1                   |0                     |2024-08-01    |2024-08-01   |North |25.29                  |\n",
      "|105        |Emma Brown   |Standard    |Basic        |Single-Category  |2                |424.25     |212.13         |1                   |80                    |2024-03-01    |2024-05-20   |West  |6.71                   |\n",
      "|103        |Carol Davis  |Premium     |Basic        |Single-Category  |3                |313.43     |104.48         |1                   |144                   |2024-01-18    |2024-06-10   |North |8.81                   |\n",
      "|108        |Henry Taylor |Standard    |Basic        |Single-Category  |1                |189.99     |189.99         |1                   |0                     |2024-06-01    |2024-06-01   |South |13.22                  |\n",
      "|102        |Bob Smith    |Standard    |Basic        |Single-Category  |3                |148.87     |49.62          |1                   |120                   |2024-01-16    |2024-05-15   |East  |42.19                  |\n",
      "|109        |Ivy Chen     |Premium     |Basic        |Single-Category  |1                |125.0      |125.0          |1                   |0                     |2024-07-05    |2024-07-05   |East  |35.42                  |\n",
      "|106        |Frank Miller |Premium     |Basic        |Single-Category  |1                |78.99      |78.99          |1                   |0                     |2024-04-05    |2024-04-05   |East  |22.39                  |\n",
      "+-----------+-------------+------------+-------------+-----------------+-----------------+-----------+---------------+--------------------+----------------------+--------------+-------------+------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Common Table Expressions (CTEs)\n",
    "print(\"🎯 2.1 Common Table Expressions (CTEs) for Complex Analysis\")\n",
    "\n",
    "# Multiple CTEs for comprehensive customer analysis\n",
    "comprehensive_cte = spark.sql(\"\"\"\n",
    "    WITH customer_metrics AS (\n",
    "        -- Customer transaction metrics\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            COUNT(*) as transaction_count,\n",
    "            SUM(amount) as total_spent,\n",
    "            AVG(amount) as avg_transaction,\n",
    "            MIN(transaction_date) as first_purchase,\n",
    "            MAX(transaction_date) as last_purchase,\n",
    "            COUNT(DISTINCT category) as categories_purchased,\n",
    "            COLLECT_SET(category) as category_list\n",
    "        FROM sales\n",
    "        GROUP BY customer_id\n",
    "    ),\n",
    "    \n",
    "    customer_segments AS (\n",
    "        -- Segment customers based on behavior\n",
    "        SELECT \n",
    "            *,\n",
    "            DATEDIFF(last_purchase, first_purchase) as customer_lifetime_days,\n",
    "            CASE \n",
    "                WHEN total_spent >= 2000 THEN 'VIP'\n",
    "                WHEN total_spent >= 1000 THEN 'Premium'\n",
    "                WHEN total_spent >= 500 THEN 'Standard'\n",
    "                ELSE 'Basic'\n",
    "            END as spending_tier,\n",
    "            CASE \n",
    "                WHEN categories_purchased >= 3 THEN 'Diversified'\n",
    "                WHEN categories_purchased = 2 THEN 'Focused' \n",
    "                ELSE 'Single-Category'\n",
    "            END as purchase_behavior\n",
    "        FROM customer_metrics\n",
    "    ),\n",
    "    \n",
    "    regional_performance AS (\n",
    "        -- Regional sales performance\n",
    "        SELECT \n",
    "            region,\n",
    "            COUNT(*) as regional_transactions,\n",
    "            SUM(amount) as regional_revenue,\n",
    "            AVG(amount) as regional_avg_transaction,\n",
    "            COUNT(DISTINCT customer_id) as unique_customers\n",
    "        FROM sales\n",
    "        GROUP BY region\n",
    "    )\n",
    "    \n",
    "    -- Final comprehensive report\n",
    "    SELECT \n",
    "        cs.customer_id,\n",
    "        c.name,\n",
    "        c.tier as loyalty_tier,\n",
    "        cs.spending_tier,\n",
    "        cs.purchase_behavior,\n",
    "        cs.transaction_count,\n",
    "        ROUND(cs.total_spent, 2) as total_spent,\n",
    "        ROUND(cs.avg_transaction, 2) as avg_transaction,\n",
    "        cs.categories_purchased,\n",
    "        cs.customer_lifetime_days,\n",
    "        cs.first_purchase,\n",
    "        cs.last_purchase,\n",
    "        -- Join with regional data\n",
    "        rp.region,\n",
    "        ROUND(cs.total_spent / rp.regional_revenue * 100, 2) as pct_of_regional_revenue\n",
    "    FROM customer_segments cs\n",
    "    JOIN customers c ON cs.customer_id = c.customer_id  \n",
    "    JOIN (SELECT DISTINCT customer_id, region FROM sales) s ON cs.customer_id = s.customer_id\n",
    "    JOIN regional_performance rp ON s.region = rp.region\n",
    "    ORDER BY cs.total_spent DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n📊 Comprehensive Customer Analysis using CTEs:\")\n",
    "comprehensive_cte.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcde259c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 2.2 Complex Subqueries and Correlated Queries\n",
      "\n",
      "📊 Customers Above Regional Average:\n",
      "+-----------+-------------+------+--------------+----------------+------------------+---------------------+------------------+\n",
      "|customer_id|name         |region|customer_total|regional_average|above_regional_avg|customer_transactions|regional_customers|\n",
      "+-----------+-------------+------+--------------+----------------+------------------+---------------------+------------------+\n",
      "|101        |Alice Johnson|West  |5899.97       |3162.11         |2737.86           |4                    |2                 |\n",
      "|107        |Grace Lee    |North |2345.0        |1186.14         |1158.86           |1                    |3                 |\n",
      "|104        |David Wilson |South |1247.39       |718.69          |528.7             |3                    |2                 |\n",
      "|102        |Bob Smith    |East  |148.87        |117.62          |31.25             |3                    |3                 |\n",
      "|109        |Ivy Chen     |East  |125.0         |117.62          |7.38              |1                    |3                 |\n",
      "+-----------+-------------+------+--------------+----------------+------------------+---------------------+------------------+\n",
      "\n",
      "\n",
      "🎯 2.3 Advanced EXISTS and NOT EXISTS Patterns\n",
      "\n",
      "📊 Customers with Multi-Category Purchases:\n",
      "+-----------+-------------+------+--------------+----------------+------------------+---------------------+------------------+\n",
      "|customer_id|name         |region|customer_total|regional_average|above_regional_avg|customer_transactions|regional_customers|\n",
      "+-----------+-------------+------+--------------+----------------+------------------+---------------------+------------------+\n",
      "|101        |Alice Johnson|West  |5899.97       |3162.11         |2737.86           |4                    |2                 |\n",
      "|107        |Grace Lee    |North |2345.0        |1186.14         |1158.86           |1                    |3                 |\n",
      "|104        |David Wilson |South |1247.39       |718.69          |528.7             |3                    |2                 |\n",
      "|102        |Bob Smith    |East  |148.87        |117.62          |31.25             |3                    |3                 |\n",
      "|109        |Ivy Chen     |East  |125.0         |117.62          |7.38              |1                    |3                 |\n",
      "+-----------+-------------+------+--------------+----------------+------------------+---------------------+------------------+\n",
      "\n",
      "\n",
      "🎯 2.3 Advanced EXISTS and NOT EXISTS Patterns\n",
      "\n",
      "📊 Customers with Multi-Category Purchases:\n",
      "+-----------+----+----+----------------+\n",
      "|customer_id|name|tier|categories_count|\n",
      "+-----------+----+----+----------------+\n",
      "+-----------+----+----+----------------+\n",
      "\n",
      "\n",
      "🎯 2.4 Set Operations and Advanced Filtering\n",
      "\n",
      "📈 Customer Segmentation using Set Operations:\n",
      "+-----------+----+----+----------------+\n",
      "|customer_id|name|tier|categories_count|\n",
      "+-----------+----+----+----------------+\n",
      "+-----------+----+----+----------------+\n",
      "\n",
      "\n",
      "🎯 2.4 Set Operations and Advanced Filtering\n",
      "\n",
      "📈 Customer Segmentation using Set Operations:\n",
      "+-----------+--------------+------------------+\n",
      "|customer_id|category      |total             |\n",
      "+-----------+--------------+------------------+\n",
      "|101        |Frequent Buyer|4.0               |\n",
      "|101        |High Spender  |5899.969999999999 |\n",
      "|102        |Frequent Buyer|3.0               |\n",
      "|103        |Frequent Buyer|3.0               |\n",
      "|104        |Frequent Buyer|3.0               |\n",
      "|104        |High Spender  |1247.3899999999999|\n",
      "|107        |High Spender  |2345.0            |\n",
      "+-----------+--------------+------------------+\n",
      "\n",
      "+-----------+--------------+------------------+\n",
      "|customer_id|category      |total             |\n",
      "+-----------+--------------+------------------+\n",
      "|101        |Frequent Buyer|4.0               |\n",
      "|101        |High Spender  |5899.969999999999 |\n",
      "|102        |Frequent Buyer|3.0               |\n",
      "|103        |Frequent Buyer|3.0               |\n",
      "|104        |Frequent Buyer|3.0               |\n",
      "|104        |High Spender  |1247.3899999999999|\n",
      "|107        |High Spender  |2345.0            |\n",
      "+-----------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Complex Subqueries and Correlated Queries\n",
    "print(\"🎯 2.2 Complex Subqueries and Correlated Queries\")\n",
    "\n",
    "# Subquery examples for advanced analytics using CTEs\n",
    "subquery_analysis = spark.sql(\"\"\"\n",
    "    WITH regional_stats AS (\n",
    "        SELECT \n",
    "            region,\n",
    "            AVG(customer_total) as regional_avg,\n",
    "            COUNT(*) as regional_customers\n",
    "        FROM (\n",
    "            SELECT \n",
    "                customer_id,\n",
    "                region,\n",
    "                SUM(amount) as customer_total\n",
    "            FROM sales\n",
    "            GROUP BY customer_id, region\n",
    "        ) customer_totals\n",
    "        GROUP BY region\n",
    "    ),\n",
    "    customer_totals AS (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            region,\n",
    "            SUM(amount) as customer_total,\n",
    "            COUNT(*) as customer_transactions\n",
    "        FROM sales\n",
    "        GROUP BY customer_id, region\n",
    "    )\n",
    "    \n",
    "    SELECT \n",
    "        ct.customer_id,\n",
    "        c.name,\n",
    "        ct.region,\n",
    "        ROUND(ct.customer_total, 2) as customer_total,\n",
    "        ROUND(rs.regional_avg, 2) as regional_average,\n",
    "        ROUND(ct.customer_total - rs.regional_avg, 2) as above_regional_avg,\n",
    "        ct.customer_transactions,\n",
    "        rs.regional_customers\n",
    "    FROM customer_totals ct\n",
    "    JOIN regional_stats rs ON ct.region = rs.region\n",
    "    JOIN customers c ON ct.customer_id = c.customer_id\n",
    "    WHERE ct.customer_total > rs.regional_avg\n",
    "    ORDER BY above_regional_avg DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n📊 Customers Above Regional Average:\")\n",
    "subquery_analysis.show(truncate=False)\n",
    "\n",
    "print(\"\\n🎯 2.3 Advanced EXISTS and NOT EXISTS Patterns\")\n",
    "\n",
    "# Demonstrate EXISTS patterns instead of complex correlated queries\n",
    "exists_patterns = spark.sql(\"\"\"\n",
    "    -- Find customers who have purchased in multiple categories\n",
    "    SELECT DISTINCT\n",
    "        c.customer_id,\n",
    "        c.name,\n",
    "        c.tier,\n",
    "        COUNT(DISTINCT s.category) as categories_count\n",
    "    FROM customers c\n",
    "    JOIN sales s ON c.customer_id = s.customer_id\n",
    "    WHERE EXISTS (\n",
    "        SELECT 1 \n",
    "        FROM sales s2 \n",
    "        WHERE s2.customer_id = c.customer_id \n",
    "        AND s2.category != s.category\n",
    "    )\n",
    "    GROUP BY c.customer_id, c.name, c.tier\n",
    "    ORDER BY categories_count DESC, c.customer_id\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n📊 Customers with Multi-Category Purchases:\")\n",
    "exists_patterns.show(truncate=False)\n",
    "\n",
    "# IN vs EXISTS performance comparison example\n",
    "print(\"\\n🎯 2.4 Set Operations and Advanced Filtering\")\n",
    "\n",
    "# Set operations example\n",
    "set_operations = spark.sql(\"\"\"\n",
    "    -- Customers who spent more than $1000\n",
    "    SELECT customer_id, 'High Spender' as category, SUM(amount) as total\n",
    "    FROM sales \n",
    "    GROUP BY customer_id\n",
    "    HAVING SUM(amount) > 1000\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    -- Customers with more than 2 transactions\n",
    "    SELECT customer_id, 'Frequent Buyer' as category, COUNT(*) as total\n",
    "    FROM sales\n",
    "    GROUP BY customer_id  \n",
    "    HAVING COUNT(*) > 2\n",
    "    \n",
    "    ORDER BY customer_id, category\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n📈 Customer Segmentation using Set Operations:\")\n",
    "set_operations.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bed53454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 2.5 Complex CASE Statements and Conditional Logic\n",
      "\n",
      "📊 Advanced Customer Classification:\n",
      "+-----------+-------------+--------+---+-----------------+-----------+---------------+----------------+-------------+----------------------+-------------+---------------+------------------+----------+-------------+\n",
      "|customer_id|name         |tier    |age|transaction_count|total_spent|avg_transaction|categories_count|regions_count|customer_lifetime_days|business_tier|lifecycle_stage|purchase_behavior |generation|risk_category|\n",
      "+-----------+-------------+--------+---+-----------------+-----------+---------------+----------------+-------------+----------------------+-------------+---------------+------------------+----------+-------------+\n",
      "|101        |Alice Johnson|Premium |32 |4                |5899.97    |1474.99        |1               |1            |168                   |Platinum Plus|Lost           |Focused High Value|Millennial|High Risk    |\n",
      "|107        |Grace Lee    |Gold    |29 |1                |2345.0     |2345.0         |1               |1            |0                     |Silver Active|Lost           |Focused High Value|Millennial|High Risk    |\n",
      "|104        |David Wilson |Gold    |52 |3                |1247.39    |415.8          |1               |1            |154                   |Silver Active|Lost           |Frequent Focused  |Boomer+   |High Risk    |\n",
      "|110        |Jack Anderson|Gold    |47 |1                |899.99     |899.99         |1               |1            |0                     |Bronze       |Lost           |Occasional Buyer  |Gen X     |High Risk    |\n",
      "|105        |Emma Brown   |Standard|36 |2                |424.25     |212.13         |1               |1            |80                    |Basic        |Lost           |Occasional Buyer  |Gen X     |High Risk    |\n",
      "|103        |Carol Davis  |Premium |28 |3                |313.43     |104.48         |1               |1            |144                   |Silver Active|Lost           |Frequent Focused  |Millennial|High Risk    |\n",
      "|108        |Henry Taylor |Standard|38 |1                |189.99     |189.99         |1               |1            |0                     |Basic        |Lost           |Occasional Buyer  |Gen X     |High Risk    |\n",
      "|102        |Bob Smith    |Standard|45 |3                |148.87     |49.62          |1               |1            |120                   |Silver Active|Lost           |Frequent Focused  |Gen X     |High Risk    |\n",
      "|109        |Ivy Chen     |Premium |33 |1                |125.0      |125.0          |1               |1            |0                     |Basic        |Lost           |Occasional Buyer  |Millennial|High Risk    |\n",
      "|106        |Frank Miller |Premium |41 |1                |78.99      |78.99          |1               |1            |0                     |Basic        |Lost           |Occasional Buyer  |Gen X     |High Risk    |\n",
      "+-----------+-------------+--------+---+-----------------+-----------+---------------+----------------+-------------+----------------------+-------------+---------------+------------------+----------+-------------+\n",
      "\n",
      "+-----------+-------------+--------+---+-----------------+-----------+---------------+----------------+-------------+----------------------+-------------+---------------+------------------+----------+-------------+\n",
      "|customer_id|name         |tier    |age|transaction_count|total_spent|avg_transaction|categories_count|regions_count|customer_lifetime_days|business_tier|lifecycle_stage|purchase_behavior |generation|risk_category|\n",
      "+-----------+-------------+--------+---+-----------------+-----------+---------------+----------------+-------------+----------------------+-------------+---------------+------------------+----------+-------------+\n",
      "|101        |Alice Johnson|Premium |32 |4                |5899.97    |1474.99        |1               |1            |168                   |Platinum Plus|Lost           |Focused High Value|Millennial|High Risk    |\n",
      "|107        |Grace Lee    |Gold    |29 |1                |2345.0     |2345.0         |1               |1            |0                     |Silver Active|Lost           |Focused High Value|Millennial|High Risk    |\n",
      "|104        |David Wilson |Gold    |52 |3                |1247.39    |415.8          |1               |1            |154                   |Silver Active|Lost           |Frequent Focused  |Boomer+   |High Risk    |\n",
      "|110        |Jack Anderson|Gold    |47 |1                |899.99     |899.99         |1               |1            |0                     |Bronze       |Lost           |Occasional Buyer  |Gen X     |High Risk    |\n",
      "|105        |Emma Brown   |Standard|36 |2                |424.25     |212.13         |1               |1            |80                    |Basic        |Lost           |Occasional Buyer  |Gen X     |High Risk    |\n",
      "|103        |Carol Davis  |Premium |28 |3                |313.43     |104.48         |1               |1            |144                   |Silver Active|Lost           |Frequent Focused  |Millennial|High Risk    |\n",
      "|108        |Henry Taylor |Standard|38 |1                |189.99     |189.99         |1               |1            |0                     |Basic        |Lost           |Occasional Buyer  |Gen X     |High Risk    |\n",
      "|102        |Bob Smith    |Standard|45 |3                |148.87     |49.62          |1               |1            |120                   |Silver Active|Lost           |Frequent Focused  |Gen X     |High Risk    |\n",
      "|109        |Ivy Chen     |Premium |33 |1                |125.0      |125.0          |1               |1            |0                     |Basic        |Lost           |Occasional Buyer  |Millennial|High Risk    |\n",
      "|106        |Frank Miller |Premium |41 |1                |78.99      |78.99          |1               |1            |0                     |Basic        |Lost           |Occasional Buyer  |Gen X     |High Risk    |\n",
      "+-----------+-------------+--------+---+-----------------+-----------+---------------+----------------+-------------+----------------------+-------------+---------------+------------------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Complex CASE Statements and Conditional Logic\n",
    "print(\"🎯 2.5 Complex CASE Statements and Conditional Logic\")\n",
    "\n",
    "# Advanced CASE statements for business logic\n",
    "complex_case_logic = spark.sql(\"\"\"\n",
    "    WITH customer_analysis AS (\n",
    "        SELECT \n",
    "            s.customer_id,\n",
    "            c.name,\n",
    "            c.tier,\n",
    "            c.age,\n",
    "            COUNT(*) as transaction_count,\n",
    "            SUM(s.amount) as total_spent,\n",
    "            AVG(s.amount) as avg_transaction,\n",
    "            MIN(s.transaction_date) as first_purchase,\n",
    "            MAX(s.transaction_date) as last_purchase,\n",
    "            COUNT(DISTINCT s.category) as categories_count,\n",
    "            COUNT(DISTINCT s.region) as regions_count\n",
    "        FROM sales s\n",
    "        JOIN customers c ON s.customer_id = c.customer_id\n",
    "        GROUP BY s.customer_id, c.name, c.tier, c.age\n",
    "    )\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        name,\n",
    "        tier,\n",
    "        age,\n",
    "        transaction_count,\n",
    "        ROUND(total_spent, 2) as total_spent,\n",
    "        ROUND(avg_transaction, 2) as avg_transaction,\n",
    "        categories_count,\n",
    "        regions_count,\n",
    "        DATEDIFF(last_purchase, first_purchase) as customer_lifetime_days,\n",
    "        \n",
    "        -- Complex business tier assignment\n",
    "        CASE \n",
    "            WHEN total_spent >= 5000 AND transaction_count >= 3 THEN 'Platinum Plus'\n",
    "            WHEN total_spent >= 2000 AND categories_count >= 2 THEN 'Gold Elite' \n",
    "            WHEN total_spent >= 1000 OR transaction_count >= 3 THEN 'Silver Active'\n",
    "            WHEN total_spent >= 500 THEN 'Bronze'\n",
    "            ELSE 'Basic'\n",
    "        END as business_tier,\n",
    "        \n",
    "        -- Customer lifecycle stage\n",
    "        CASE\n",
    "            WHEN DATEDIFF(CURRENT_DATE(), last_purchase) <= 30 THEN 'Active'\n",
    "            WHEN DATEDIFF(CURRENT_DATE(), last_purchase) <= 90 THEN 'At Risk'\n",
    "            WHEN DATEDIFF(CURRENT_DATE(), last_purchase) <= 180 THEN 'Dormant'\n",
    "            ELSE 'Lost'\n",
    "        END as lifecycle_stage,\n",
    "        \n",
    "        -- Purchase behavior classification\n",
    "        CASE \n",
    "            WHEN categories_count >= 3 AND avg_transaction >= 500 THEN 'Diversified High Value'\n",
    "            WHEN categories_count >= 3 THEN 'Diversified Explorer' \n",
    "            WHEN avg_transaction >= 1000 THEN 'Focused High Value'\n",
    "            WHEN transaction_count >= 3 THEN 'Frequent Focused'\n",
    "            ELSE 'Occasional Buyer'\n",
    "        END as purchase_behavior,\n",
    "        \n",
    "        -- Age-based segmentation\n",
    "        CASE \n",
    "            WHEN age BETWEEN 18 AND 25 THEN 'Gen Z'\n",
    "            WHEN age BETWEEN 26 AND 35 THEN 'Millennial'\n",
    "            WHEN age BETWEEN 36 AND 50 THEN 'Gen X'\n",
    "            WHEN age > 50 THEN 'Boomer+'\n",
    "            ELSE 'Unknown'\n",
    "        END as generation,\n",
    "        \n",
    "        -- Risk assessment\n",
    "        CASE \n",
    "            WHEN total_spent >= 3000 AND DATEDIFF(CURRENT_DATE(), last_purchase) <= 45 THEN 'Low Risk'\n",
    "            WHEN total_spent >= 1000 AND DATEDIFF(CURRENT_DATE(), last_purchase) <= 90 THEN 'Medium Risk'\n",
    "            WHEN DATEDIFF(CURRENT_DATE(), last_purchase) > 180 THEN 'High Risk'\n",
    "            ELSE 'Monitoring'\n",
    "        END as risk_category\n",
    "        \n",
    "    FROM customer_analysis\n",
    "    ORDER BY total_spent DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n📊 Advanced Customer Classification:\")\n",
    "complex_case_logic.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16bd78a",
   "metadata": {},
   "source": [
    "## 📊 Section 3: Statistical & Mathematical Operations\n",
    "\n",
    "PySpark provides extensive statistical and mathematical functions for advanced analytics. These operations are essential for data science workflows, quality assessment, and business intelligence applications.\n",
    "\n",
    "### 🎯 **Statistical Functions:**\n",
    "- **Descriptive Statistics**: Mean, median, mode, standard deviation\n",
    "- **Distribution Analysis**: Skewness, kurtosis, percentiles\n",
    "- **Correlation Analysis**: Pearson and Spearman correlation\n",
    "- **Covariance**: Population and sample covariance calculations\n",
    "- **Sampling**: Random sampling and stratified sampling\n",
    "\n",
    "### 📐 **Mathematical Operations:**\n",
    "- **Aggregation Functions**: Advanced sum, count, average calculations\n",
    "- **Mathematical Functions**: Trigonometric, logarithmic, exponential\n",
    "- **Rounding and Precision**: ROUND, CEIL, FLOOR functions\n",
    "- **Null Handling**: COALESCE, ISNULL, NULLIF patterns\n",
    "- **Type Conversions**: CAST and implicit conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d290654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 3.1 Comprehensive Statistical Analysis\n",
      "\n",
      "📊 Statistical Summary by Category:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/25 21:13:37 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+----------------+-------------+-----------+-------------+-------------+---------+----------+----------+--------+--------+------+-------+\n",
      "|category   |transaction_count|unique_customers|total_revenue|mean_amount|median_amount|std_deviation|variance |min_amount|max_amount|skewness|kurtosis|q1    |q3     |\n",
      "+-----------+-----------------+----------------+-------------+-----------+-------------+-------------+---------+----------+----------+--------+--------+------+-------+\n",
      "|Overall    |20               |10              |11672.88     |583.64     |189.99       |722.46       |521946.99|34.99     |2345.0    |1.4041  |0.7169  |89.99 |899.99 |\n",
      "|Electronics|4                |1               |5899.97      |1474.99    |1200.0       |561.99       |315831.5 |899.99    |2199.99   |0.3732  |-1.2554 |899.99|1599.99|\n",
      "|Automotive |1                |1               |2345.0       |2345.0     |2345.0       |NULL         |NULL     |2345.0    |2345.0    |NULL    |NULL    |2345.0|2345.0 |\n",
      "|Home       |3                |1               |1247.39      |415.8      |445.0        |168.6        |28426.85 |234.5     |567.89    |-0.3087 |-1.5    |234.5 |567.89 |\n",
      "|Jewelry    |1                |1               |899.99       |899.99     |899.99       |NULL         |NULL     |899.99    |899.99    |NULL    |NULL    |899.99|899.99 |\n",
      "|Sports     |2                |1               |424.25       |212.13     |156.75       |78.31        |6132.78  |156.75    |267.5     |0.0     |-2.0    |156.75|267.5  |\n",
      "|Clothing   |3                |1               |313.43       |104.48     |99.99        |17.18        |294.99   |89.99     |123.45    |0.4472  |-1.5    |89.99 |123.45 |\n",
      "|Garden     |1                |1               |189.99       |189.99     |189.99       |NULL         |NULL     |189.99    |189.99    |NULL    |NULL    |189.99|189.99 |\n",
      "|Books      |3                |1               |148.87       |49.62      |45.99        |16.75        |280.5    |34.99     |67.89     |0.3798  |-1.5    |34.99 |67.89  |\n",
      "|Toys       |1                |1               |125.0        |125.0      |125.0        |NULL         |NULL     |125.0     |125.0     |NULL    |NULL    |125.0 |125.0  |\n",
      "|Beauty     |1                |1               |78.99        |78.99      |78.99        |NULL         |NULL     |78.99     |78.99     |NULL    |NULL    |78.99 |78.99  |\n",
      "+-----------+-----------------+----------------+-------------+-----------+-------------+-------------+---------+----------+----------+--------+--------+------+-------+\n",
      "\n",
      "\n",
      "🎯 3.2 Statistical Analysis using DataFrame API\n",
      "\n",
      "📈 DataFrame describe() method:\n",
      "summary            amount\n",
      "  count                20\n",
      "   mean           583.644\n",
      " stddev 722.4589915690353\n",
      "    min             34.99\n",
      "    max            2345.0\n",
      "\n",
      "🎯 3.3 Advanced Statistical Functions\n",
      "\n",
      "📊 Advanced Statistical Measures:\n",
      "\n",
      "📈 DataFrame describe() method:\n",
      "summary            amount\n",
      "  count                20\n",
      "   mean           583.644\n",
      " stddev 722.4589915690353\n",
      "    min             34.99\n",
      "    max            2345.0\n",
      "\n",
      "🎯 3.3 Advanced Statistical Functions\n",
      "\n",
      "📊 Advanced Statistical Measures:\n",
      "   count: <built-in method count of Row object at 0x1190495e0>\n",
      "   total: 11672.8800\n",
      "   mean: 583.6440\n",
      "   stddev: 722.4590\n",
      "   stddev_pop: 704.1659\n",
      "   stddev_samp: 722.4590\n",
      "   variance: 521946.9945\n",
      "   var_pop: 495849.6448\n",
      "   var_samp: 521946.9945\n",
      "   skewness: 1.4041\n",
      "   kurtosis: 0.7169\n",
      "   min: 34.9900\n",
      "   max: 2345.0000\n",
      "   count: <built-in method count of Row object at 0x1190495e0>\n",
      "   total: 11672.8800\n",
      "   mean: 583.6440\n",
      "   stddev: 722.4590\n",
      "   stddev_pop: 704.1659\n",
      "   stddev_samp: 722.4590\n",
      "   variance: 521946.9945\n",
      "   var_pop: 495849.6448\n",
      "   var_samp: 521946.9945\n",
      "   skewness: 1.4041\n",
      "   kurtosis: 0.7169\n",
      "   min: 34.9900\n",
      "   max: 2345.0000\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Statistical Analysis\n",
    "print(\"🎯 3.1 Comprehensive Statistical Analysis\")\n",
    "\n",
    "# Descriptive statistics using both DataFrame API and SQL\n",
    "stats_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        'Overall' as category,\n",
    "        COUNT(*) as transaction_count,\n",
    "        COUNT(DISTINCT customer_id) as unique_customers,\n",
    "        ROUND(SUM(amount), 2) as total_revenue,\n",
    "        ROUND(AVG(amount), 2) as mean_amount,\n",
    "        ROUND(PERCENTILE_APPROX(amount, 0.5), 2) as median_amount,\n",
    "        ROUND(STDDEV(amount), 2) as std_deviation,\n",
    "        ROUND(VARIANCE(amount), 2) as variance,\n",
    "        ROUND(MIN(amount), 2) as min_amount,\n",
    "        ROUND(MAX(amount), 2) as max_amount,\n",
    "        ROUND(SKEWNESS(amount), 4) as skewness,\n",
    "        ROUND(KURTOSIS(amount), 4) as kurtosis,\n",
    "        ROUND(PERCENTILE_APPROX(amount, 0.25), 2) as q1,\n",
    "        ROUND(PERCENTILE_APPROX(amount, 0.75), 2) as q3\n",
    "    FROM sales\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        category,\n",
    "        COUNT(*) as transaction_count,\n",
    "        COUNT(DISTINCT customer_id) as unique_customers,\n",
    "        ROUND(SUM(amount), 2) as total_revenue,\n",
    "        ROUND(AVG(amount), 2) as mean_amount,\n",
    "        ROUND(PERCENTILE_APPROX(amount, 0.5), 2) as median_amount,\n",
    "        ROUND(STDDEV(amount), 2) as std_deviation,\n",
    "        ROUND(VARIANCE(amount), 2) as variance,\n",
    "        ROUND(MIN(amount), 2) as min_amount,\n",
    "        ROUND(MAX(amount), 2) as max_amount,\n",
    "        ROUND(SKEWNESS(amount), 4) as skewness,\n",
    "        ROUND(KURTOSIS(amount), 4) as kurtosis,\n",
    "        ROUND(PERCENTILE_APPROX(amount, 0.25), 2) as q1,\n",
    "        ROUND(PERCENTILE_APPROX(amount, 0.75), 2) as q3\n",
    "    FROM sales\n",
    "    GROUP BY category\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n📊 Statistical Summary by Category:\")\n",
    "stats_analysis.show(truncate=False)\n",
    "\n",
    "# Using DataFrame API for the same statistics\n",
    "print(\"\\n🎯 3.2 Statistical Analysis using DataFrame API\")\n",
    "\n",
    "# Calculate statistics using DataFrame methods\n",
    "df_stats = sales_df.select(\n",
    "    \"amount\"\n",
    ").describe().toPandas()\n",
    "\n",
    "print(\"\\n📈 DataFrame describe() method:\")\n",
    "print(df_stats.to_string(index=False))\n",
    "\n",
    "# Advanced statistical functions\n",
    "print(\"\\n🎯 3.3 Advanced Statistical Functions\")\n",
    "from pyspark.sql.functions import skewness, kurtosis, stddev_pop, stddev_samp\n",
    "\n",
    "advanced_stats = sales_df.agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    sum(\"amount\").alias(\"total\"),\n",
    "    mean(\"amount\").alias(\"mean\"),\n",
    "    stddev(\"amount\").alias(\"stddev\"),\n",
    "    stddev_pop(\"amount\").alias(\"stddev_pop\"),\n",
    "    stddev_samp(\"amount\").alias(\"stddev_samp\"),\n",
    "    variance(\"amount\").alias(\"variance\"),\n",
    "    var_pop(\"amount\").alias(\"var_pop\"),\n",
    "    var_samp(\"amount\").alias(\"var_samp\"),\n",
    "    skewness(\"amount\").alias(\"skewness\"),\n",
    "    kurtosis(\"amount\").alias(\"kurtosis\"),\n",
    "    min(\"amount\").alias(\"min\"),\n",
    "    max(\"amount\").alias(\"max\")\n",
    ")\n",
    "\n",
    "print(\"\\n📊 Advanced Statistical Measures:\")\n",
    "for row in advanced_stats.collect():\n",
    "    for field in advanced_stats.columns:\n",
    "        value = getattr(row, field)\n",
    "        if isinstance(value, float):\n",
    "            print(f\"   {field}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"   {field}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de402ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 3.4 Correlation and Covariance Analysis\n",
      "\n",
      "📊 Correlation Analysis Results:\n",
      "+---------------------------+-----------------------+---------------------+-----------------+-----------+\n",
      "|correlation_pair           |correlation_coefficient|population_covariance|sample_covariance|sample_size|\n",
      "+---------------------------+-----------------------+---------------------+-----------------+-----------+\n",
      "|amount_vs_age              |-0.3076                |-1801.06             |-1895.85         |20         |\n",
      "|amount_vs_performance_score|0.1367                 |1947.42              |2049.91          |20         |\n",
      "|amount_vs_days_since_signup|0.534                  |38110.9              |40116.74         |20         |\n",
      "|amount_vs_transaction_month|0.2306                 |353.11               |371.7            |20         |\n",
      "+---------------------------+-----------------------+---------------------+-----------------+-----------+\n",
      "\n",
      "\n",
      "🎯 3.5 Correlation Matrix using DataFrame API\n",
      "\n",
      "📈 Correlation Matrix:\n",
      "+---------------------------+-----------------------+---------------------+-----------------+-----------+\n",
      "|correlation_pair           |correlation_coefficient|population_covariance|sample_covariance|sample_size|\n",
      "+---------------------------+-----------------------+---------------------+-----------------+-----------+\n",
      "|amount_vs_age              |-0.3076                |-1801.06             |-1895.85         |20         |\n",
      "|amount_vs_performance_score|0.1367                 |1947.42              |2049.91          |20         |\n",
      "|amount_vs_days_since_signup|0.534                  |38110.9              |40116.74         |20         |\n",
      "|amount_vs_transaction_month|0.2306                 |353.11               |371.7            |20         |\n",
      "+---------------------------+-----------------------+---------------------+-----------------+-----------+\n",
      "\n",
      "\n",
      "🎯 3.5 Correlation Matrix using DataFrame API\n",
      "\n",
      "📈 Correlation Matrix:\n",
      "amount               |  1.0000 | -0.3076 |  0.1367 |  0.5340 |  0.2306\n",
      "amount               |  1.0000 | -0.3076 |  0.1367 |  0.5340 |  0.2306\n",
      "age                  | -0.3076 |  1.0000 |  0.0239 |  0.2284 |  0.0817\n",
      "age                  | -0.3076 |  1.0000 |  0.0239 |  0.2284 |  0.0817\n",
      "performance_score    |  0.1367 |  0.0239 |  1.0000 |  0.1254 |  0.0948\n",
      "performance_score    |  0.1367 |  0.0239 |  1.0000 |  0.1254 |  0.0948\n",
      "days_since_signup    |  0.5340 |  0.2284 |  0.1254 |  1.0000 |  0.6151\n",
      "days_since_signup    |  0.5340 |  0.2284 |  0.1254 |  1.0000 |  0.6151\n",
      "transaction_month    |  0.2306 |  0.0817 |  0.0948 |  0.6151 |  1.0000\n",
      "\n",
      "Column Headers:\n",
      "                     | amount  | age     | performance_score | days_since_signup | transaction_month\n",
      "\n",
      "🎯 3.6 Outlier Detection using Statistical Methods\n",
      "\n",
      "📊 Outlier Analysis Results:\n",
      "transaction_month    |  0.2306 |  0.0817 |  0.0948 |  0.6151 |  1.0000\n",
      "\n",
      "Column Headers:\n",
      "                     | amount  | age     | performance_score | days_since_signup | transaction_month\n",
      "\n",
      "🎯 3.6 Outlier Detection using Statistical Methods\n",
      "\n",
      "📊 Outlier Analysis Results:\n",
      "+--------------+-----------+-------+-----------+-----------+----------+-------+-----------+-----------+--------------+\n",
      "|transaction_id|customer_id|amount |category   |mean_amount|std_amount|z_score|lower_fence|upper_fence|outlier_status|\n",
      "+--------------+-----------+-------+-----------+-----------+----------+-------+-----------+-----------+--------------+\n",
      "|12            |107        |2345.0 |Automotive |583.64     |722.46    |2.44   |-1125.01   |2114.99    |High Outlier  |\n",
      "|17            |101        |2199.99|Electronics|583.64     |722.46    |2.24   |-1125.01   |2114.99    |High Outlier  |\n",
      "|9             |101        |1599.99|Electronics|583.64     |722.46    |1.41   |-1125.01   |2114.99    |Normal        |\n",
      "|1             |101        |1200.0 |Electronics|583.64     |722.46    |0.85   |-1125.01   |2114.99    |Normal        |\n",
      "|13            |102        |34.99  |Books      |583.64     |722.46    |-0.76  |-1125.01   |2114.99    |Normal        |\n",
      "|2             |102        |45.99  |Books      |583.64     |722.46    |-0.74  |-1125.01   |2114.99    |Normal        |\n",
      "|6             |102        |67.89  |Books      |583.64     |722.46    |-0.71  |-1125.01   |2114.99    |Normal        |\n",
      "|10            |106        |78.99  |Beauty     |583.64     |722.46    |-0.7   |-1125.01   |2114.99    |Normal        |\n",
      "|3             |103        |89.99  |Clothing   |583.64     |722.46    |-0.68  |-1125.01   |2114.99    |Normal        |\n",
      "|16            |103        |99.99  |Clothing   |583.64     |722.46    |-0.67  |-1125.01   |2114.99    |Normal        |\n",
      "|8             |103        |123.45 |Clothing   |583.64     |722.46    |-0.64  |-1125.01   |2114.99    |Normal        |\n",
      "|18            |109        |125.0  |Toys       |583.64     |722.46    |-0.63  |-1125.01   |2114.99    |Normal        |\n",
      "|7             |105        |156.75 |Sports     |583.64     |722.46    |-0.59  |-1125.01   |2114.99    |Normal        |\n",
      "|15            |108        |189.99 |Garden     |583.64     |722.46    |-0.54  |-1125.01   |2114.99    |Normal        |\n",
      "|5             |104        |234.5  |Home       |583.64     |722.46    |-0.48  |-1125.01   |2114.99    |Normal        |\n",
      "|20            |110        |899.99 |Jewelry    |583.64     |722.46    |0.44   |-1125.01   |2114.99    |Normal        |\n",
      "|4             |101        |899.99 |Electronics|583.64     |722.46    |0.44   |-1125.01   |2114.99    |Normal        |\n",
      "|14            |105        |267.5  |Sports     |583.64     |722.46    |-0.44  |-1125.01   |2114.99    |Normal        |\n",
      "|11            |104        |445.0  |Home       |583.64     |722.46    |-0.19  |-1125.01   |2114.99    |Normal        |\n",
      "|19            |104        |567.89 |Home       |583.64     |722.46    |-0.02  |-1125.01   |2114.99    |Normal        |\n",
      "+--------------+-----------+-------+-----------+-----------+----------+-------+-----------+-----------+--------------+\n",
      "\n",
      "+--------------+-----------+-------+-----------+-----------+----------+-------+-----------+-----------+--------------+\n",
      "|transaction_id|customer_id|amount |category   |mean_amount|std_amount|z_score|lower_fence|upper_fence|outlier_status|\n",
      "+--------------+-----------+-------+-----------+-----------+----------+-------+-----------+-----------+--------------+\n",
      "|12            |107        |2345.0 |Automotive |583.64     |722.46    |2.44   |-1125.01   |2114.99    |High Outlier  |\n",
      "|17            |101        |2199.99|Electronics|583.64     |722.46    |2.24   |-1125.01   |2114.99    |High Outlier  |\n",
      "|9             |101        |1599.99|Electronics|583.64     |722.46    |1.41   |-1125.01   |2114.99    |Normal        |\n",
      "|1             |101        |1200.0 |Electronics|583.64     |722.46    |0.85   |-1125.01   |2114.99    |Normal        |\n",
      "|13            |102        |34.99  |Books      |583.64     |722.46    |-0.76  |-1125.01   |2114.99    |Normal        |\n",
      "|2             |102        |45.99  |Books      |583.64     |722.46    |-0.74  |-1125.01   |2114.99    |Normal        |\n",
      "|6             |102        |67.89  |Books      |583.64     |722.46    |-0.71  |-1125.01   |2114.99    |Normal        |\n",
      "|10            |106        |78.99  |Beauty     |583.64     |722.46    |-0.7   |-1125.01   |2114.99    |Normal        |\n",
      "|3             |103        |89.99  |Clothing   |583.64     |722.46    |-0.68  |-1125.01   |2114.99    |Normal        |\n",
      "|16            |103        |99.99  |Clothing   |583.64     |722.46    |-0.67  |-1125.01   |2114.99    |Normal        |\n",
      "|8             |103        |123.45 |Clothing   |583.64     |722.46    |-0.64  |-1125.01   |2114.99    |Normal        |\n",
      "|18            |109        |125.0  |Toys       |583.64     |722.46    |-0.63  |-1125.01   |2114.99    |Normal        |\n",
      "|7             |105        |156.75 |Sports     |583.64     |722.46    |-0.59  |-1125.01   |2114.99    |Normal        |\n",
      "|15            |108        |189.99 |Garden     |583.64     |722.46    |-0.54  |-1125.01   |2114.99    |Normal        |\n",
      "|5             |104        |234.5  |Home       |583.64     |722.46    |-0.48  |-1125.01   |2114.99    |Normal        |\n",
      "|20            |110        |899.99 |Jewelry    |583.64     |722.46    |0.44   |-1125.01   |2114.99    |Normal        |\n",
      "|4             |101        |899.99 |Electronics|583.64     |722.46    |0.44   |-1125.01   |2114.99    |Normal        |\n",
      "|14            |105        |267.5  |Sports     |583.64     |722.46    |-0.44  |-1125.01   |2114.99    |Normal        |\n",
      "|11            |104        |445.0  |Home       |583.64     |722.46    |-0.19  |-1125.01   |2114.99    |Normal        |\n",
      "|19            |104        |567.89 |Home       |583.64     |722.46    |-0.02  |-1125.01   |2114.99    |Normal        |\n",
      "+--------------+-----------+-------+-----------+-----------+----------+-------+-----------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Correlation and Covariance Analysis\n",
    "print(\"🎯 3.4 Correlation and Covariance Analysis\")\n",
    "\n",
    "# Create additional numeric variables for correlation analysis\n",
    "enhanced_sales = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        s.*,\n",
    "        c.age,\n",
    "        e.salary,\n",
    "        e.performance_score,\n",
    "        EXTRACT(MONTH FROM s.transaction_date) as transaction_month,\n",
    "        EXTRACT(DAY FROM s.transaction_date) as transaction_day,\n",
    "        ROW_NUMBER() OVER (PARTITION BY s.customer_id ORDER BY s.transaction_date) as customer_transaction_number,\n",
    "        DATEDIFF(s.transaction_date, c.signup_date) as days_since_signup\n",
    "    FROM sales s\n",
    "    JOIN customers c ON s.customer_id = c.customer_id\n",
    "    JOIN employees e ON s.salesperson_id = e.salesperson_id\n",
    "\"\"\")\n",
    "\n",
    "enhanced_sales.createOrReplaceTempView(\"enhanced_sales\")\n",
    "\n",
    "# Calculate correlations using SQL\n",
    "correlation_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        'amount_vs_age' as correlation_pair,\n",
    "        ROUND(CORR(amount, age), 4) as correlation_coefficient,\n",
    "        ROUND(COVAR_POP(amount, age), 2) as population_covariance,\n",
    "        ROUND(COVAR_SAMP(amount, age), 2) as sample_covariance,\n",
    "        COUNT(*) as sample_size\n",
    "    FROM enhanced_sales\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'amount_vs_performance_score' as correlation_pair,\n",
    "        ROUND(CORR(amount, performance_score), 4) as correlation_coefficient,\n",
    "        ROUND(COVAR_POP(amount, performance_score), 2) as population_covariance,\n",
    "        ROUND(COVAR_SAMP(amount, performance_score), 2) as sample_covariance,\n",
    "        COUNT(*) as sample_size\n",
    "    FROM enhanced_sales\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'amount_vs_days_since_signup' as correlation_pair,\n",
    "        ROUND(CORR(amount, days_since_signup), 4) as correlation_coefficient,\n",
    "        ROUND(COVAR_POP(amount, days_since_signup), 2) as population_covariance,\n",
    "        ROUND(COVAR_SAMP(amount, days_since_signup), 2) as sample_covariance,\n",
    "        COUNT(*) as sample_size\n",
    "    FROM enhanced_sales\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'amount_vs_transaction_month' as correlation_pair,\n",
    "        ROUND(CORR(amount, transaction_month), 4) as correlation_coefficient,\n",
    "        ROUND(COVAR_POP(amount, transaction_month), 2) as population_covariance,\n",
    "        ROUND(COVAR_SAMP(amount, transaction_month), 2) as sample_covariance,\n",
    "        COUNT(*) as sample_size\n",
    "    FROM enhanced_sales\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n📊 Correlation Analysis Results:\")\n",
    "correlation_analysis.show(truncate=False)\n",
    "\n",
    "# Using DataFrame API for correlation matrix\n",
    "print(\"\\n🎯 3.5 Correlation Matrix using DataFrame API\")\n",
    "\n",
    "# Select numeric columns for correlation analysis\n",
    "numeric_columns = [\"amount\", \"age\", \"performance_score\", \"days_since_signup\", \"transaction_month\"]\n",
    "\n",
    "# Calculate correlation matrix using DataFrame methods\n",
    "print(\"\\n📈 Correlation Matrix:\")\n",
    "import builtins\n",
    "for col1 in numeric_columns:\n",
    "    correlations = []\n",
    "    for col2 in numeric_columns:\n",
    "        if col1 == col2:\n",
    "            correlations.append(1.0000)\n",
    "        else:\n",
    "            corr_value = enhanced_sales.stat.corr(col1, col2)\n",
    "            correlations.append(builtins.round(corr_value, 4) if corr_value is not None else 0.0000)\n",
    "    \n",
    "    corr_str = \" | \".join([f\"{c:7.4f}\" for c in correlations])\n",
    "    print(f\"{col1:20} | {corr_str}\")\n",
    "\n",
    "# Print header\n",
    "print(\"\\nColumn Headers:\")\n",
    "header = \" | \".join([f\"{col:7}\" for col in numeric_columns])\n",
    "print(f\"{'':20} | {header}\")\n",
    "\n",
    "# Outlier Detection using Statistical Methods\n",
    "print(\"\\n🎯 3.6 Outlier Detection using Statistical Methods\")\n",
    "\n",
    "outlier_analysis = spark.sql(\"\"\"\n",
    "    WITH stats AS (\n",
    "        SELECT \n",
    "            AVG(amount) as mean_amount,\n",
    "            STDDEV(amount) as std_amount,\n",
    "            PERCENTILE_APPROX(amount, 0.25) as q1,\n",
    "            PERCENTILE_APPROX(amount, 0.75) as q3,\n",
    "            PERCENTILE_APPROX(amount, 0.75) - PERCENTILE_APPROX(amount, 0.25) as iqr\n",
    "        FROM sales\n",
    "    )\n",
    "    SELECT \n",
    "        s.transaction_id,\n",
    "        s.customer_id,\n",
    "        s.amount,\n",
    "        s.category,\n",
    "        ROUND(st.mean_amount, 2) as mean_amount,\n",
    "        ROUND(st.std_amount, 2) as std_amount,\n",
    "        ROUND((s.amount - st.mean_amount) / st.std_amount, 2) as z_score,\n",
    "        ROUND(st.q1 - 1.5 * st.iqr, 2) as lower_fence,\n",
    "        ROUND(st.q3 + 1.5 * st.iqr, 2) as upper_fence,\n",
    "        CASE \n",
    "            WHEN s.amount < (st.q1 - 1.5 * st.iqr) THEN 'Low Outlier'\n",
    "            WHEN s.amount > (st.q3 + 1.5 * st.iqr) THEN 'High Outlier'\n",
    "            WHEN ABS((s.amount - st.mean_amount) / st.std_amount) > 2 THEN 'Statistical Outlier'\n",
    "            ELSE 'Normal'\n",
    "        END as outlier_status\n",
    "    FROM sales s\n",
    "    CROSS JOIN stats st\n",
    "    ORDER BY ABS((s.amount - st.mean_amount) / st.std_amount) DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n📊 Outlier Analysis Results:\")\n",
    "outlier_analysis.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844178a9",
   "metadata": {},
   "source": [
    "## 🕐 Section 4: Time Series & Date Analysis\n",
    "\n",
    "Time series analysis is crucial for understanding temporal patterns, trends, and seasonality in data. PySpark provides comprehensive date/time functions for sophisticated temporal analytics.\n",
    "\n",
    "### 🎯 **Date/Time Functions:**\n",
    "- **Date Arithmetic**: DATEDIFF, DATE_ADD, DATE_SUB operations\n",
    "- **Date Formatting**: Date parsing, formatting, and extraction\n",
    "- **Time Windows**: Tumbling and sliding window operations\n",
    "- **Calendar Functions**: EXTRACT, YEAR, MONTH, DAY, DAYOFWEEK\n",
    "- **Time Zone Handling**: UTC conversions and time zone operations\n",
    "\n",
    "### 📈 **Time Series Patterns:**\n",
    "- **Temporal Aggregations**: Daily, weekly, monthly summaries\n",
    "- **Trend Analysis**: Period-over-period comparisons\n",
    "- **Seasonality Detection**: Cyclical pattern identification\n",
    "- **Event Sequence Analysis**: Time-based event ordering\n",
    "- **Business Calendar**: Working days, holidays, fiscal periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9f27891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 4.1 Date/Time Functions and Temporal Analysis\n",
      "\n",
      "📊 Time Series Analysis Results:\n",
      "+--------------+-----------+----------------+-------+-----------+------+----------------+-----------------+---------------+-----------+------------+-----------+--------+----------+----------+------------+------------+--------+-----------------+-------------------+------------------+-----------------+--------+------+\n",
      "|transaction_id|customer_id|transaction_date|amount |category   |region|transaction_year|transaction_month|transaction_day|day_of_week|week_of_year|quarter_num|day_name|month_name|year_month|plus_30_days|minus_7_days|days_ago|last_day_of_month|next_month_same_day|first_day_of_month|first_day_of_year|day_type|season|\n",
      "+--------------+-----------+----------------+-------+-----------+------+----------------+-----------------+---------------+-----------+------------+-----------+--------+----------+----------+------------+------------+--------+-----------------+-------------------+------------------+-----------------+--------+------+\n",
      "|1             |101        |2024-01-15      |1200.0 |Electronics|West  |2024            |1                |15             |2          |3           |1          |Monday  |January   |2024-01   |2024-02-14  |2024-01-08  |588     |2024-01-31       |2024-02-15         |2024-01-01        |2024-01-01       |Weekday |Winter|\n",
      "|2             |102        |2024-01-16      |45.99  |Books      |East  |2024            |1                |16             |3          |3           |1          |Tuesday |January   |2024-01   |2024-02-15  |2024-01-09  |587     |2024-01-31       |2024-02-16         |2024-01-01        |2024-01-01       |Weekday |Winter|\n",
      "|3             |103        |2024-01-18      |89.99  |Clothing   |North |2024            |1                |18             |5          |3           |1          |Thursday|January   |2024-01   |2024-02-17  |2024-01-11  |585     |2024-01-31       |2024-02-18         |2024-01-01        |2024-01-01       |Weekday |Winter|\n",
      "|4             |101        |2024-02-10      |899.99 |Electronics|West  |2024            |2                |10             |7          |6           |1          |Saturday|February  |2024-02   |2024-03-11  |2024-02-03  |562     |2024-02-29       |2024-03-10         |2024-02-01        |2024-01-01       |Weekend |Winter|\n",
      "|5             |104        |2024-02-12      |234.5  |Home       |South |2024            |2                |12             |2          |7           |1          |Monday  |February  |2024-02   |2024-03-13  |2024-02-05  |560     |2024-02-29       |2024-03-12         |2024-02-01        |2024-01-01       |Weekday |Winter|\n",
      "|6             |102        |2024-02-15      |67.89  |Books      |East  |2024            |2                |15             |5          |7           |1          |Thursday|February  |2024-02   |2024-03-16  |2024-02-08  |557     |2024-02-29       |2024-03-15         |2024-02-01        |2024-01-01       |Weekday |Winter|\n",
      "|7             |105        |2024-03-01      |156.75 |Sports     |West  |2024            |3                |1              |6          |9           |1          |Friday  |March     |2024-03   |2024-03-31  |2024-02-23  |542     |2024-03-31       |2024-04-01         |2024-03-01        |2024-01-01       |Weekday |Spring|\n",
      "|8             |103        |2024-03-05      |123.45 |Clothing   |North |2024            |3                |5              |3          |10          |1          |Tuesday |March     |2024-03   |2024-04-04  |2024-02-27  |538     |2024-03-31       |2024-04-05         |2024-03-01        |2024-01-01       |Weekday |Spring|\n",
      "|9             |101        |2024-04-01      |1599.99|Electronics|West  |2024            |4                |1              |2          |14          |2          |Monday  |April     |2024-04   |2024-05-01  |2024-03-25  |511     |2024-04-30       |2024-05-01         |2024-04-01        |2024-01-01       |Weekday |Spring|\n",
      "|10            |106        |2024-04-05      |78.99  |Beauty     |East  |2024            |4                |5              |6          |14          |2          |Friday  |April     |2024-04   |2024-05-05  |2024-03-29  |507     |2024-04-30       |2024-05-05         |2024-04-01        |2024-01-01       |Weekday |Spring|\n",
      "+--------------+-----------+----------------+-------+-----------+------+----------------+-----------------+---------------+-----------+------------+-----------+--------+----------+----------+------------+------------+--------+-----------------+-------------------+------------------+-----------------+--------+------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Time Series Analysis\n",
    "print(\"🎯 4.1 Date/Time Functions and Temporal Analysis\")\n",
    "\n",
    "# Advanced date/time analysis\n",
    "time_series_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        transaction_id,\n",
    "        customer_id,\n",
    "        transaction_date,\n",
    "        amount,\n",
    "        category,\n",
    "        region,\n",
    "        \n",
    "        -- Date extraction functions\n",
    "        YEAR(transaction_date) as transaction_year,\n",
    "        MONTH(transaction_date) as transaction_month,\n",
    "        DAY(transaction_date) as transaction_day,\n",
    "        DAYOFWEEK(transaction_date) as day_of_week,\n",
    "        WEEKOFYEAR(transaction_date) as week_of_year,\n",
    "        QUARTER(transaction_date) as quarter_num,\n",
    "        \n",
    "        -- Date formatting and naming\n",
    "        DATE_FORMAT(transaction_date, 'EEEE') as day_name,\n",
    "        DATE_FORMAT(transaction_date, 'MMMM') as month_name,\n",
    "        DATE_FORMAT(transaction_date, 'yyyy-MM') as year_month,\n",
    "        \n",
    "        -- Date arithmetic\n",
    "        DATE_ADD(transaction_date, 30) as plus_30_days,\n",
    "        DATE_SUB(transaction_date, 7) as minus_7_days,\n",
    "        DATEDIFF(CURRENT_DATE(), transaction_date) as days_ago,\n",
    "        \n",
    "        -- Relative date calculations\n",
    "        LAST_DAY(transaction_date) as last_day_of_month,\n",
    "        ADD_MONTHS(transaction_date, 1) as next_month_same_day,\n",
    "        TRUNC(transaction_date, 'MM') as first_day_of_month,\n",
    "        TRUNC(transaction_date, 'YYYY') as first_day_of_year,\n",
    "        \n",
    "        -- Business logic\n",
    "        CASE \n",
    "            WHEN DAYOFWEEK(transaction_date) IN (1, 7) THEN 'Weekend'\n",
    "            ELSE 'Weekday'\n",
    "        END as day_type,\n",
    "        \n",
    "        CASE \n",
    "            WHEN MONTH(transaction_date) IN (12, 1, 2) THEN 'Winter'\n",
    "            WHEN MONTH(transaction_date) IN (3, 4, 5) THEN 'Spring'\n",
    "            WHEN MONTH(transaction_date) IN (6, 7, 8) THEN 'Summer'\n",
    "            ELSE 'Fall'\n",
    "        END as season\n",
    "        \n",
    "    FROM sales\n",
    "    ORDER BY transaction_date\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n📊 Time Series Analysis Results:\")\n",
    "time_series_analysis.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66e1e88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Section 5.1: Time Series Analysis (Month-over-Month Performance)\n",
      " Regional Month-over-Month Analysis:\n",
      " Regional Month-over-Month Analysis:\n",
      "+----+-----+------+-----------------+-------------+---------------------+------------------+--------------+\n",
      "|year|month|region|transaction_count|total_revenue|avg_transaction_value|mom_revenue_change|mom_growth_pct|\n",
      "+----+-----+------+-----------------+-------------+---------------------+------------------+--------------+\n",
      "|2024|1    |East  |1                |45.99        |45.99                |NULL              |NULL          |\n",
      "|2024|2    |East  |1                |67.89        |67.89                |21.9              |47.62         |\n",
      "|2024|4    |East  |1                |78.99        |78.99                |11.1              |16.35         |\n",
      "|2024|5    |East  |1                |34.99        |34.99                |-44.0             |-55.7         |\n",
      "|2024|7    |East  |1                |125.0        |125.0                |90.01             |257.24        |\n",
      "|2024|1    |North |1                |89.99        |89.99                |NULL              |NULL          |\n",
      "|2024|3    |North |1                |123.45       |123.45               |33.46             |37.18         |\n",
      "|2024|5    |North |1                |2345.0       |2345.0               |2221.55           |1799.55       |\n",
      "|2024|6    |North |1                |99.99        |99.99                |-2245.01          |-95.74        |\n",
      "|2024|8    |North |1                |899.99       |899.99               |800.0             |800.08        |\n",
      "|2024|2    |South |1                |234.5        |234.5                |NULL              |NULL          |\n",
      "|2024|4    |South |1                |445.0        |445.0                |210.5             |89.77         |\n",
      "|2024|6    |South |1                |189.99       |189.99               |-255.01           |-57.31        |\n",
      "|2024|7    |South |1                |567.89       |567.89               |377.9             |198.91        |\n",
      "|2024|1    |West  |1                |1200.0       |1200.0               |NULL              |NULL          |\n",
      "|2024|2    |West  |1                |899.99       |899.99               |-300.01           |-25.0         |\n",
      "|2024|3    |West  |1                |156.75       |156.75               |-743.24           |-82.58        |\n",
      "|2024|4    |West  |1                |1599.99      |1599.99              |1443.24           |920.73        |\n",
      "|2024|5    |West  |1                |267.5        |267.5                |-1332.49          |-83.28        |\n",
      "|2024|7    |West  |1                |2199.99      |2199.99              |1932.49           |722.43        |\n",
      "+----+-----+------+-----------------+-------------+---------------------+------------------+--------------+\n",
      "\n",
      "+----+-----+------+-----------------+-------------+---------------------+------------------+--------------+\n",
      "|year|month|region|transaction_count|total_revenue|avg_transaction_value|mom_revenue_change|mom_growth_pct|\n",
      "+----+-----+------+-----------------+-------------+---------------------+------------------+--------------+\n",
      "|2024|1    |East  |1                |45.99        |45.99                |NULL              |NULL          |\n",
      "|2024|2    |East  |1                |67.89        |67.89                |21.9              |47.62         |\n",
      "|2024|4    |East  |1                |78.99        |78.99                |11.1              |16.35         |\n",
      "|2024|5    |East  |1                |34.99        |34.99                |-44.0             |-55.7         |\n",
      "|2024|7    |East  |1                |125.0        |125.0                |90.01             |257.24        |\n",
      "|2024|1    |North |1                |89.99        |89.99                |NULL              |NULL          |\n",
      "|2024|3    |North |1                |123.45       |123.45               |33.46             |37.18         |\n",
      "|2024|5    |North |1                |2345.0       |2345.0               |2221.55           |1799.55       |\n",
      "|2024|6    |North |1                |99.99        |99.99                |-2245.01          |-95.74        |\n",
      "|2024|8    |North |1                |899.99       |899.99               |800.0             |800.08        |\n",
      "|2024|2    |South |1                |234.5        |234.5                |NULL              |NULL          |\n",
      "|2024|4    |South |1                |445.0        |445.0                |210.5             |89.77         |\n",
      "|2024|6    |South |1                |189.99       |189.99               |-255.01           |-57.31        |\n",
      "|2024|7    |South |1                |567.89       |567.89               |377.9             |198.91        |\n",
      "|2024|1    |West  |1                |1200.0       |1200.0               |NULL              |NULL          |\n",
      "|2024|2    |West  |1                |899.99       |899.99               |-300.01           |-25.0         |\n",
      "|2024|3    |West  |1                |156.75       |156.75               |-743.24           |-82.58        |\n",
      "|2024|4    |West  |1                |1599.99      |1599.99              |1443.24           |920.73        |\n",
      "|2024|5    |West  |1                |267.5        |267.5                |-1332.49          |-83.28        |\n",
      "|2024|7    |West  |1                |2199.99      |2199.99              |1932.49           |722.43        |\n",
      "+----+-----+------+-----------------+-------------+---------------------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\" Section 5.1: Time Series Analysis (Month-over-Month Performance)\")\n",
    "monthly_trends = spark.sql(\"\"\"\n",
    "    WITH monthly_aggregates AS (\n",
    "        SELECT \n",
    "            YEAR(transaction_date) as year,\n",
    "            MONTH(transaction_date) as month,\n",
    "            region,  -- Add region for partitioning\n",
    "            COUNT(*) as transaction_count,\n",
    "            ROUND(SUM(amount), 2) as total_revenue,\n",
    "            ROUND(AVG(amount), 2) as avg_transaction_value\n",
    "        FROM sales\n",
    "        GROUP BY YEAR(transaction_date), MONTH(transaction_date), region\n",
    "    ),\n",
    "    regional_mom_trends AS (\n",
    "        SELECT \n",
    "            year, month, region,\n",
    "            transaction_count, total_revenue, avg_transaction_value,\n",
    "            ROUND(total_revenue - LAG(total_revenue, 1) OVER (\n",
    "                PARTITION BY region \n",
    "                ORDER BY year, month\n",
    "            ), 2) as mom_revenue_change,\n",
    "            ROUND(CASE \n",
    "                WHEN LAG(total_revenue, 1) OVER (\n",
    "                    PARTITION BY region \n",
    "                    ORDER BY year, month\n",
    "                ) > 0 THEN \n",
    "                    ((total_revenue - LAG(total_revenue, 1) OVER (\n",
    "                        PARTITION BY region \n",
    "                        ORDER BY year, month\n",
    "                    )) / LAG(total_revenue, 1) OVER (\n",
    "                        PARTITION BY region \n",
    "                        ORDER BY year, month\n",
    "                    )) * 100\n",
    "                ELSE NULL \n",
    "            END, 2) as mom_growth_pct\n",
    "        FROM monthly_aggregates\n",
    "    )\n",
    "    SELECT * FROM regional_mom_trends \n",
    "    ORDER BY region, year, month\n",
    "\"\"\")\n",
    "\n",
    "print(\" Regional Month-over-Month Analysis:\")\n",
    "monthly_trends.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0389fd",
   "metadata": {},
   "source": [
    "## ⚡ Section 5: Performance & Optimization\n",
    "\n",
    "Performance optimization is crucial for production PySpark applications. Understanding query execution, partitioning strategies, and optimization techniques ensures efficient processing of large datasets.\n",
    "\n",
    "### 🎯 **Performance Concepts:**\n",
    "- **Window Function Optimization**: Proper partitioning to avoid single partition warnings\n",
    "- **Query Plan Analysis**: Understanding execution plans and bottlenecks\n",
    "- **Partitioning Strategies**: Data distribution and partition pruning\n",
    "- **Caching & Persistence**: Strategic use of cache() and persist()\n",
    "- **Broadcast Joins**: Optimizing small table joins\n",
    "- **Cost-Based Optimization**: Statistics collection and CBO usage\n",
    "\n",
    "### ⚠️ **Common Performance Issues:**\n",
    "- **Single Partition Windows**: All data moved to one partition\n",
    "- **Data Skew**: Uneven data distribution across partitions  \n",
    "- **Shuffle Operations**: Excessive data movement between nodes\n",
    "- **Memory Issues**: Out-of-memory errors and garbage collection\n",
    "- **Small Files Problem**: Too many small files affecting performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdbe67c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 5.1 Fixing Window Function Performance Warnings\n",
      "⚠️  Issue: 'No Partition Defined for Window operation! Moving all data to a single partition'\n",
      "✅ Solution: Use proper PARTITION BY clauses to distribute data efficiently\n",
      "\n",
      "❌ PROBLEMATIC: Window without partitioning (single partition)\n",
      "   Query that causes warning:\n",
      "   \n",
      "       SELECT \n",
      "           customer_id,\n",
      "           amount,\n",
      "           ROW_NUMBER() OVER (ORDER BY amount DESC) as global_rank\n",
      "       FROM sales\n",
      "   \n",
      "\n",
      "✅ OPTIMIZED: Window with proper partitioning\n",
      "\n",
      "📊 Optimized Window Functions (Partitioned for Performance):\n",
      "+-----------+------+----------+------+----------------+-----------+-------------+-----------------+-----------------+------------------------+----------------------+------------------------+--------------------+\n",
      "|customer_id|region|category  |amount|transaction_date|region_rank|category_rank|customer_sequence|region_avg_amount|region_transaction_count|category_running_total|previous_purchase_amount|next_purchase_amount|\n",
      "+-----------+------+----------+------+----------------+-----------+-------------+-----------------+-----------------+------------------------+----------------------+------------------------+--------------------+\n",
      "|106        |East  |Beauty    |78.99 |2024-04-05      |2          |1            |1                |70.572           |5                       |78.99                 |NULL                    |NULL                |\n",
      "|102        |East  |Books     |67.89 |2024-02-15      |3          |1            |2                |70.572           |5                       |113.88                |45.99                   |34.99               |\n",
      "|109        |East  |Toys      |125.0 |2024-07-05      |1          |1            |1                |70.572           |5                       |125.0                 |NULL                    |NULL                |\n",
      "|102        |East  |Books     |45.99 |2024-01-16      |4          |2            |1                |70.572           |5                       |45.99                 |NULL                    |67.89               |\n",
      "|102        |East  |Books     |34.99 |2024-05-15      |5          |3            |3                |70.572           |5                       |148.87                |67.89                   |NULL                |\n",
      "|107        |North |Automotive|2345.0|2024-05-01      |1          |1            |1                |711.684          |5                       |2345.0                |NULL                    |NULL                |\n",
      "|103        |North |Clothing  |123.45|2024-03-05      |3          |1            |2                |711.684          |5                       |213.44                |89.99                   |99.99               |\n",
      "|110        |North |Jewelry   |899.99|2024-08-01      |2          |1            |1                |711.684          |5                       |899.99                |NULL                    |NULL                |\n",
      "|103        |North |Clothing  |99.99 |2024-06-10      |4          |2            |3                |711.684          |5                       |313.43                |123.45                  |NULL                |\n",
      "|103        |North |Clothing  |89.99 |2024-01-18      |5          |3            |1                |711.684          |5                       |89.99                 |NULL                    |123.45              |\n",
      "+-----------+------+----------+------+----------------+-----------+-------------+-----------------+-----------------+------------------------+----------------------+------------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "+-----------+------+----------+------+----------------+-----------+-------------+-----------------+-----------------+------------------------+----------------------+------------------------+--------------------+\n",
      "|customer_id|region|category  |amount|transaction_date|region_rank|category_rank|customer_sequence|region_avg_amount|region_transaction_count|category_running_total|previous_purchase_amount|next_purchase_amount|\n",
      "+-----------+------+----------+------+----------------+-----------+-------------+-----------------+-----------------+------------------------+----------------------+------------------------+--------------------+\n",
      "|106        |East  |Beauty    |78.99 |2024-04-05      |2          |1            |1                |70.572           |5                       |78.99                 |NULL                    |NULL                |\n",
      "|102        |East  |Books     |67.89 |2024-02-15      |3          |1            |2                |70.572           |5                       |113.88                |45.99                   |34.99               |\n",
      "|109        |East  |Toys      |125.0 |2024-07-05      |1          |1            |1                |70.572           |5                       |125.0                 |NULL                    |NULL                |\n",
      "|102        |East  |Books     |45.99 |2024-01-16      |4          |2            |1                |70.572           |5                       |45.99                 |NULL                    |67.89               |\n",
      "|102        |East  |Books     |34.99 |2024-05-15      |5          |3            |3                |70.572           |5                       |148.87                |67.89                   |NULL                |\n",
      "|107        |North |Automotive|2345.0|2024-05-01      |1          |1            |1                |711.684          |5                       |2345.0                |NULL                    |NULL                |\n",
      "|103        |North |Clothing  |123.45|2024-03-05      |3          |1            |2                |711.684          |5                       |213.44                |89.99                   |99.99               |\n",
      "|110        |North |Jewelry   |899.99|2024-08-01      |2          |1            |1                |711.684          |5                       |899.99                |NULL                    |NULL                |\n",
      "|103        |North |Clothing  |99.99 |2024-06-10      |4          |2            |3                |711.684          |5                       |313.43                |123.45                  |NULL                |\n",
      "|103        |North |Clothing  |89.99 |2024-01-18      |5          |3            |1                |711.684          |5                       |89.99                 |NULL                    |123.45              |\n",
      "+-----------+------+----------+------+----------------+-----------+-------------+-----------------+-----------------+------------------------+----------------------+------------------------+--------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Window Function Performance Optimization\n",
    "print(\"🎯 5.1 Fixing Window Function Performance Warnings\")\n",
    "print(\"⚠️  Issue: 'No Partition Defined for Window operation! Moving all data to a single partition'\")\n",
    "print(\"✅ Solution: Use proper PARTITION BY clauses to distribute data efficiently\")\n",
    "\n",
    "# BEFORE: Poor performance - no partitioning (causes warnings)\n",
    "print(\"\\n❌ PROBLEMATIC: Window without partitioning (single partition)\")\n",
    "problematic_query = \"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        amount,\n",
    "        ROW_NUMBER() OVER (ORDER BY amount DESC) as global_rank\n",
    "    FROM sales\n",
    "\"\"\"\n",
    "\n",
    "# This would cause the warning, so we'll just show it as text\n",
    "print(\"   Query that causes warning:\")\n",
    "print(\"   \" + problematic_query.replace(\"\\n\", \"\\n   \"))\n",
    "\n",
    "# AFTER: Optimized performance - proper partitioning\n",
    "print(\"\\n✅ OPTIMIZED: Window with proper partitioning\")\n",
    "optimized_ranking = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        region,\n",
    "        category,\n",
    "        amount,\n",
    "        transaction_date,\n",
    "        \n",
    "        -- Properly partitioned windows (no single partition warning)\n",
    "        ROW_NUMBER() OVER (PARTITION BY region ORDER BY amount DESC) as region_rank,\n",
    "        ROW_NUMBER() OVER (PARTITION BY category ORDER BY amount DESC) as category_rank,\n",
    "        ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY transaction_date) as customer_sequence,\n",
    "        \n",
    "        -- Regional statistics within partitions\n",
    "        AVG(amount) OVER (PARTITION BY region) as region_avg_amount,\n",
    "        COUNT(*) OVER (PARTITION BY region) as region_transaction_count,\n",
    "        \n",
    "        -- Category-based running totals\n",
    "        SUM(amount) OVER (\n",
    "            PARTITION BY category \n",
    "            ORDER BY transaction_date \n",
    "            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "        ) as category_running_total,\n",
    "        \n",
    "        -- Customer purchase patterns\n",
    "        LAG(amount, 1) OVER (\n",
    "            PARTITION BY customer_id \n",
    "            ORDER BY transaction_date\n",
    "        ) as previous_purchase_amount,\n",
    "        \n",
    "        LEAD(amount, 1) OVER (\n",
    "            PARTITION BY customer_id \n",
    "            ORDER BY transaction_date\n",
    "        ) as next_purchase_amount\n",
    "        \n",
    "    FROM sales\n",
    "    ORDER BY region, category_rank\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n📊 Optimized Window Functions (Partitioned for Performance):\")\n",
    "optimized_ranking.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36436480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 5.2 Query Plan Analysis and Optimization\n",
      "\n",
      "📊 Query Execution Plan Analysis:\n",
      "Query: High-value transactions by category\n",
      "Execution Plan:\n",
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['category], ['category, 'count(*) AS high_value_transactions#9823, 'sum('amount) AS total_high_value#9824, 'avg('amount) AS avg_high_value#9825]\n",
      "+- Filter (amount#3 > cast(500 as double))\n",
      "   +- Project [transaction_id#0, customer_id#1, category#2, amount#3, to_date(transaction_date#4, Some(yyyy-MM-dd), Some(America/New_York), true) AS transaction_date#8, quarter#5, region#6, salesperson_id#7]\n",
      "      +- LogicalRDD [transaction_id#0, customer_id#1, category#2, amount#3, transaction_date#4, quarter#5, region#6, salesperson_id#7], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "category: string, high_value_transactions: bigint, total_high_value: double, avg_high_value: double\n",
      "Aggregate [category#2], [category#2, count(1) AS high_value_transactions#9823L, sum(amount#3) AS total_high_value#9824, avg(amount#3) AS avg_high_value#9825]\n",
      "+- Filter (amount#3 > cast(500 as double))\n",
      "   +- Project [transaction_id#0, customer_id#1, category#2, amount#3, to_date(transaction_date#4, Some(yyyy-MM-dd), Some(America/New_York), true) AS transaction_date#8, quarter#5, region#6, salesperson_id#7]\n",
      "      +- LogicalRDD [transaction_id#0, customer_id#1, category#2, amount#3, transaction_date#4, quarter#5, region#6, salesperson_id#7], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [category#2], [category#2, count(1) AS high_value_transactions#9823L, sum(amount#3) AS total_high_value#9824, avg(amount#3) AS avg_high_value#9825]\n",
      "+- Project [category#2, amount#3]\n",
      "   +- Filter (isnotnull(amount#3) AND (amount#3 > 500.0))\n",
      "      +- LogicalRDD [transaction_id#0, customer_id#1, category#2, amount#3, transaction_date#4, quarter#5, region#6, salesperson_id#7], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[category#2], functions=[count(1), sum(amount#3), avg(amount#3)], output=[category#2, high_value_transactions#9823L, total_high_value#9824, avg_high_value#9825])\n",
      "   +- Exchange hashpartitioning(category#2, 12), ENSURE_REQUIREMENTS, [plan_id=13522]\n",
      "      +- HashAggregate(keys=[category#2], functions=[partial_count(1), partial_sum(amount#3), partial_avg(amount#3)], output=[category#2, count#9841L, sum#9842, sum#9843, count#9844L])\n",
      "         +- Project [category#2, amount#3]\n",
      "            +- Filter (isnotnull(amount#3) AND (amount#3 > 500.0))\n",
      "               +- Scan ExistingRDD[transaction_id#0,customer_id#1,category#2,amount#3,transaction_date#4,quarter#5,region#6,salesperson_id#7]\n",
      "\n",
      "\n",
      "🎯 5.3 Strategic Caching for Performance\n",
      "Caching complex customer metrics for reuse...\n",
      "Dataset cached. Record count: 10\n",
      "Is cached: True\n",
      "\n",
      "📈 Using Cached Data for Multiple Analytics:\n",
      "High-value customer segments:\n",
      "+-----------+-------------+-------+------------------+------------------+\n",
      "|customer_id|name         |tier   |lifetime_value    |category_diversity|\n",
      "+-----------+-------------+-------+------------------+------------------+\n",
      "|104        |David Wilson |Gold   |1247.3899999999999|1                 |\n",
      "|107        |Grace Lee    |Gold   |2345.0            |1                 |\n",
      "|101        |Alice Johnson|Premium|5899.969999999999 |1                 |\n",
      "+-----------+-------------+-------+------------------+------------------+\n",
      "\n",
      "\n",
      "Tier-based customer analysis:\n",
      "Dataset cached. Record count: 10\n",
      "Is cached: True\n",
      "\n",
      "📈 Using Cached Data for Multiple Analytics:\n",
      "High-value customer segments:\n",
      "+-----------+-------------+-------+------------------+------------------+\n",
      "|customer_id|name         |tier   |lifetime_value    |category_diversity|\n",
      "+-----------+-------------+-------+------------------+------------------+\n",
      "|104        |David Wilson |Gold   |1247.3899999999999|1                 |\n",
      "|107        |Grace Lee    |Gold   |2345.0            |1                 |\n",
      "|101        |Alice Johnson|Premium|5899.969999999999 |1                 |\n",
      "+-----------+-------------+-------+------------------+------------------+\n",
      "\n",
      "\n",
      "Tier-based customer analysis:\n",
      "+--------+------------------+------------------+--------------+\n",
      "|tier    |avg_age           |avg_lifetime_value|avg_categories|\n",
      "+--------+------------------+------------------+--------------+\n",
      "|Standard|39.666666666666664|254.37            |1.0           |\n",
      "|Gold    |42.666666666666664|1497.46           |1.0           |\n",
      "|Premium |33.5              |1604.3474999999999|1.0           |\n",
      "+--------+------------------+------------------+--------------+\n",
      "\n",
      "\n",
      "✅ Cache cleared to free memory\n",
      "+--------+------------------+------------------+--------------+\n",
      "|tier    |avg_age           |avg_lifetime_value|avg_categories|\n",
      "+--------+------------------+------------------+--------------+\n",
      "|Standard|39.666666666666664|254.37            |1.0           |\n",
      "|Gold    |42.666666666666664|1497.46           |1.0           |\n",
      "|Premium |33.5              |1604.3474999999999|1.0           |\n",
      "+--------+------------------+------------------+--------------+\n",
      "\n",
      "\n",
      "✅ Cache cleared to free memory\n"
     ]
    }
   ],
   "source": [
    "# Query Plan Analysis and Caching Strategies\n",
    "print(\"🎯 5.2 Query Plan Analysis and Optimization\")\n",
    "\n",
    "# Demonstrate explain() for understanding query execution\n",
    "print(\"\\n📊 Query Execution Plan Analysis:\")\n",
    "sample_query = sales_df.filter(col(\"amount\") > 500).groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"high_value_transactions\"),\n",
    "    sum(\"amount\").alias(\"total_high_value\"),\n",
    "    avg(\"amount\").alias(\"avg_high_value\")\n",
    ")\n",
    "\n",
    "print(\"Query: High-value transactions by category\")\n",
    "print(\"Execution Plan:\")\n",
    "sample_query.explain(True)  # Show extended explain plan\n",
    "\n",
    "# Caching strategies for repeated operations\n",
    "print(\"\\n🎯 5.3 Strategic Caching for Performance\")\n",
    "\n",
    "# Create a complex derived dataset that might be reused\n",
    "complex_customer_metrics = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.customer_id,\n",
    "        c.name,\n",
    "        c.tier,\n",
    "        c.age,\n",
    "        COUNT(s.transaction_id) as total_transactions,\n",
    "        SUM(s.amount) as lifetime_value,\n",
    "        AVG(s.amount) as avg_transaction,\n",
    "        MIN(s.transaction_date) as first_purchase,\n",
    "        MAX(s.transaction_date) as last_purchase,\n",
    "        COLLECT_LIST(s.category) as categories_purchased,\n",
    "        COUNT(DISTINCT s.category) as category_diversity\n",
    "    FROM customers c\n",
    "    LEFT JOIN sales s ON c.customer_id = s.customer_id\n",
    "    GROUP BY c.customer_id, c.name, c.tier, c.age\n",
    "\"\"\")\n",
    "\n",
    "# Cache this complex result for reuse\n",
    "print(\"Caching complex customer metrics for reuse...\")\n",
    "complex_customer_metrics.cache()\n",
    "\n",
    "# Force computation and show cache statistics\n",
    "print(f\"Dataset cached. Record count: {complex_customer_metrics.count()}\")\n",
    "print(f\"Is cached: {complex_customer_metrics.is_cached}\")\n",
    "\n",
    "# Now reuse the cached data efficiently\n",
    "print(\"\\n📈 Using Cached Data for Multiple Analytics:\")\n",
    "\n",
    "# Analytics 1: Customer segmentation using cached data\n",
    "segment_analysis = complex_customer_metrics.select(\n",
    "    \"customer_id\", \"name\", \"tier\", \"lifetime_value\", \"category_diversity\"\n",
    ").filter(col(\"lifetime_value\") > 1000)\n",
    "\n",
    "print(\"High-value customer segments:\")\n",
    "segment_analysis.show(5, truncate=False)\n",
    "\n",
    "# Analytics 2: Age-based analysis using same cached data  \n",
    "age_analysis = complex_customer_metrics.groupBy(\"tier\").agg(\n",
    "    avg(\"age\").alias(\"avg_age\"),\n",
    "    avg(\"lifetime_value\").alias(\"avg_lifetime_value\"),\n",
    "    avg(\"category_diversity\").alias(\"avg_categories\")\n",
    ")\n",
    "\n",
    "print(\"\\nTier-based customer analysis:\")\n",
    "age_analysis.show(truncate=False)\n",
    "\n",
    "# Unpersist when done to free memory\n",
    "complex_customer_metrics.unpersist()\n",
    "print(\"\\n✅ Cache cleared to free memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf8ac57",
   "metadata": {},
   "source": [
    "## 🏗️ Section 6: Advanced DataFrame Patterns\n",
    "\n",
    "Advanced DataFrame operations enable sophisticated data manipulation and analysis. These patterns are essential for handling complex data structures and implementing advanced analytics workflows.\n",
    "\n",
    "### 🎯 **Advanced Patterns:**\n",
    "- **Dynamic Column Operations**: Programmatic column manipulation\n",
    "- **Complex Data Types**: Arrays, maps, structs, and nested data\n",
    "- **Pivot and Unpivot**: Data reshaping and transformation\n",
    "- **Regular Expressions**: Advanced text processing and extraction\n",
    "- **JSON Processing**: Semi-structured data handling\n",
    "- **User-Defined Functions**: Custom business logic implementation\n",
    "\n",
    "### 🔧 **Data Engineering Techniques:**\n",
    "- **Schema Evolution**: Handling changing data structures\n",
    "- **Data Quality**: Validation and cleansing patterns\n",
    "- **Incremental Processing**: Delta and change data capture\n",
    "- **Cross-Platform Patterns**: Local to cloud migration strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08669028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 6.1 Pivot Operations and Data Reshaping\n",
      "\n",
      "📊 Pivot Table: Sales by Region and Category:\n",
      "\n",
      "📊 Pivot Table: Sales by Region and Category:\n",
      "+------+----------+------+------+--------+-----------------+------+------------------+-------+------+-----+\n",
      "|region|Automotive|Beauty|Books |Clothing|Electronics      |Garden|Home              |Jewelry|Sports|Toys |\n",
      "+------+----------+------+------+--------+-----------------+------+------------------+-------+------+-----+\n",
      "|East  |0.0       |78.99 |148.87|0.0     |0.0              |0.0   |0.0               |0.0    |0.0   |125.0|\n",
      "|North |2345.0    |0.0   |0.0   |313.43  |0.0              |0.0   |0.0               |899.99 |0.0   |0.0  |\n",
      "|West  |0.0       |0.0   |0.0   |0.0     |5899.969999999999|0.0   |0.0               |0.0    |424.25|0.0  |\n",
      "|South |0.0       |0.0   |0.0   |0.0     |0.0              |189.99|1247.3899999999999|0.0    |0.0   |0.0  |\n",
      "+------+----------+------+------+--------+-----------------+------+------------------+-------+------+-----+\n",
      "\n",
      "\n",
      "📈 Monthly Sales Pivot by Category:\n",
      "+------+----------+------+------+--------+-----------------+------+------------------+-------+------+-----+\n",
      "|region|Automotive|Beauty|Books |Clothing|Electronics      |Garden|Home              |Jewelry|Sports|Toys |\n",
      "+------+----------+------+------+--------+-----------------+------+------------------+-------+------+-----+\n",
      "|East  |0.0       |78.99 |148.87|0.0     |0.0              |0.0   |0.0               |0.0    |0.0   |125.0|\n",
      "|North |2345.0    |0.0   |0.0   |313.43  |0.0              |0.0   |0.0               |899.99 |0.0   |0.0  |\n",
      "|West  |0.0       |0.0   |0.0   |0.0     |5899.969999999999|0.0   |0.0               |0.0    |424.25|0.0  |\n",
      "|South |0.0       |0.0   |0.0   |0.0     |0.0              |189.99|1247.3899999999999|0.0    |0.0   |0.0  |\n",
      "+------+----------+------+------+--------+-----------------+------+------------------+-------+------+-----+\n",
      "\n",
      "\n",
      "📈 Monthly Sales Pivot by Category:\n",
      "+-----------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "|category   |2024-01|2024-02|2024-03|2024-04|2024-05|2024-06|2024-07|2024-08|\n",
      "+-----------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "|Books      |45.99  |67.89  |0.0    |0.0    |34.99  |0.0    |0.0    |0.0    |\n",
      "|Home       |0.0    |234.5  |0.0    |445.0  |0.0    |0.0    |567.89 |0.0    |\n",
      "|Automotive |0.0    |0.0    |0.0    |0.0    |2345.0 |0.0    |0.0    |0.0    |\n",
      "|Beauty     |0.0    |0.0    |0.0    |78.99  |0.0    |0.0    |0.0    |0.0    |\n",
      "|Toys       |0.0    |0.0    |0.0    |0.0    |0.0    |0.0    |125.0  |0.0    |\n",
      "|Clothing   |89.99  |0.0    |123.45 |0.0    |0.0    |99.99  |0.0    |0.0    |\n",
      "|Jewelry    |0.0    |0.0    |0.0    |0.0    |0.0    |0.0    |0.0    |899.99 |\n",
      "|Electronics|1200.0 |899.99 |0.0    |1599.99|0.0    |0.0    |2199.99|0.0    |\n",
      "|Garden     |0.0    |0.0    |0.0    |0.0    |0.0    |189.99 |0.0    |0.0    |\n",
      "|Sports     |0.0    |0.0    |156.75 |0.0    |267.5  |0.0    |0.0    |0.0    |\n",
      "+-----------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "\n",
      "\n",
      "🎯 6.2 Complex Data Types and Nested Operations\n",
      "\n",
      "🏗️ Complex Nested Data Structures:\n",
      "+-----------+---------------------------------------------------------+\n",
      "|customer_id|summary                                                  |\n",
      "+-----------+---------------------------------------------------------+\n",
      "|101        |{4, 5899.969999999999, 1474.9924999999998, [Electronics]}|\n",
      "|102        |{3, 148.87, 49.623333333333335, [Books]}                 |\n",
      "|103        |{3, 313.43, 104.47666666666667, [Clothing]}              |\n",
      "|104        |{3, 1247.3899999999999, 415.7966666666666, [Home]}       |\n",
      "|105        |{2, 424.25, 212.125, [Sports]}                           |\n",
      "+-----------+---------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "📊 Extracting from Complex Structures:\n",
      "Extracted summary data:\n",
      "+-----------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "|category   |2024-01|2024-02|2024-03|2024-04|2024-05|2024-06|2024-07|2024-08|\n",
      "+-----------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "|Books      |45.99  |67.89  |0.0    |0.0    |34.99  |0.0    |0.0    |0.0    |\n",
      "|Home       |0.0    |234.5  |0.0    |445.0  |0.0    |0.0    |567.89 |0.0    |\n",
      "|Automotive |0.0    |0.0    |0.0    |0.0    |2345.0 |0.0    |0.0    |0.0    |\n",
      "|Beauty     |0.0    |0.0    |0.0    |78.99  |0.0    |0.0    |0.0    |0.0    |\n",
      "|Toys       |0.0    |0.0    |0.0    |0.0    |0.0    |0.0    |125.0  |0.0    |\n",
      "|Clothing   |89.99  |0.0    |123.45 |0.0    |0.0    |99.99  |0.0    |0.0    |\n",
      "|Jewelry    |0.0    |0.0    |0.0    |0.0    |0.0    |0.0    |0.0    |899.99 |\n",
      "|Electronics|1200.0 |899.99 |0.0    |1599.99|0.0    |0.0    |2199.99|0.0    |\n",
      "|Garden     |0.0    |0.0    |0.0    |0.0    |0.0    |189.99 |0.0    |0.0    |\n",
      "|Sports     |0.0    |0.0    |156.75 |0.0    |267.5  |0.0    |0.0    |0.0    |\n",
      "+-----------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "\n",
      "\n",
      "🎯 6.2 Complex Data Types and Nested Operations\n",
      "\n",
      "🏗️ Complex Nested Data Structures:\n",
      "+-----------+---------------------------------------------------------+\n",
      "|customer_id|summary                                                  |\n",
      "+-----------+---------------------------------------------------------+\n",
      "|101        |{4, 5899.969999999999, 1474.9924999999998, [Electronics]}|\n",
      "|102        |{3, 148.87, 49.623333333333335, [Books]}                 |\n",
      "|103        |{3, 313.43, 104.47666666666667, [Clothing]}              |\n",
      "|104        |{3, 1247.3899999999999, 415.7966666666666, [Home]}       |\n",
      "|105        |{2, 424.25, 212.125, [Sports]}                           |\n",
      "+-----------+---------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "📊 Extracting from Complex Structures:\n",
      "Extracted summary data:\n",
      "+-----------+------------------+------------------+------------------+----------+\n",
      "|customer_id|total_transactions|total_spent       |avg_transaction   |array_size|\n",
      "+-----------+------------------+------------------+------------------+----------+\n",
      "|101        |4                 |5899.969999999999 |1474.9924999999998|4         |\n",
      "|102        |3                 |148.87            |49.623333333333335|3         |\n",
      "|103        |3                 |313.43            |104.47666666666667|3         |\n",
      "|104        |3                 |1247.3899999999999|415.7966666666666 |3         |\n",
      "|105        |2                 |424.25            |212.125           |2         |\n",
      "|106        |1                 |78.99             |78.99             |1         |\n",
      "|107        |1                 |2345.0            |2345.0            |1         |\n",
      "|108        |1                 |189.99            |189.99            |1         |\n",
      "|109        |1                 |125.0             |125.0             |1         |\n",
      "|110        |1                 |899.99            |899.99            |1         |\n",
      "+-----------+------------------+------------------+------------------+----------+\n",
      "\n",
      "+-----------+------------------+------------------+------------------+----------+\n",
      "|customer_id|total_transactions|total_spent       |avg_transaction   |array_size|\n",
      "+-----------+------------------+------------------+------------------+----------+\n",
      "|101        |4                 |5899.969999999999 |1474.9924999999998|4         |\n",
      "|102        |3                 |148.87            |49.623333333333335|3         |\n",
      "|103        |3                 |313.43            |104.47666666666667|3         |\n",
      "|104        |3                 |1247.3899999999999|415.7966666666666 |3         |\n",
      "|105        |2                 |424.25            |212.125           |2         |\n",
      "|106        |1                 |78.99             |78.99             |1         |\n",
      "|107        |1                 |2345.0            |2345.0            |1         |\n",
      "|108        |1                 |189.99            |189.99            |1         |\n",
      "|109        |1                 |125.0             |125.0             |1         |\n",
      "|110        |1                 |899.99            |899.99            |1         |\n",
      "+-----------+------------------+------------------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Advanced DataFrame Patterns\n",
    "print(\"🎯 6.1 Pivot Operations and Data Reshaping\")\n",
    "\n",
    "# Pivot table: Categories by Region (fixed aggregation)\n",
    "pivot_by_region = sales_df.groupBy(\"region\").pivot(\"category\").agg(\n",
    "    sum(\"amount\")\n",
    ").fillna(0)\n",
    "\n",
    "print(\"\\n📊 Pivot Table: Sales by Region and Category:\")\n",
    "pivot_by_region.show(truncate=False)\n",
    "\n",
    "# Monthly sales pivot\n",
    "monthly_pivot = sales_df.withColumn(\"year_month\", \n",
    "    date_format(\"transaction_date\", \"yyyy-MM\")\n",
    ").groupBy(\"category\").pivot(\"year_month\").agg(\n",
    "    sum(\"amount\")\n",
    ").fillna(0)\n",
    "\n",
    "print(\"\\n📈 Monthly Sales Pivot by Category:\")\n",
    "monthly_pivot.show(truncate=False)\n",
    "\n",
    "print(\"\\n🎯 6.2 Complex Data Types and Nested Operations\")\n",
    "\n",
    "# Create complex nested structure (simplified to avoid duplicate keys)\n",
    "complex_data = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        COLLECT_LIST(\n",
    "            STRUCT(\n",
    "                transaction_date,\n",
    "                amount,\n",
    "                category,\n",
    "                region\n",
    "            )\n",
    "        ) as transactions,\n",
    "        \n",
    "        COLLECT_LIST(amount) as amounts_array,\n",
    "        \n",
    "        STRUCT(\n",
    "            COUNT(*) as total_transactions,\n",
    "            SUM(amount) as total_spent,\n",
    "            AVG(amount) as avg_transaction,\n",
    "            COLLECT_SET(category) as unique_categories\n",
    "        ) as summary\n",
    "        \n",
    "    FROM sales\n",
    "    GROUP BY customer_id\n",
    "    ORDER BY customer_id\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n🏗️ Complex Nested Data Structures:\")\n",
    "complex_data.select(\"customer_id\", \"summary\").show(5, truncate=False)\n",
    "\n",
    "# Extract from complex structures\n",
    "print(\"\\n📊 Extracting from Complex Structures:\")\n",
    "extracted_data = complex_data.select(\n",
    "    \"customer_id\",\n",
    "    col(\"summary.total_transactions\").alias(\"total_transactions\"),\n",
    "    col(\"summary.total_spent\").alias(\"total_spent\"),\n",
    "    col(\"summary.avg_transaction\").alias(\"avg_transaction\"),\n",
    "    size(\"amounts_array\").alias(\"array_size\")\n",
    ")\n",
    "\n",
    "print(\"Extracted summary data:\")\n",
    "extracted_data.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5798f0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 6.3 Performance Best Practices Summary\n",
      "\n",
      "⚡ WINDOW FUNCTION OPTIMIZATION:\n",
      "✅ DO: Use PARTITION BY to distribute data across partitions\n",
      "❌ AVOID: Global windows without partitioning (causes single partition warning)\n",
      "📊 Example: ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY date)\n",
      "\n",
      "🚀 GENERAL PERFORMANCE BEST PRACTICES:\n",
      "   📌 Partitioning: Always partition window functions by meaningful columns\n",
      "   📌 Caching: Cache DataFrames that are reused multiple times\n",
      "   📌 Column Selection: Select only needed columns early in transformations\n",
      "   📌 Predicate Pushdown: Apply filters as early as possible\n",
      "   📌 Broadcast Joins: Use broadcast() for small lookup tables\n",
      "   📌 Coalesce: Use coalesce() to reduce partition count after filters\n",
      "   📌 Explain Plans: Use explain() to understand query execution\n",
      "   📌 Adaptive Query Execution: Enable AQE for automatic optimizations\n",
      "   📌 Statistics: Collect table statistics for cost-based optimization\n",
      "   📌 Data Formats: Use columnar formats like Parquet for analytics\n",
      "\n",
      "🔧 OPTIMIZATION CONFIGURATION SETTINGS:\n",
      "   ⚙️  spark.sql.adaptive.enabled = true\n",
      "      └─ Enable Adaptive Query Execution\n",
      "   ⚙️  spark.sql.adaptive.coalescePartitions.enabled = true\n",
      "      └─ Coalesce small partitions\n",
      "   ⚙️  spark.sql.adaptive.skewJoin.enabled = true\n",
      "      └─ Handle data skew in joins\n",
      "   ⚙️  spark.sql.cbo.enabled = true\n",
      "      └─ Enable cost-based optimization\n",
      "   ⚙️  spark.sql.statistics.histogram.enabled = true\n",
      "      └─ Collect column histograms\n",
      "   ⚙️  spark.sql.shuffle.partitions = 200-400\n",
      "      └─ Adjust based on cluster size\n",
      "\n",
      "📊 MODULE 4 COMPLETION SUMMARY:\n",
      "✅ Window Functions: ROW_NUMBER, RANK, LAG, LEAD, running totals\n",
      "✅ Advanced SQL: CTEs, subqueries, CASE statements, set operations\n",
      "✅ Statistical Analysis: Descriptive stats, correlation, outlier detection\n",
      "✅ Time Series: Date functions, temporal aggregations, trend analysis\n",
      "✅ Performance: Window optimization, caching, query plan analysis\n",
      "✅ Advanced DataFrames: Pivot tables, complex data types, nested operations\n",
      "\n",
      "🎯 Ready for production PySpark analytics!\n",
      "📝 Total cells executed successfully: 27+\n",
      "⚡ Window performance warnings: RESOLVED with proper partitioning\n"
     ]
    }
   ],
   "source": [
    "# Performance Best Practices Summary\n",
    "print(\"🎯 6.3 Performance Best Practices Summary\")\n",
    "\n",
    "print(\"\\n⚡ WINDOW FUNCTION OPTIMIZATION:\")\n",
    "print(\"✅ DO: Use PARTITION BY to distribute data across partitions\")\n",
    "print(\"❌ AVOID: Global windows without partitioning (causes single partition warning)\")\n",
    "print(\"📊 Example: ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY date)\")\n",
    "\n",
    "print(\"\\n🚀 GENERAL PERFORMANCE BEST PRACTICES:\")\n",
    "\n",
    "best_practices = [\n",
    "    (\"Partitioning\", \"Always partition window functions by meaningful columns\"),\n",
    "    (\"Caching\", \"Cache DataFrames that are reused multiple times\"),\n",
    "    (\"Column Selection\", \"Select only needed columns early in transformations\"),\n",
    "    (\"Predicate Pushdown\", \"Apply filters as early as possible\"),\n",
    "    (\"Broadcast Joins\", \"Use broadcast() for small lookup tables\"),\n",
    "    (\"Coalesce\", \"Use coalesce() to reduce partition count after filters\"),\n",
    "    (\"Explain Plans\", \"Use explain() to understand query execution\"),\n",
    "    (\"Adaptive Query Execution\", \"Enable AQE for automatic optimizations\"),\n",
    "    (\"Statistics\", \"Collect table statistics for cost-based optimization\"),\n",
    "    (\"Data Formats\", \"Use columnar formats like Parquet for analytics\")\n",
    "]\n",
    "\n",
    "for practice, description in best_practices:\n",
    "    print(f\"   📌 {practice}: {description}\")\n",
    "\n",
    "print(\"\\n🔧 OPTIMIZATION CONFIGURATION SETTINGS:\")\n",
    "optimization_configs = [\n",
    "    (\"spark.sql.adaptive.enabled\", \"true\", \"Enable Adaptive Query Execution\"),\n",
    "    (\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\", \"Coalesce small partitions\"),\n",
    "    (\"spark.sql.adaptive.skewJoin.enabled\", \"true\", \"Handle data skew in joins\"),\n",
    "    (\"spark.sql.cbo.enabled\", \"true\", \"Enable cost-based optimization\"),\n",
    "    (\"spark.sql.statistics.histogram.enabled\", \"true\", \"Collect column histograms\"),\n",
    "    (\"spark.sql.shuffle.partitions\", \"200-400\", \"Adjust based on cluster size\")\n",
    "]\n",
    "\n",
    "for config, value, description in optimization_configs:\n",
    "    print(f\"   ⚙️  {config} = {value}\")\n",
    "    print(f\"      └─ {description}\")\n",
    "\n",
    "print(\"\\n📊 MODULE 4 COMPLETION SUMMARY:\")\n",
    "print(\"✅ Window Functions: ROW_NUMBER, RANK, LAG, LEAD, running totals\")\n",
    "print(\"✅ Advanced SQL: CTEs, subqueries, CASE statements, set operations\") \n",
    "print(\"✅ Statistical Analysis: Descriptive stats, correlation, outlier detection\")\n",
    "print(\"✅ Time Series: Date functions, temporal aggregations, trend analysis\")\n",
    "print(\"✅ Performance: Window optimization, caching, query plan analysis\")\n",
    "print(\"✅ Advanced DataFrames: Pivot tables, complex data types, nested operations\")\n",
    "\n",
    "print(f\"\\n🎯 Ready for production PySpark analytics!\")\n",
    "print(f\"📝 Total cells executed successfully: 27+\")\n",
    "print(f\"⚡ Window performance warnings: RESOLVED with proper partitioning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71122c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  ADDRESSING SPECIFIC WARNING:\n",
      "'No Partition Defined for Window operation! Moving all data to a single partition'\n",
      "\n",
      "🔍 PROBLEM ANALYSIS:\n",
      "❌ When window functions lack PARTITION BY clauses, Spark moves ALL data to a single partition\n",
      "❌ This causes severe performance degradation on large datasets\n",
      "❌ Single partition can't utilize cluster parallelism\n",
      "\n",
      "✅ SOLUTION DEMONSTRATION:\n",
      "\n",
      "📊 BEFORE (Causes Warning):\n",
      "   ROW_NUMBER() OVER (ORDER BY amount DESC)\n",
      "   ↳ Forces all data to single partition\n",
      "\n",
      "📊 AFTER (Optimized):\n",
      "   ROW_NUMBER() OVER (PARTITION BY region ORDER BY amount DESC)\n",
      "   ↳ Distributes data across multiple partitions by region\n",
      "\n",
      "🎯 PROPERLY PARTITIONED WINDOW EXAMPLES:\n",
      "✅ Customer-partitioned windows (no warnings):\n",
      "+-----------+----------------+-------+-----------------+----------------------+\n",
      "|customer_id|transaction_date|amount |purchase_sequence|running_customer_total|\n",
      "+-----------+----------------+-------+-----------------+----------------------+\n",
      "|101        |2024-01-15      |1200.0 |1                |1200.0                |\n",
      "|101        |2024-02-10      |899.99 |2                |2099.99               |\n",
      "|101        |2024-04-01      |1599.99|3                |3699.9799999999996    |\n",
      "|101        |2024-07-01      |2199.99|4                |5899.969999999999     |\n",
      "|102        |2024-01-16      |45.99  |1                |45.99                 |\n",
      "|102        |2024-02-15      |67.89  |2                |113.88                |\n",
      "|102        |2024-05-15      |34.99  |3                |148.87                |\n",
      "|104        |2024-02-12      |234.5  |1                |234.5                 |\n",
      "|104        |2024-04-10      |445.0  |2                |679.5                 |\n",
      "|104        |2024-07-15      |567.89 |3                |1247.3899999999999    |\n",
      "+-----------+----------------+-------+-----------------+----------------------+\n",
      "\n",
      "\n",
      "✅ Region-partitioned windows (no warnings):\n",
      "+-----------+----------------+-------+-----------------+----------------------+\n",
      "|customer_id|transaction_date|amount |purchase_sequence|running_customer_total|\n",
      "+-----------+----------------+-------+-----------------+----------------------+\n",
      "|101        |2024-01-15      |1200.0 |1                |1200.0                |\n",
      "|101        |2024-02-10      |899.99 |2                |2099.99               |\n",
      "|101        |2024-04-01      |1599.99|3                |3699.9799999999996    |\n",
      "|101        |2024-07-01      |2199.99|4                |5899.969999999999     |\n",
      "|102        |2024-01-16      |45.99  |1                |45.99                 |\n",
      "|102        |2024-02-15      |67.89  |2                |113.88                |\n",
      "|102        |2024-05-15      |34.99  |3                |148.87                |\n",
      "|104        |2024-02-12      |234.5  |1                |234.5                 |\n",
      "|104        |2024-04-10      |445.0  |2                |679.5                 |\n",
      "|104        |2024-07-15      |567.89 |3                |1247.3899999999999    |\n",
      "+-----------+----------------+-------+-----------------+----------------------+\n",
      "\n",
      "\n",
      "✅ Region-partitioned windows (no warnings):\n",
      "+------+----------+------+-----------+-----------------+\n",
      "|region|category  |amount|region_rank|region_percentile|\n",
      "+------+----------+------+-----------+-----------------+\n",
      "|East  |Toys      |125.0 |1          |1.0              |\n",
      "|East  |Beauty    |78.99 |2          |0.75             |\n",
      "|East  |Books     |67.89 |3          |0.5              |\n",
      "|East  |Books     |45.99 |4          |0.25             |\n",
      "|East  |Books     |34.99 |5          |0.0              |\n",
      "|North |Automotive|2345.0|1          |1.0              |\n",
      "|North |Jewelry   |899.99|2          |0.75             |\n",
      "|North |Clothing  |123.45|3          |0.5              |\n",
      "|North |Clothing  |99.99 |4          |0.25             |\n",
      "|North |Clothing  |89.99 |5          |0.0              |\n",
      "+------+----------+------+-----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "🚀 PERFORMANCE IMPACT:\n",
      "✅ Multi-partition: Each partition processes independently\n",
      "✅ Parallelism: Full cluster utilization\n",
      "✅ Scalability: Performance scales with cluster size\n",
      "✅ Memory: Distributed memory usage\n",
      "\n",
      "⚡ RESULT: Window performance warnings ELIMINATED!\n",
      "📊 All window operations now use proper partitioning for optimal performance\n",
      "+------+----------+------+-----------+-----------------+\n",
      "|region|category  |amount|region_rank|region_percentile|\n",
      "+------+----------+------+-----------+-----------------+\n",
      "|East  |Toys      |125.0 |1          |1.0              |\n",
      "|East  |Beauty    |78.99 |2          |0.75             |\n",
      "|East  |Books     |67.89 |3          |0.5              |\n",
      "|East  |Books     |45.99 |4          |0.25             |\n",
      "|East  |Books     |34.99 |5          |0.0              |\n",
      "|North |Automotive|2345.0|1          |1.0              |\n",
      "|North |Jewelry   |899.99|2          |0.75             |\n",
      "|North |Clothing  |123.45|3          |0.5              |\n",
      "|North |Clothing  |99.99 |4          |0.25             |\n",
      "|North |Clothing  |89.99 |5          |0.0              |\n",
      "+------+----------+------+-----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "🚀 PERFORMANCE IMPACT:\n",
      "✅ Multi-partition: Each partition processes independently\n",
      "✅ Parallelism: Full cluster utilization\n",
      "✅ Scalability: Performance scales with cluster size\n",
      "✅ Memory: Distributed memory usage\n",
      "\n",
      "⚡ RESULT: Window performance warnings ELIMINATED!\n",
      "📊 All window operations now use proper partitioning for optimal performance\n"
     ]
    }
   ],
   "source": [
    "# WINDOW PERFORMANCE WARNING RESOLUTION\n",
    "print(\"⚠️  ADDRESSING SPECIFIC WARNING:\")\n",
    "print(\"'No Partition Defined for Window operation! Moving all data to a single partition'\")\n",
    "\n",
    "print(\"\\n🔍 PROBLEM ANALYSIS:\")\n",
    "print(\"❌ When window functions lack PARTITION BY clauses, Spark moves ALL data to a single partition\")\n",
    "print(\"❌ This causes severe performance degradation on large datasets\")\n",
    "print(\"❌ Single partition can't utilize cluster parallelism\")\n",
    "\n",
    "print(\"\\n✅ SOLUTION DEMONSTRATION:\")\n",
    "\n",
    "# Show the difference with concrete examples\n",
    "print(\"\\n📊 BEFORE (Causes Warning):\")\n",
    "print(\"   ROW_NUMBER() OVER (ORDER BY amount DESC)\")\n",
    "print(\"   ↳ Forces all data to single partition\")\n",
    "\n",
    "print(\"\\n📊 AFTER (Optimized):\")\n",
    "print(\"   ROW_NUMBER() OVER (PARTITION BY region ORDER BY amount DESC)\")\n",
    "print(\"   ↳ Distributes data across multiple partitions by region\")\n",
    "\n",
    "# Demonstrate with actual queries that DON'T trigger warnings\n",
    "print(\"\\n🎯 PROPERLY PARTITIONED WINDOW EXAMPLES:\")\n",
    "\n",
    "# Example 1: Customer-level partitioning\n",
    "customer_partitioned = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        transaction_date,\n",
    "        amount,\n",
    "        ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY transaction_date) as purchase_sequence,\n",
    "        SUM(amount) OVER (PARTITION BY customer_id ORDER BY transaction_date \n",
    "                          ROWS UNBOUNDED PRECEDING) as running_customer_total\n",
    "    FROM sales\n",
    "    WHERE customer_id IN (101, 102, 104)\n",
    "    ORDER BY customer_id, transaction_date\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Customer-partitioned windows (no warnings):\")\n",
    "customer_partitioned.show(truncate=False)\n",
    "\n",
    "# Example 2: Region-level partitioning  \n",
    "print(\"\\n✅ Region-partitioned windows (no warnings):\")\n",
    "region_partitioned = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        region,\n",
    "        category,\n",
    "        amount,\n",
    "        RANK() OVER (PARTITION BY region ORDER BY amount DESC) as region_rank,\n",
    "        PERCENT_RANK() OVER (PARTITION BY region ORDER BY amount) as region_percentile\n",
    "    FROM sales\n",
    "    ORDER BY region, region_rank\n",
    "\"\"\")\n",
    "\n",
    "region_partitioned.show(10, truncate=False)\n",
    "\n",
    "print(\"\\n🚀 PERFORMANCE IMPACT:\")\n",
    "print(\"✅ Multi-partition: Each partition processes independently\")\n",
    "print(\"✅ Parallelism: Full cluster utilization\")  \n",
    "print(\"✅ Scalability: Performance scales with cluster size\")\n",
    "print(\"✅ Memory: Distributed memory usage\")\n",
    "\n",
    "print(\"\\n⚡ RESULT: Window performance warnings ELIMINATED!\")\n",
    "print(\"📊 All window operations now use proper partitioning for optimal performance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
