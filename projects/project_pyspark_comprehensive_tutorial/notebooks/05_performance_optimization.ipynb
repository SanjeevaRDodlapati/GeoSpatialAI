{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3947add6",
   "metadata": {},
   "source": [
    "# ðŸš€ Module 5: PySpark Performance Optimization\n",
    "*Comprehensive Guide to Optimizing PySpark Applications for Production*\n",
    "\n",
    "## ðŸ“‹ Learning Objectives\n",
    "By the end of this module, you will master:\n",
    "\n",
    "ðŸŽ¯ **Partitioning Strategies**\n",
    "- Hash, range, and custom partitioning\n",
    "- Bucketing for join optimization\n",
    "- Partition pruning techniques\n",
    "\n",
    "âš¡ **Caching & Persistence**\n",
    "- Storage levels and memory management\n",
    "- Checkpoint operations\n",
    "- When and how to cache effectively\n",
    "\n",
    "ðŸ”§ **Query Optimization**\n",
    "- Catalyst optimizer deep dive\n",
    "- Adaptive Query Execution (AQE)\n",
    "- Broadcast joins and predicate pushdown\n",
    "\n",
    "ðŸ’ª **Resource Management**\n",
    "- Dynamic allocation\n",
    "- Memory tuning and garbage collection\n",
    "- Parallelism optimization\n",
    "\n",
    "ðŸ“Š **Performance Monitoring**\n",
    "- Spark UI analysis\n",
    "- Metrics and monitoring tools\n",
    "- Bottleneck identification\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ—ï¸ Module Structure\n",
    "1. **Partitioning Strategies** - Data distribution optimization\n",
    "2. **Caching & Persistence** - Memory management techniques  \n",
    "3. **Query Optimization** - Catalyst and AQE optimization\n",
    "4. **Resource Management** - Cluster resource tuning\n",
    "5. **Performance Monitoring** - Real-time performance analysis\n",
    "6. **Production Best Practices** - Enterprise-ready optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbf9963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸš€ Module 5: PySpark Performance Optimization Setup\n",
    "print(\"ðŸ”§ Setting up PySpark Performance Optimization Environment...\")\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Configure Spark for performance optimization demonstrations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark-Performance-Optimization\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.default.parallelism\", \"8\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce noise\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"âœ… Spark Session Created with Performance Optimizations\")\n",
    "print(f\"ðŸŽ¯ Spark Version: {spark.version}\")\n",
    "print(f\"âš¡ Default Parallelism: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"ðŸ”„ Shuffle Partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print(f\"ðŸ§  AQE Enabled: {spark.conf.get('spark.sql.adaptive.enabled')}\")\n",
    "\n",
    "# Display current Spark configuration\n",
    "print(\"\\nðŸ“Š Key Performance Configurations:\")\n",
    "perf_configs = [\n",
    "    \"spark.sql.adaptive.enabled\",\n",
    "    \"spark.sql.adaptive.coalescePartitions.enabled\", \n",
    "    \"spark.sql.adaptive.skewJoin.enabled\",\n",
    "    \"spark.serializer\",\n",
    "    \"spark.sql.execution.arrow.pyspark.enabled\"\n",
    "]\n",
    "\n",
    "for config in perf_configs:\n",
    "    value = spark.conf.get(config, \"Not Set\")\n",
    "    print(f\"   {config}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3215e7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Generate Test Dataset for Performance Demonstrations\n",
    "print(\" Creating Performance Test Dataset...\")\n",
    "\n",
    "# Create a medium-sized dataset for performance testing\n",
    "from pyspark.sql.functions import rand, when, floor, date_add, lit\n",
    "from datetime import date\n",
    "\n",
    "# Generate synthetic sales data efficiently using Spark functions\n",
    "print(\" Generating synthetic sales data...\")\n",
    "\n",
    "# Create base DataFrame with sequential IDs\n",
    "base_df = spark.range(1, 100001).withColumnRenamed(\"id\", \"transaction_id\")\n",
    "\n",
    "# Add synthetic columns using Spark functions for better performance\n",
    "sales_df = base_df \\\n",
    "    .withColumn(\"customer_id\", floor(rand() * 50000).cast(\"int\")) \\\n",
    "    .withColumn(\"product_id\", floor(rand() * 10000).cast(\"int\")) \\\n",
    "    .withColumn(\"category\", \n",
    "                when(col(\"transaction_id\") % 6 == 0, \"Electronics\")\n",
    "                .when(col(\"transaction_id\") % 6 == 1, \"Clothing\") \n",
    "                .when(col(\"transaction_id\") % 6 == 2, \"Books\")\n",
    "                .when(col(\"transaction_id\") % 6 == 3, \"Home\")\n",
    "                .when(col(\"transaction_id\") % 6 == 4, \"Sports\")\n",
    "                .otherwise(\"Automotive\")) \\\n",
    "    .withColumn(\"region\",\n",
    "                when(col(\"transaction_id\") % 5 == 0, \"North\")\n",
    "                .when(col(\"transaction_id\") % 5 == 1, \"South\")\n",
    "                .when(col(\"transaction_id\") % 5 == 2, \"East\") \n",
    "                .when(col(\"transaction_id\") % 5 == 3, \"West\")\n",
    "                .otherwise(\"Central\")) \\\n",
    "    .withColumn(\"amount\", (rand() * 1990 + 10).cast(\"decimal(10,2)\")) \\\n",
    "    .withColumn(\"quantity\", floor(rand() * 10 + 1).cast(\"int\")) \\\n",
    "    .withColumn(\"transaction_date\", \n",
    "                date_add(lit(date(2023, 1, 1)), floor(rand() * 365).cast(\"int\"))) \\\n",
    "    .withColumn(\"discount_pct\", (rand() * 30).cast(\"decimal(5,2)\"))\n",
    "\n",
    "# Cache the DataFrame for reuse\n",
    "sales_df.cache()\n",
    "\n",
    "# Trigger action to materialize the data\n",
    "record_count = sales_df.count()\n",
    "\n",
    "print(f\"âœ… Performance Test Dataset Created\")\n",
    "print(f\"ðŸ“Š Records: {record_count:,}\")\n",
    "print(f\" Partitions: {sales_df.rdd.getNumPartitions()}\")\n",
    "print(f\"ðŸ’¾ Cached: {sales_df.is_cached}\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nðŸ” Sample Data:\")\n",
    "sales_df.show(5, truncate=False)\n",
    "\n",
    "print(\"\\nðŸ“‹ Schema:\")\n",
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7292566e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸŽ¯ Section 1: Partitioning Strategies\n",
    "\n",
    "## ðŸ“š Core Concepts\n",
    "\n",
    "**Partitioning** is fundamental to Spark performance. It determines:\n",
    "- How data is distributed across the cluster\n",
    "- Parallelism level for operations  \n",
    "- Network shuffle requirements\n",
    "- Join optimization opportunities\n",
    "\n",
    "### ðŸ”‘ Key Partitioning Types\n",
    "\n",
    "1. **Hash Partitioning** - Default for most operations\n",
    "2. **Range Partitioning** - Ordered data distribution\n",
    "3. **Custom Partitioning** - Application-specific logic\n",
    "4. **Bucketing** - Pre-partitioned storage optimization\n",
    "\n",
    "### âš¡ Performance Impact\n",
    "\n",
    "- **Good partitioning**: Parallel processing, minimal shuffles\n",
    "- **Poor partitioning**: Data skew, excessive network I/O, slow joins\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a633e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ Section 1.1: Analyzing Current Partitioning\n",
    "print(\"ðŸ” Analyzing Current Data Partitioning...\")\n",
    "\n",
    "# First, let's create a simple dataset to work with\n",
    "from pyspark.sql.functions import rand, when, floor, date_add, lit\n",
    "from datetime import date\n",
    "\n",
    "# Create sample dataset for partitioning demonstrations\n",
    "sample_df = spark.range(1, 50001) \\\n",
    "    .withColumnRenamed(\"id\", \"record_id\") \\\n",
    "    .withColumn(\"region\", when(col(\"record_id\") % 4 == 0, \"North\")\n",
    "                .when(col(\"record_id\") % 4 == 1, \"South\") \n",
    "                .when(col(\"record_id\") % 4 == 2, \"East\")\n",
    "                .otherwise(\"West\")) \\\n",
    "    .withColumn(\"category\", when(col(\"record_id\") % 3 == 0, \"A\")\n",
    "                .when(col(\"record_id\") % 3 == 1, \"B\")\n",
    "                .otherwise(\"C\")) \\\n",
    "    .withColumn(\"value\", (rand() * 1000).cast(\"decimal(10,2)\"))\n",
    "\n",
    "print(f\"âœ… Sample Dataset Created: {sample_df.count():,} records\")\n",
    "print(f\"ðŸ“Š Current Partitions: {sample_df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Analyze partition distribution\n",
    "print(\"\\nðŸ” Partition Distribution Analysis:\")\n",
    "partition_counts = sample_df.rdd.glom().map(len).collect()\n",
    "print(f\"Records per partition: {partition_counts}\")\n",
    "print(f\"Min records/partition: {min(partition_counts):,}\")\n",
    "print(f\"Max records/partition: {max(partition_counts):,}\")\n",
    "print(f\"Avg records/partition: {sum(partition_counts)/len(partition_counts):.1f}\")\n",
    "\n",
    "# Check for data skew by region\n",
    "print(\"\\nðŸ“ˆ Data Distribution by Region:\")\n",
    "sample_df.groupBy(\"region\").count().orderBy(\"region\").show()\n",
    "\n",
    "print(\"\\nðŸ“ˆ Data Distribution by Category:\")\n",
    "sample_df.groupBy(\"category\").count().orderBy(\"category\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1293eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ Section 1.2: Hash Partitioning Strategies\n",
    "print(\"ðŸ”— Demonstrating Hash Partitioning Techniques...\")\n",
    "\n",
    "# Create a dataset for partitioning demonstrations\n",
    "data_df = spark.range(1, 100001) \\\n",
    "    .withColumnRenamed(\"id\", \"customer_id\") \\\n",
    "    .withColumn(\"region\", when(col(\"customer_id\") % 5 == 0, \"North\")\n",
    "                .when(col(\"customer_id\") % 5 == 1, \"South\")\n",
    "                .when(col(\"customer_id\") % 5 == 2, \"East\") \n",
    "                .when(col(\"customer_id\") % 5 == 3, \"West\")\n",
    "                .otherwise(\"Central\")) \\\n",
    "    .withColumn(\"amount\", (rand() * 1000).cast(\"decimal(10,2)\"))\n",
    "\n",
    "print(f\"ðŸ“Š Original dataset: {data_df.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "# 1. Hash partitioning by region (optimal for region-based analytics)\n",
    "print(\"\\nðŸ”— Hash Partitioning by Region:\")\n",
    "hash_partitioned = data_df.repartition(8, \"region\")\n",
    "print(f\"   Partitions after repartition: {hash_partitioned.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Check distribution across partitions\n",
    "print(\"   Records per partition:\")\n",
    "partition_sizes = hash_partitioned.rdd.glom().map(len).collect()\n",
    "for i, size in enumerate(partition_sizes):\n",
    "    print(f\"     Partition {i}: {size:,} records\")\n",
    "\n",
    "# 2. Multiple column hash partitioning\n",
    "print(\"\\nðŸ”— Multi-Column Hash Partitioning:\")\n",
    "multi_hash = data_df.repartition(8, \"region\", \"customer_id\")\n",
    "print(f\"   Partitions: {multi_hash.rdd.getNumPartitions()}\")\n",
    "\n",
    "# 3. Compare performance for region-based aggregation\n",
    "print(\"\\nâš¡ Performance Comparison: Region Aggregation\")\n",
    "\n",
    "# Original partitioning\n",
    "start_time = time.time()\n",
    "result1 = data_df.groupBy(\"region\").agg(\n",
    "    count(\"*\").alias(\"total_customers\"),\n",
    "    sum(\"amount\").alias(\"total_amount\"),\n",
    "    avg(\"amount\").alias(\"avg_amount\")\n",
    ").collect()\n",
    "original_time = time.time() - start_time\n",
    "\n",
    "# Hash partitioned by region\n",
    "start_time = time.time()\n",
    "result2 = hash_partitioned.groupBy(\"region\").agg(\n",
    "    count(\"*\").alias(\"total_customers\"), \n",
    "    sum(\"amount\").alias(\"total_amount\"),\n",
    "    avg(\"amount\").alias(\"avg_amount\")\n",
    ").collect()\n",
    "hash_time = time.time() - start_time\n",
    "\n",
    "print(f\"   Original partitioning: {original_time:.3f}s\")\n",
    "print(f\"   Hash partitioned: {hash_time:.3f}s\")\n",
    "print(f\"   Improvement: {((original_time - hash_time) / original_time * 100):.1f}%\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nðŸ“ˆ Aggregation Results:\")\n",
    "result_df = spark.createDataFrame(result2)\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a632e68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ Section 1.3: Range Partitioning for Sorted Data\n",
    "print(\"ðŸ“Š Demonstrating Range Partitioning...\")\n",
    "\n",
    "# Range partitioning is optimal for:\n",
    "# - Time series data\n",
    "# - Ordered data access patterns  \n",
    "# - Range queries\n",
    "\n",
    "# Create time series dataset\n",
    "from pyspark.sql.functions import date_add, lit, expr\n",
    "from datetime import date\n",
    "\n",
    "time_series_df = spark.range(1, 100001) \\\n",
    "    .withColumnRenamed(\"id\", \"event_id\") \\\n",
    "    .withColumn(\"event_date\", \n",
    "                date_add(lit(date(2023, 1, 1)), \n",
    "                        floor(col(\"event_id\") / 274).cast(\"int\"))) \\\n",
    "    .withColumn(\"sensor_id\", floor(rand() * 100).cast(\"int\")) \\\n",
    "    .withColumn(\"temperature\", (rand() * 40 + 10).cast(\"decimal(5,2)\")) \\\n",
    "    .withColumn(\"humidity\", (rand() * 100).cast(\"decimal(5,2)\"))\n",
    "\n",
    "print(f\"ðŸ“… Time Series Dataset: {time_series_df.count():,} records\")\n",
    "print(f\"ðŸ“Š Original partitions: {time_series_df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Show date range\n",
    "date_range = time_series_df.agg(\n",
    "    min(\"event_date\").alias(\"start_date\"),\n",
    "    max(\"event_date\").alias(\"end_date\")\n",
    ").collect()[0]\n",
    "print(f\"ðŸ“… Date range: {date_range['start_date']} to {date_range['end_date']}\")\n",
    "\n",
    "# 1. Range partition by date (optimal for time-based queries)\n",
    "print(\"\\nðŸ“Š Range Partitioning by Date:\")\n",
    "range_partitioned = time_series_df.repartitionByRange(8, \"event_date\")\n",
    "print(f\"   Partitions: {range_partitioned.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Check how data is distributed across partitions\n",
    "print(\"   Data distribution by partition:\")\n",
    "partition_dates = []\n",
    "partitions = range_partitioned.rdd.glom().collect()\n",
    "for i, partition_data in enumerate(partitions):\n",
    "    if partition_data:\n",
    "        dates = [row.event_date for row in partition_data]\n",
    "        min_date = min(dates)\n",
    "        max_date = max(dates)\n",
    "        print(f\"     Partition {i}: {len(partition_data):,} records ({min_date} to {max_date})\")\n",
    "    else:\n",
    "        print(f\"     Partition {i}: 0 records (empty)\")\n",
    "\n",
    "# 2. Performance comparison for date range queries\n",
    "print(\"\\nâš¡ Performance Test: Date Range Query\")\n",
    "\n",
    "# Test query: Get data for a specific month\n",
    "test_date_start = date(2023, 6, 1)\n",
    "test_date_end = date(2023, 6, 30)\n",
    "\n",
    "# Original partitioning\n",
    "start_time = time.time()\n",
    "result1 = time_series_df.filter(\n",
    "    (col(\"event_date\") >= lit(test_date_start)) & \n",
    "    (col(\"event_date\") <= lit(test_date_end))\n",
    ").agg(\n",
    "    count(\"*\").alias(\"records\"),\n",
    "    avg(\"temperature\").alias(\"avg_temp\"),\n",
    "    avg(\"humidity\").alias(\"avg_humidity\")\n",
    ").collect()[0]\n",
    "original_time = time.time() - start_time\n",
    "\n",
    "# Range partitioned\n",
    "start_time = time.time() \n",
    "result2 = range_partitioned.filter(\n",
    "    (col(\"event_date\") >= lit(test_date_start)) & \n",
    "    (col(\"event_date\") <= lit(test_date_end))\n",
    ").agg(\n",
    "    count(\"*\").alias(\"records\"),\n",
    "    avg(\"temperature\").alias(\"avg_temp\"), \n",
    "    avg(\"humidity\").alias(\"avg_humidity\")\n",
    ").collect()[0]\n",
    "range_time = time.time() - start_time\n",
    "\n",
    "print(f\"   Original partitioning: {original_time:.3f}s\")\n",
    "print(f\"   Range partitioned: {range_time:.3f}s\")\n",
    "print(f\"   Query result: {result2['records']:,} records, Temp: {result2['avg_temp']:.1f}Â°C\")\n",
    "\n",
    "# 3. Partition pruning demonstration\n",
    "print(\"\\nðŸŽ¯ Partition Pruning Benefits:\")\n",
    "print(\"   Range partitioning enables partition pruning for date queries\")\n",
    "print(\"   - Only relevant partitions are read\")\n",
    "print(\"   - Significant I/O reduction for large datasets\")\n",
    "print(\"   - Better for time series analytics and reporting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33547614",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ’¾ Section 2: Caching & Persistence Strategies\n",
    "\n",
    "## ðŸŽ¯ Key Concepts\n",
    "\n",
    "**Caching** stores DataFrames in memory/disk for faster subsequent access:\n",
    "- **Memory-only**: Fastest access, limited by available memory\n",
    "- **Memory + Disk**: Spills to disk when memory full\n",
    "- **Disk-only**: Slower but handles large datasets\n",
    "- **Serialized**: Compressed storage, slower access\n",
    "\n",
    "### ðŸ”‘ Storage Levels\n",
    "\n",
    "| Level | Memory | Disk | Serialized | Replication |\n",
    "|-------|---------|------|------------|-------------|\n",
    "| `MEMORY_ONLY` | âœ… | âŒ | âŒ | 1x |\n",
    "| `MEMORY_AND_DISK` | âœ… | âœ… | âŒ | 1x |\n",
    "| `MEMORY_ONLY_SER` | âœ… | âŒ | âœ… | 1x |\n",
    "| `DISK_ONLY` | âŒ | âœ… | âŒ | 1x |\n",
    "| `MEMORY_AND_DISK_2` | âœ… | âœ… | âŒ | 2x |\n",
    "\n",
    "### âš¡ When to Cache\n",
    "\n",
    "âœ… **Good candidates:**\n",
    "- DataFrames used multiple times\n",
    "- Intermediate results in iterative algorithms\n",
    "- Lookup tables and dimension data\n",
    "- Expensive computations\n",
    "\n",
    "âŒ **Avoid caching:**\n",
    "- Data used only once\n",
    "- Very large datasets (memory pressure)\n",
    "- Simple transformations (filtering, selecting)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab72382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ’¾ Section 2.1: Caching Performance Demonstration\n",
    "print(\"ðŸ”„ Demonstrating Caching Performance Benefits...\")\n",
    "\n",
    "# Create a computational expensive DataFrame\n",
    "expensive_df = spark.range(1, 200001) \\\n",
    "    .withColumnRenamed(\"id\", \"transaction_id\") \\\n",
    "    .withColumn(\"customer_segment\", \n",
    "                when(col(\"transaction_id\") % 10 == 0, \"Premium\")\n",
    "                .when(col(\"transaction_id\") % 5 == 0, \"Gold\") \n",
    "                .otherwise(\"Standard\")) \\\n",
    "    .withColumn(\"complex_calc\", \n",
    "                # Simulate expensive computation\n",
    "                sqrt(col(\"transaction_id\")) * sin(col(\"transaction_id\") / 1000) + \n",
    "                cos(col(\"transaction_id\") / 500)) \\\n",
    "    .withColumn(\"amount\", (rand() * 2000 + 100).cast(\"decimal(10,2)\"))\n",
    "\n",
    "print(f\"ðŸ“Š Expensive DataFrame created: {expensive_df.count():,} records\")\n",
    "\n",
    "# Test 1: Without caching - multiple operations\n",
    "print(\"\\nðŸš« Performance WITHOUT Caching:\")\n",
    "start_time = time.time()\n",
    "\n",
    "# First operation\n",
    "result1 = expensive_df.groupBy(\"customer_segment\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    avg(\"amount\").alias(\"avg_amount\")\n",
    ").collect()\n",
    "\n",
    "# Second operation  \n",
    "result2 = expensive_df.filter(col(\"amount\") > 1000).count()\n",
    "\n",
    "# Third operation\n",
    "result3 = expensive_df.agg(\n",
    "    sum(\"complex_calc\").alias(\"total_calc\"),\n",
    "    max(\"amount\").alias(\"max_amount\")\n",
    ").collect()[0]\n",
    "\n",
    "no_cache_time = time.time() - start_time\n",
    "print(f\"   Total time (3 operations): {no_cache_time:.3f}s\")\n",
    "\n",
    "# Test 2: With caching - same operations\n",
    "print(\"\\nâœ… Performance WITH Caching:\")\n",
    "cached_df = expensive_df.cache()\n",
    "\n",
    "# Trigger caching with first action\n",
    "cache_start = time.time()\n",
    "cached_count = cached_df.count()\n",
    "cache_load_time = time.time() - cache_start\n",
    "\n",
    "# Now run the same operations\n",
    "start_time = time.time()\n",
    "\n",
    "result1_cached = cached_df.groupBy(\"customer_segment\").agg(\n",
    "    count(\"*\").alias(\"count\"), \n",
    "    avg(\"amount\").alias(\"avg_amount\")\n",
    ").collect()\n",
    "\n",
    "result2_cached = cached_df.filter(col(\"amount\") > 1000).count()\n",
    "\n",
    "result3_cached = cached_df.agg(\n",
    "    sum(\"complex_calc\").alias(\"total_calc\"),\n",
    "    max(\"amount\").alias(\"max_amount\")\n",
    ").collect()[0]\n",
    "\n",
    "cached_time = time.time() - start_time\n",
    "total_cached_time = cache_load_time + cached_time\n",
    "\n",
    "print(f\"   Cache loading time: {cache_load_time:.3f}s\")\n",
    "print(f\"   Operations time: {cached_time:.3f}s\")\n",
    "print(f\"   Total time: {total_cached_time:.3f}s\")\n",
    "\n",
    "# Performance comparison\n",
    "print(f\"\\nâš¡ Performance Improvement:\")\n",
    "if total_cached_time < no_cache_time:\n",
    "    improvement = ((no_cache_time - total_cached_time) / no_cache_time) * 100\n",
    "    print(f\"   Speedup: {improvement:.1f}% faster with caching\")\n",
    "else:\n",
    "    overhead = ((total_cached_time - no_cache_time) / no_cache_time) * 100\n",
    "    print(f\"   Overhead: {overhead:.1f}% slower (cache loading cost)\")\n",
    "\n",
    "print(f\"   Break-even: Cached approach faster after 2+ operations\")\n",
    "\n",
    "# Display cache statistics\n",
    "print(f\"\\nðŸ“ˆ Cache Statistics:\")\n",
    "print(f\"   Dataset cached: {cached_df.is_cached}\")\n",
    "print(f\"   Storage level: {cached_df.storageLevel}\")\n",
    "print(f\"   Records cached: {cached_count:,}\")\n",
    "\n",
    "# Show results verification\n",
    "print(f\"\\nâœ… Results Verification:\")\n",
    "print(f\"   High-value transactions: {result2:,} (no cache) vs {result2_cached:,} (cached)\")\n",
    "print(f\"   Results match: {result2 == result2_cached}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1caa4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ’¾ Section 2.2: Storage Levels Comparison\n",
    "print(\"ðŸ” Comparing Different Storage Levels...\")\n",
    "\n",
    "# Create test dataset\n",
    "test_df = spark.range(1, 50001) \\\n",
    "    .withColumnRenamed(\"id\", \"record_id\") \\\n",
    "    .withColumn(\"data\", concat(lit(\"DATA_\"), col(\"record_id\").cast(\"string\"))) \\\n",
    "    .withColumn(\"value\", (rand() * 1000).cast(\"decimal(10,2)\")) \\\n",
    "    .withColumn(\"category\", when(col(\"record_id\") % 3 == 0, \"A\")\n",
    "                .when(col(\"record_id\") % 3 == 1, \"B\")\n",
    "                .otherwise(\"C\"))\n",
    "\n",
    "print(f\"ðŸ“Š Test dataset: {test_df.count():,} records\")\n",
    "\n",
    "# Test different storage levels\n",
    "storage_levels = {\n",
    "    \"MEMORY_ONLY\": StorageLevel.MEMORY_ONLY,\n",
    "    \"MEMORY_AND_DISK\": StorageLevel.MEMORY_AND_DISK, \n",
    "    \"MEMORY_ONLY_SER\": StorageLevel.MEMORY_ONLY_SER,\n",
    "    \"DISK_ONLY\": StorageLevel.DISK_ONLY\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for level_name, storage_level in storage_levels.items():\n",
    "    print(f\"\\nðŸ” Testing {level_name}:\")\n",
    "    \n",
    "    # Create DataFrame with specific storage level\n",
    "    test_cached = test_df.persist(storage_level)\n",
    "    \n",
    "    # Time the caching operation\n",
    "    start_time = time.time()\n",
    "    count = test_cached.count()  # Trigger caching\n",
    "    cache_time = time.time() - start_time\n",
    "    \n",
    "    # Time a simple operation  \n",
    "    start_time = time.time()\n",
    "    agg_result = test_cached.groupBy(\"category\").count().collect()\n",
    "    operation_time = time.time() - start_time\n",
    "    \n",
    "    results[level_name] = {\n",
    "        \"cache_time\": cache_time,\n",
    "        \"operation_time\": operation_time,\n",
    "        \"storage_level\": storage_level\n",
    "    }\n",
    "    \n",
    "    print(f\"   Cache time: {cache_time:.3f}s\")\n",
    "    print(f\"   Operation time: {operation_time:.3f}s\")\n",
    "    print(f\"   Storage level: {storage_level}\")\n",
    "    \n",
    "    # Unpersist to clean up\n",
    "    test_cached.unpersist()\n",
    "\n",
    "# Summary comparison\n",
    "print(f\"\\nðŸ“Š Storage Level Performance Summary:\")\n",
    "print(f\"{'Level':<20} {'Cache Time':<12} {'Op Time':<10} {'Total':<10}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for level_name, metrics in results.items():\n",
    "    total_time = metrics[\"cache_time\"] + metrics[\"operation_time\"]\n",
    "    print(f\"{level_name:<20} {metrics['cache_time']:<12.3f} {metrics['operation_time']:<10.3f} {total_time:<10.3f}\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\nðŸ’¡ Storage Level Recommendations:\")\n",
    "print(f\"   ðŸš€ MEMORY_ONLY: Fastest for datasets that fit in memory\")\n",
    "print(f\"   âš–ï¸  MEMORY_AND_DISK: Best balance for most use cases\")\n",
    "print(f\"   ðŸ’¾ MEMORY_ONLY_SER: Memory-efficient for large datasets\")\n",
    "print(f\"   ðŸŒ DISK_ONLY: Slowest but handles very large datasets\")\n",
    "\n",
    "# Cache management best practices\n",
    "print(f\"\\nðŸŽ¯ Cache Management Best Practices:\")\n",
    "print(f\"   1. Monitor memory usage with Spark UI\")\n",
    "print(f\"   2. Unpersist DataFrames when no longer needed\")\n",
    "print(f\"   3. Use broadcast for small lookup tables\")\n",
    "print(f\"   4. Consider serialization for memory-constrained environments\")\n",
    "print(f\"   5. Test different storage levels for your use case\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
