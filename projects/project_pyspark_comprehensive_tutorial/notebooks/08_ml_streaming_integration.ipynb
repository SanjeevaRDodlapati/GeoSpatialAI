{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c226389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 8: ML with Streaming - Environment Setup\n",
    "print(\"Setting up Machine Learning + Streaming Environment...\")\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Iterator\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Core PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.streaming import StreamingQuery\n",
    "\n",
    "# MLlib imports for machine learning\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# Advanced ML imports\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf, pandas_udf, PandasUDFType\n",
    "\n",
    "# Configure Spark for ML + Streaming workloads\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark-ML-Streaming-Integration\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/ml-streaming-checkpoints\") \\\n",
    "    .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .config(\"spark.default.parallelism\", \"4\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.ml.streaming.numPartitions\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level for cleaner output\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"ML + Streaming Session Created\")\n",
    "print(\"Spark Version: {}\".format(spark.version))\n",
    "print(\"ML streaming checkpoint location: /tmp/ml-streaming-checkpoints\")\n",
    "\n",
    "# Create directories for ML models and outputs\n",
    "model_dir = \"/tmp/ml_streaming_models\"\n",
    "output_dir = \"/tmp/ml_streaming_output\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Model directory: {model_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# Display ML streaming configurations\n",
    "print(\"\\nML + Streaming Configuration:\")\n",
    "ml_configs = [\n",
    "    \"spark.sql.adaptive.enabled\",\n",
    "    \"spark.sql.streaming.checkpointLocation\", \n",
    "    \"spark.default.parallelism\",\n",
    "    \"spark.ml.streaming.numPartitions\"\n",
    "]\n",
    "\n",
    "for config in ml_configs:\n",
    "    value = spark.conf.get(config, \"Not Set\")\n",
    "    print(\"   {}: {}\".format(config, value))\n",
    "\n",
    "print(\"\\nML + Streaming environment ready!\")\n",
    "print(\"Ready for real-time machine learning pipelines and inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3feba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-time Model Inference on Streaming Data\n",
    "print(\"Setting up real-time ML inference pipeline...\")\n",
    "\n",
    "print(\"=== 1. Training Initial Model ===\")\n",
    "\n",
    "# Create training data for a fraud detection model\n",
    "training_data = spark.createDataFrame([\n",
    "    (1.2, 45.0, 2, 150.0, 0),   # Normal transaction\n",
    "    (15.5, 67.0, 1, 2500.0, 1), # Fraudulent \n",
    "    (0.8, 23.0, 3, 89.0, 0),    # Normal\n",
    "    (22.3, 45.0, 1, 5000.0, 1), # Fraudulent\n",
    "    (2.1, 34.0, 2, 200.0, 0),   # Normal\n",
    "    (18.7, 56.0, 1, 3200.0, 1), # Fraudulent\n",
    "    (1.5, 29.0, 4, 95.0, 0),    # Normal\n",
    "    (25.1, 78.0, 1, 7500.0, 1), # Fraudulent\n",
    "    (0.9, 41.0, 3, 120.0, 0),   # Normal\n",
    "    (19.8, 52.0, 1, 4100.0, 1), # Fraudulent\n",
    "], [\"transaction_velocity\", \"age\", \"num_locations\", \"amount\", \"is_fraud\"])\n",
    "\n",
    "print(\"Created training dataset with fraud detection features\")\n",
    "\n",
    "# Prepare features for ML pipeline\n",
    "feature_cols = [\"transaction_velocity\", \"age\", \"num_locations\", \"amount\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "lr = LogisticRegression(featuresCol=\"scaled_features\", labelCol=\"is_fraud\", predictionCol=\"fraud_prediction\")\n",
    "\n",
    "# Create and train the pipeline\n",
    "pipeline = Pipeline(stages=[assembler, scaler, lr])\n",
    "model = pipeline.fit(training_data)\n",
    "\n",
    "# Save the model for streaming inference\n",
    "model_path = f\"{model_dir}/fraud_detection_model\"\n",
    "model.write().overwrite().save(model_path)\n",
    "print(f\"Fraud detection model trained and saved to: {model_path}\")\n",
    "\n",
    "# Test the model on training data\n",
    "predictions = model.transform(training_data)\n",
    "predictions.select(\"amount\", \"is_fraud\", \"fraud_prediction\", \"probability\").show()\n",
    "\n",
    "print(\"=== 2. Real-time Streaming Inference ===\")\n",
    "\n",
    "# Create streaming data that simulates real-time transactions\n",
    "streaming_transactions = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 3) \\\n",
    "    .load() \\\n",
    "    .withColumn(\"transaction_velocity\", (rand() * 30)) \\\n",
    "    .withColumn(\"age\", (rand() * 80 + 18)) \\\n",
    "    .withColumn(\"num_locations\", (rand() * 5 + 1).cast(\"int\")) \\\n",
    "    .withColumn(\"amount\", (rand() * 10000 + 50)) \\\n",
    "    .withColumn(\"transaction_id\", concat(lit(\"txn_\"), col(\"value\").cast(\"string\"))) \\\n",
    "    .withColumn(\"transaction_time\", col(\"timestamp\")) \\\n",
    "    .select(\"transaction_id\", \"transaction_velocity\", \"age\", \"num_locations\", \"amount\", \"transaction_time\")\n",
    "\n",
    "print(\"Created streaming transaction data source\")\n",
    "\n",
    "# Load the saved model for inference\n",
    "loaded_model = Pipeline.load(model_path)\n",
    "\n",
    "# Apply real-time fraud detection\n",
    "fraud_predictions = loaded_model.transform(streaming_transactions) \\\n",
    "    .withColumn(\"fraud_risk\", \n",
    "        when(col(\"fraud_prediction\") == 1.0, \"HIGH\")\n",
    "        .when(col(\"probability\").getItem(1) > 0.7, \"MEDIUM\")\n",
    "        .otherwise(\"LOW\")) \\\n",
    "    .select(\"transaction_id\", \"amount\", \"fraud_prediction\", \"fraud_risk\", \"transaction_time\")\n",
    "\n",
    "# Start real-time inference query\n",
    "inference_query = fraud_predictions \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(\"Real-time fraud detection started!\")\n",
    "print(\"Processing transactions and predicting fraud risk...\")\n",
    "\n",
    "# Let the inference run for demonstration\n",
    "time.sleep(20)\n",
    "\n",
    "print(\"\\nReal-time ML inference demonstration complete!\")\n",
    "print(\"Showed how to apply pre-trained models to streaming data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4d4639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online Learning and Model Updates\n",
    "print(\"Setting up online learning pipeline...\")\n",
    "\n",
    "# Stop previous query\n",
    "inference_query.stop()\n",
    "time.sleep(2)\n",
    "\n",
    "print(\"=== 3. Incremental Learning with Streaming Data ===\")\n",
    "\n",
    "# Create a streaming data source with labeled examples for online learning\n",
    "online_learning_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 2) \\\n",
    "    .load() \\\n",
    "    .withColumn(\"feature1\", rand() * 10) \\\n",
    "    .withColumn(\"feature2\", rand() * 20) \\\n",
    "    .withColumn(\"feature3\", (rand() * 100).cast(\"int\")) \\\n",
    "    .withColumn(\"true_label\", \n",
    "        when((col(\"feature1\") > 5) & (col(\"feature2\") > 10), 1).otherwise(0)) \\\n",
    "    .withColumn(\"batch_id\", (col(\"value\") / 10).cast(\"int\")) \\\n",
    "    .select(\"feature1\", \"feature2\", \"feature3\", \"true_label\", \"batch_id\", \"timestamp\")\n",
    "\n",
    "print(\"Created streaming data source with ground truth labels\")\n",
    "\n",
    "# Define a function to retrain model on each batch\n",
    "def update_model(batch_df, batch_id):\n",
    "    \"\"\"\n",
    "    Function to incrementally update the model with new data\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Processing Batch {batch_id} for Model Update ---\")\n",
    "    \n",
    "    if batch_df.count() == 0:\n",
    "        print(\"Empty batch, skipping model update\")\n",
    "        return\n",
    "    \n",
    "    # Show current batch statistics\n",
    "    print(f\"Batch size: {batch_df.count()} records\")\n",
    "    batch_df.groupBy(\"true_label\").count().show()\n",
    "    \n",
    "    # Prepare features for this batch\n",
    "    feature_assembler = VectorAssembler(\n",
    "        inputCols=[\"feature1\", \"feature2\", \"feature3\"],\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "    \n",
    "    batch_features = feature_assembler.transform(batch_df)\n",
    "    \n",
    "    # Train/update model on this batch\n",
    "    lr_incremental = LogisticRegression(\n",
    "        featuresCol=\"features\", \n",
    "        labelCol=\"true_label\",\n",
    "        predictionCol=\"prediction\",\n",
    "        maxIter=10\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Train model on current batch\n",
    "        incremental_model = lr_incremental.fit(batch_features)\n",
    "        \n",
    "        # Make predictions on the same batch to evaluate\n",
    "        predictions = incremental_model.transform(batch_features)\n",
    "        \n",
    "        # Calculate accuracy for this batch\n",
    "        correct_predictions = predictions.filter(col(\"prediction\") == col(\"true_label\")).count()\n",
    "        total_predictions = predictions.count()\n",
    "        accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "        \n",
    "        print(f\"Batch accuracy: {accuracy:.3f}\")\n",
    "        print(f\"Model coefficients: {incremental_model.stages[-1].coefficients}\")\n",
    "        \n",
    "        # In production, you would save the updated model here\n",
    "        # incremental_model.write().overwrite().save(f\"{model_dir}/online_model_batch_{batch_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error training on batch {batch_id}: {e}\")\n",
    "\n",
    "# Start online learning with foreachBatch\n",
    "online_learning_query = online_learning_stream \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(update_model) \\\n",
    "    .trigger(processingTime='8 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(\"Online learning pipeline started!\")\n",
    "print(\"Model will be retrained on each new batch of data...\")\n",
    "\n",
    "# Let it run to see model updates\n",
    "time.sleep(30)\n",
    "online_learning_query.stop()\n",
    "\n",
    "print(\"\\n=== 4. Concept Drift Detection ===\")\n",
    "\n",
    "# Create a simple concept drift detection example\n",
    "concept_drift_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 4) \\\n",
    "    .load() \\\n",
    "    .withColumn(\"feature\", rand() * 100) \\\n",
    "    .withColumn(\"time_period\", (col(\"value\") / 20).cast(\"int\")) \\\n",
    "    .withColumn(\"drifted_label\",\n",
    "        # Simulate concept drift: pattern changes over time\n",
    "        when(col(\"time_period\") < 2, \n",
    "             when(col(\"feature\") > 50, 1).otherwise(0))  # Original pattern\n",
    "        .otherwise(\n",
    "             when(col(\"feature\") > 30, 1).otherwise(0))) # Drifted pattern\n",
    "    .select(\"feature\", \"drifted_label\", \"time_period\", \"timestamp\")\n",
    "\n",
    "def detect_concept_drift(batch_df, batch_id):\n",
    "    \"\"\"\n",
    "    Simple concept drift detection based on label distribution changes\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Concept Drift Analysis - Batch {batch_id} ---\")\n",
    "    \n",
    "    if batch_df.count() == 0:\n",
    "        return\n",
    "    \n",
    "    # Calculate label distribution for current batch\n",
    "    current_dist = batch_df.groupBy(\"drifted_label\").count().collect()\n",
    "    \n",
    "    if len(current_dist) >= 2:\n",
    "        total = sum([row['count'] for row in current_dist])\n",
    "        positive_ratio = next((row['count']/total for row in current_dist if row['drifted_label'] == 1), 0)\n",
    "        \n",
    "        print(f\"Positive label ratio: {positive_ratio:.3f}\")\n",
    "        \n",
    "        # Simple drift detection: if ratio changes significantly from expected 0.5\n",
    "        if abs(positive_ratio - 0.5) > 0.2:\n",
    "            print(\"âš ï¸  CONCEPT DRIFT DETECTED!\")\n",
    "            print(\"   Consider retraining the model with recent data\")\n",
    "        else:\n",
    "            print(\"âœ… No significant concept drift detected\")\n",
    "            \n",
    "        # Show time period distribution to visualize drift\n",
    "        batch_df.groupBy(\"time_period\", \"drifted_label\").count().orderBy(\"time_period\").show()\n",
    "\n",
    "# Start concept drift monitoring\n",
    "drift_query = concept_drift_stream \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(detect_concept_drift) \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(\"Concept drift detection started!\")\n",
    "print(\"Monitoring for changes in data patterns...\")\n",
    "\n",
    "time.sleep(25)\n",
    "drift_query.stop()\n",
    "\n",
    "print(\"\\nOnline learning and concept drift detection complete!\")\n",
    "print(\"Demonstrated incremental model updates and drift monitoring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00512ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-time Feature Engineering for ML\n",
    "print(\"Setting up real-time feature engineering pipeline...\")\n",
    "\n",
    "print(\"=== 5. Streaming Feature Engineering ===\")\n",
    "\n",
    "# Create a streaming source for e-commerce events\n",
    "ecommerce_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 5) \\\n",
    "    .load() \\\n",
    "    .withColumn(\"user_id\", (col(\"value\") % 100).cast(\"string\")) \\\n",
    "    .withColumn(\"product_id\", concat(lit(\"prod_\"), ((col(\"value\") % 50) + 1).cast(\"string\"))) \\\n",
    "    .withColumn(\"action\", \n",
    "        when(col(\"value\") % 4 == 0, \"view\")\n",
    "        .when(col(\"value\") % 4 == 1, \"click\")\n",
    "        .when(col(\"value\") % 4 == 2, \"add_to_cart\")\n",
    "        .otherwise(\"purchase\")) \\\n",
    "    .withColumn(\"price\", rand() * 1000 + 10) \\\n",
    "    .withColumn(\"event_time\", col(\"timestamp\")) \\\n",
    "    .select(\"user_id\", \"product_id\", \"action\", \"price\", \"event_time\")\n",
    "\n",
    "print(\"Created e-commerce event stream\")\n",
    "\n",
    "# Real-time feature engineering with windowing\n",
    "feature_engineering = ecommerce_stream \\\n",
    "    .withWatermark(\"event_time\", \"1 minute\") \\\n",
    "    .groupBy(\n",
    "        col(\"user_id\"),\n",
    "        window(col(\"event_time\"), \"2 minutes\", \"30 seconds\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        # Behavioral features\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        countDistinct(\"product_id\").alias(\"unique_products_viewed\"),\n",
    "        sum(when(col(\"action\") == \"purchase\", 1).otherwise(0)).alias(\"purchase_count\"),\n",
    "        sum(when(col(\"action\") == \"view\", 1).otherwise(0)).alias(\"view_count\"),\n",
    "        sum(when(col(\"action\") == \"add_to_cart\", 1).otherwise(0)).alias(\"cart_additions\"),\n",
    "        \n",
    "        # Financial features\n",
    "        avg(\"price\").alias(\"avg_price_viewed\"),\n",
    "        max(\"price\").alias(\"max_price_viewed\"), \n",
    "        sum(when(col(\"action\") == \"purchase\", col(\"price\")).otherwise(0)).alias(\"total_spent\"),\n",
    "        \n",
    "        # Derived features\n",
    "        (sum(when(col(\"action\") == \"purchase\", 1).otherwise(0)) / count(\"*\")).alias(\"conversion_rate\")\n",
    "    ) \\\n",
    "    .withColumn(\"window_start\", col(\"window.start\")) \\\n",
    "    .withColumn(\"window_end\", col(\"window.end\")) \\\n",
    "    .withColumn(\"engagement_score\", \n",
    "        col(\"unique_products_viewed\") * 2 + \n",
    "        col(\"purchase_count\") * 10 + \n",
    "        col(\"cart_additions\") * 3) \\\n",
    "    .withColumn(\"high_value_user\", \n",
    "        when(col(\"total_spent\") > 500, 1).otherwise(0)) \\\n",
    "    .drop(\"window\")\n",
    "\n",
    "print(\"Defined real-time feature engineering pipeline\")\n",
    "\n",
    "# Start feature engineering stream\n",
    "feature_query = feature_engineering \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(\"Real-time feature engineering started!\")\n",
    "print(\"Computing user behavior features in sliding windows...\")\n",
    "\n",
    "time.sleep(30)\n",
    "feature_query.stop()\n",
    "\n",
    "print(\"=== 6. Real-time Anomaly Detection ===\")\n",
    "\n",
    "# Create a stream for anomaly detection using the computed features\n",
    "anomaly_detection_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 3) \\\n",
    "    .load() \\\n",
    "    .withColumn(\"cpu_usage\", rand() * 100) \\\n",
    "    .withColumn(\"memory_usage\", rand() * 100) \\\n",
    "    .withColumn(\"network_io\", rand() * 1000) \\\n",
    "    .withColumn(\"disk_io\", rand() * 500) \\\n",
    "    .withColumn(\"server_id\", concat(lit(\"server_\"), (col(\"value\") % 10).cast(\"string\"))) \\\n",
    "    .withColumn(\"metric_time\", col(\"timestamp\")) \\\n",
    "    .select(\"server_id\", \"cpu_usage\", \"memory_usage\", \"network_io\", \"disk_io\", \"metric_time\")\n",
    "\n",
    "print(\"Created system metrics stream for anomaly detection\")\n",
    "\n",
    "# Real-time anomaly detection using statistical thresholds\n",
    "anomaly_detection = anomaly_detection_stream \\\n",
    "    .withWatermark(\"metric_time\", \"2 minutes\") \\\n",
    "    .groupBy(\n",
    "        col(\"server_id\"),\n",
    "        window(col(\"metric_time\"), \"1 minute\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        avg(\"cpu_usage\").alias(\"avg_cpu\"),\n",
    "        max(\"cpu_usage\").alias(\"max_cpu\"),\n",
    "        stddev(\"cpu_usage\").alias(\"stddev_cpu\"),\n",
    "        avg(\"memory_usage\").alias(\"avg_memory\"),\n",
    "        max(\"memory_usage\").alias(\"max_memory\"),\n",
    "        avg(\"network_io\").alias(\"avg_network\"),\n",
    "        max(\"network_io\").alias(\"max_network\"),\n",
    "        count(\"*\").alias(\"metric_count\")\n",
    "    ) \\\n",
    "    .withColumn(\"cpu_anomaly\", \n",
    "        when((col(\"max_cpu\") > 90) | (col(\"stddev_cpu\") > 25), 1).otherwise(0)) \\\n",
    "    .withColumn(\"memory_anomaly\",\n",
    "        when(col(\"max_memory\") > 85, 1).otherwise(0)) \\\n",
    "    .withColumn(\"network_anomaly\",\n",
    "        when(col(\"max_network\") > 800, 1).otherwise(0)) \\\n",
    "    .withColumn(\"anomaly_score\",\n",
    "        col(\"cpu_anomaly\") + col(\"memory_anomaly\") + col(\"network_anomaly\")) \\\n",
    "    .withColumn(\"alert_level\",\n",
    "        when(col(\"anomaly_score\") >= 3, \"CRITICAL\")\n",
    "        .when(col(\"anomaly_score\") >= 2, \"HIGH\")\n",
    "        .when(col(\"anomaly_score\") >= 1, \"MEDIUM\")\n",
    "        .otherwise(\"LOW\")) \\\n",
    "    .filter(col(\"anomaly_score\") > 0)  # Only show anomalies\n",
    "\n",
    "# Start anomaly detection\n",
    "anomaly_query = anomaly_detection \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .trigger(processingTime='15 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(\"Real-time anomaly detection started!\")\n",
    "print(\"Monitoring system metrics for anomalous behavior...\")\n",
    "\n",
    "time.sleep(35)\n",
    "anomaly_query.stop()\n",
    "\n",
    "print(\"\\nReal-time feature engineering and anomaly detection complete!\")\n",
    "print(\"Demonstrated streaming feature computation and ML-based monitoring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ea17b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced ML Streaming Patterns\n",
    "print(\"Setting up advanced ML streaming patterns...\")\n",
    "\n",
    "print(\"=== 7. Multi-Model Ensemble Predictions ===\")\n",
    "\n",
    "# Create streaming data for recommendation system\n",
    "recommendation_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 4) \\\n",
    "    .load() \\\n",
    "    .withColumn(\"user_id\", (col(\"value\") % 50).cast(\"string\")) \\\n",
    "    .withColumn(\"item_id\", (col(\"value\") % 100).cast(\"string\")) \\\n",
    "    .withColumn(\"rating\", rand() * 5) \\\n",
    "    .withColumn(\"user_age\", (rand() * 50 + 18).cast(\"int\")) \\\n",
    "    .withColumn(\"item_category\", \n",
    "        when(col(\"value\") % 3 == 0, \"electronics\")\n",
    "        .when(col(\"value\") % 3 == 1, \"books\")\n",
    "        .otherwise(\"clothing\")) \\\n",
    "    .withColumn(\"interaction_time\", col(\"timestamp\")) \\\n",
    "    .select(\"user_id\", \"item_id\", \"rating\", \"user_age\", \"item_category\", \"interaction_time\")\n",
    "\n",
    "print(\"Created recommendation streaming data\")\n",
    "\n",
    "# Simulate ensemble predictions with multiple models\n",
    "def ensemble_predictions(batch_df, batch_id):\n",
    "    \"\"\"\n",
    "    Apply multiple models and combine their predictions\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Ensemble Prediction - Batch {batch_id} ---\")\n",
    "    \n",
    "    if batch_df.count() == 0:\n",
    "        return\n",
    "    \n",
    "    # Model 1: Content-based (simplified)\n",
    "    content_based = batch_df.withColumn(\"content_score\",\n",
    "        when(col(\"item_category\") == \"electronics\", col(\"user_age\") / 100)\n",
    "        .when(col(\"item_category\") == \"books\", (80 - col(\"user_age\")) / 100)\n",
    "        .otherwise(col(\"user_age\") / 150))\n",
    "    \n",
    "    # Model 2: Collaborative filtering (simplified)\n",
    "    collaborative = content_based.withColumn(\"collab_score\",\n",
    "        rand() * 0.8 + 0.1)  # Simulated CF score\n",
    "    \n",
    "    # Model 3: Popularity-based (simplified)\n",
    "    popularity = collaborative.withColumn(\"popularity_score\",\n",
    "        when(col(\"item_id\").cast(\"int\") < 20, 0.9)  # Popular items\n",
    "        .otherwise(rand() * 0.5))\n",
    "    \n",
    "    # Ensemble combination (weighted average)\n",
    "    ensemble = popularity.withColumn(\"ensemble_score\",\n",
    "        col(\"content_score\") * 0.4 + \n",
    "        col(\"collab_score\") * 0.4 + \n",
    "        col(\"popularity_score\") * 0.2) \\\n",
    "    .withColumn(\"recommendation\",\n",
    "        when(col(\"ensemble_score\") > 0.6, \"RECOMMEND\").otherwise(\"NO_RECOMMEND\"))\n",
    "    \n",
    "    # Show results\n",
    "    print(f\"Batch size: {batch_df.count()} interactions\")\n",
    "    ensemble.groupBy(\"recommendation\").count().show()\n",
    "    \n",
    "    # Show sample predictions\n",
    "    ensemble.select(\"user_id\", \"item_id\", \"ensemble_score\", \"recommendation\").show(5)\n",
    "\n",
    "# Start ensemble prediction stream\n",
    "ensemble_query = recommendation_stream \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(ensemble_predictions) \\\n",
    "    .trigger(processingTime='12 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(\"Multi-model ensemble predictions started!\")\n",
    "print(\"Combining content-based, collaborative, and popularity models...\")\n",
    "\n",
    "time.sleep(30)\n",
    "ensemble_query.stop()\n",
    "\n",
    "print(\"=== 8. Time Series Forecasting with Streaming ===\")\n",
    "\n",
    "# Create time series data stream\n",
    "timeseries_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 2) \\\n",
    "    .load() \\\n",
    "    .withColumn(\"metric_id\", concat(lit(\"metric_\"), (col(\"value\") % 5).cast(\"string\"))) \\\n",
    "    .withColumn(\"hour\", hour(col(\"timestamp\"))) \\\n",
    "    .withColumn(\"minute\", minute(col(\"timestamp\"))) \\\n",
    "    .withColumn(\"base_value\", 100 + sin(col(\"hour\") * 0.26) * 20)  # Daily pattern\n",
    "    .withColumn(\"noise\", (rand() - 0.5) * 10) \\\n",
    "    .withColumn(\"observed_value\", col(\"base_value\") + col(\"noise\")) \\\n",
    "    .withColumn(\"forecast_time\", col(\"timestamp\")) \\\n",
    "    .select(\"metric_id\", \"observed_value\", \"hour\", \"minute\", \"forecast_time\")\n",
    "\n",
    "print(\"Created time series data stream with seasonal patterns\")\n",
    "\n",
    "# Simple forecasting using moving averages\n",
    "forecasting = timeseries_stream \\\n",
    "    .withWatermark(\"forecast_time\", \"5 minutes\") \\\n",
    "    .groupBy(\n",
    "        col(\"metric_id\"),\n",
    "        window(col(\"forecast_time\"), \"3 minutes\", \"1 minute\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        avg(\"observed_value\").alias(\"moving_avg\"),\n",
    "        stddev(\"observed_value\").alias(\"moving_stddev\"),\n",
    "        count(\"*\").alias(\"sample_count\"),\n",
    "        min(\"observed_value\").alias(\"min_value\"),\n",
    "        max(\"observed_value\").alias(\"max_value\")\n",
    "    ) \\\n",
    "    .withColumn(\"forecast_next\", col(\"moving_avg\")) \\\n",
    "    .withColumn(\"confidence_interval_lower\", col(\"moving_avg\") - 2 * col(\"moving_stddev\")) \\\n",
    "    .withColumn(\"confidence_interval_upper\", col(\"moving_avg\") + 2 * col(\"moving_stddev\")) \\\n",
    "    .withColumn(\"trend\",\n",
    "        when(col(\"moving_avg\") > 110, \"INCREASING\")\n",
    "        .when(col(\"moving_avg\") < 90, \"DECREASING\")\n",
    "        .otherwise(\"STABLE\")) \\\n",
    "    .select(\"metric_id\", \"forecast_next\", \"confidence_interval_lower\", \"confidence_interval_upper\", \"trend\", \"window\")\n",
    "\n",
    "# Start forecasting stream\n",
    "forecast_query = forecasting \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .trigger(processingTime='15 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(\"Time series forecasting started!\")\n",
    "print(\"Generating predictions with confidence intervals...\")\n",
    "\n",
    "time.sleep(35)\n",
    "forecast_query.stop()\n",
    "\n",
    "print(\"\\nAdvanced ML streaming patterns complete!\")\n",
    "print(\"Demonstrated ensemble models and time series forecasting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6a9993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 8 Summary and Production Best Practices\n",
    "print(\"=== Module 8: ML + Streaming Integration - Complete! ===\")\n",
    "\n",
    "# Stop any remaining active queries\n",
    "print(\"\\nCleaning up remaining streaming queries...\")\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"Stopping: {stream.name if stream.name else 'Unnamed Query'}\")\n",
    "    stream.stop()\n",
    "\n",
    "print(\"All streaming queries stopped\")\n",
    "\n",
    "# Summary of ML streaming patterns covered\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODULE 8 ML + STREAMING ACCOMPLISHMENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nâœ… REAL-TIME ML INFERENCE\")\n",
    "print(\"   â€¢ Pre-trained model deployment for streaming predictions\")\n",
    "print(\"   â€¢ Fraud detection with real-time transaction scoring\")\n",
    "print(\"   â€¢ Model loading and application to streaming data\")\n",
    "\n",
    "print(\"\\nâœ… ONLINE LEARNING & MODEL UPDATES\")\n",
    "print(\"   â€¢ Incremental model training with streaming batches\")\n",
    "print(\"   â€¢ Concept drift detection and monitoring\")\n",
    "print(\"   â€¢ Adaptive learning pipelines for evolving data\")\n",
    "\n",
    "print(\"\\nâœ… REAL-TIME FEATURE ENGINEERING\")\n",
    "print(\"   â€¢ Streaming feature computation with windowing\")\n",
    "print(\"   â€¢ Behavioral analytics and user engagement scoring\")\n",
    "print(\"   â€¢ Feature stores integration patterns\")\n",
    "\n",
    "print(\"\\nâœ… ADVANCED ML STREAMING PATTERNS\")\n",
    "print(\"   â€¢ Multi-model ensemble predictions\")\n",
    "print(\"   â€¢ Real-time anomaly detection with ML\")\n",
    "print(\"   â€¢ Time series forecasting with streaming data\")\n",
    "print(\"   â€¢ Recommendation systems with live updates\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PRODUCTION ML STREAMING BEST PRACTICES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nðŸ—ï¸ Model Deployment Strategies:\")\n",
    "print(\"   â€¢ Model versioning and A/B testing frameworks\")\n",
    "print(\"   â€¢ Hot model swapping without downtime\")\n",
    "print(\"   â€¢ Performance monitoring and latency optimization\")\n",
    "print(\"   â€¢ Graceful fallback for model failures\")\n",
    "\n",
    "print(\"\\nðŸ“Š Feature Engineering:\")\n",
    "print(\"   â€¢ Real-time feature computation and caching\")\n",
    "print(\"   â€¢ Feature drift monitoring and validation\")\n",
    "print(\"   â€¢ Cross-batch feature consistency\")\n",
    "print(\"   â€¢ Feature store integration for serving\")\n",
    "\n",
    "print(\"\\nðŸ” Model Monitoring:\")\n",
    "print(\"   â€¢ Prediction accuracy tracking over time\")\n",
    "print(\"   â€¢ Input data quality monitoring\")\n",
    "print(\"   â€¢ Model performance degradation alerts\")\n",
    "print(\"   â€¢ Business metric correlation analysis\")\n",
    "\n",
    "print(\"\\nâš™ï¸ Operational Excellence:\")\n",
    "print(\"   â€¢ Checkpoint management for model state\")\n",
    "print(\"   â€¢ Error handling and recovery strategies\")\n",
    "print(\"   â€¢ Resource scaling for ML workloads\")\n",
    "print(\"   â€¢ Integration with MLOps pipelines\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPREHENSIVE PYSPARK TUTORIAL STATUS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nðŸŽ“ Complete Module Mastery:\")\n",
    "print(\"   âœ… Module 1: PySpark Fundamentals & DataFrames\")\n",
    "print(\"   âœ… Module 2: Advanced Data Operations & SQL\")\n",
    "print(\"   âœ… Module 3: Data Cleaning & Transformation\")  \n",
    "print(\"   âœ… Module 4: Advanced Analytics & Window Functions\")\n",
    "print(\"   âœ… Module 5: Performance Optimization & Tuning\")\n",
    "print(\"   âœ… Module 6: Machine Learning with MLlib\")\n",
    "print(\"   âœ… Module 7: Structured Streaming Fundamentals\")\n",
    "print(\"   âœ… Module 7B: Advanced Streaming Patterns\")\n",
    "print(\"   âœ… Module 8: ML + Streaming Integration\")\n",
    "\n",
    "print(\"\\nðŸš€ Production-Ready Skills Achieved:\")\n",
    "print(\"   â€¢ End-to-end data processing pipelines\")\n",
    "print(\"   â€¢ Real-time streaming analytics systems\")\n",
    "print(\"   â€¢ Machine learning model deployment\")\n",
    "print(\"   â€¢ Performance optimization techniques\")\n",
    "print(\"   â€¢ Production monitoring and maintenance\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Ready for Enterprise Applications:\")\n",
    "print(\"   â€¢ Real-time fraud detection systems\")\n",
    "print(\"   â€¢ Recommendation engines with live updates\")\n",
    "print(\"   â€¢ IoT analytics and anomaly detection\")\n",
    "print(\"   â€¢ Financial trading and risk systems\")\n",
    "print(\"   â€¢ Supply chain optimization platforms\")\n",
    "\n",
    "# Clean up temporary directories\n",
    "import shutil\n",
    "try:\n",
    "    shutil.rmtree(\"/tmp/ml_streaming_models\", ignore_errors=True)\n",
    "    shutil.rmtree(\"/tmp/ml_streaming_output\", ignore_errors=True)\n",
    "    shutil.rmtree(\"/tmp/ml-streaming-checkpoints\", ignore_errors=True)\n",
    "    print(\"\\nðŸ§¹ ML streaming artifacts cleaned up\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ¯ COMPREHENSIVE PYSPARK TUTORIAL COMPLETE!\")\n",
    "print(\"ENTERPRISE-READY BIG DATA & ML SKILLS ACHIEVED!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nðŸŒŸ Congratulations! You have mastered:\")\n",
    "print(\"   ðŸ“Š Big Data Processing with PySpark\")\n",
    "print(\"   ðŸ”„ Real-time Streaming Analytics\") \n",
    "print(\"   ðŸ¤– Machine Learning Pipeline Development\")\n",
    "print(\"   âš¡ Performance Optimization Techniques\")\n",
    "print(\"   ðŸš€ Production Deployment Strategies\")\n",
    "\n",
    "print(\"\\nReady to tackle any big data challenge in production! ðŸŽ‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a54e5c",
   "metadata": {},
   "source": [
    "# Module 8: Machine Learning with Streaming Integration\n",
    "*Real-time ML Inference, Online Learning, and Production ML Pipelines*\n",
    "\n",
    "## Learning Objectives\n",
    "Master the integration of machine learning with streaming data:\n",
    "\n",
    "**Real-time ML Inference**\n",
    "- Loading pre-trained models for streaming predictions\n",
    "- Batch vs streaming inference patterns\n",
    "- Model deployment strategies for real-time systems\n",
    "- Performance optimization for low-latency inference\n",
    "\n",
    "**Online Learning & Model Updates**\n",
    "- Incremental learning with streaming data\n",
    "- Model retraining strategies and triggers\n",
    "- Feature engineering for streaming ML\n",
    "- Handling concept drift and model decay\n",
    "\n",
    "**Production ML Pipelines**\n",
    "- End-to-end ML pipelines with streaming data\n",
    "- Feature stores and real-time feature serving\n",
    "- Model monitoring and performance tracking\n",
    "- A/B testing for ML models in production\n",
    "\n",
    "**Advanced ML Streaming Patterns**\n",
    "- Multi-model ensemble predictions\n",
    "- Real-time anomaly detection with ML\n",
    "- Recommendation systems with streaming updates\n",
    "- Time series forecasting with streaming data\n",
    "\n",
    "---\n",
    "\n",
    "## Module Structure\n",
    "1. **ML Environment Setup** - MLlib streaming configuration\n",
    "2. **Real-time Model Inference** - Streaming predictions with pre-trained models\n",
    "3. **Online Learning Pipeline** - Incremental model updates\n",
    "4. **Feature Engineering Streams** - Real-time feature computation\n",
    "5. **ML Model Monitoring** - Performance tracking and drift detection\n",
    "6. **Production ML Pipeline** - End-to-end streaming ML system\n",
    "7. **Advanced ML Patterns** - Ensemble models and anomaly detection\n",
    "8. **Summary & Best Practices** - Production deployment guidelines"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
