{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e777141",
   "metadata": {},
   "source": [
    "# Module 6: PySpark Machine Learning with MLlib\n",
    "*Comprehensive Guide to Scalable Machine Learning in Production*\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this module, you will master:\n",
    "\n",
    "**Data Preparation & Feature Engineering**\n",
    "- Feature extraction and transformation pipelines\n",
    "- Handling categorical and numerical features\n",
    "- Feature scaling and normalization\n",
    "- Text processing and NLP features\n",
    "\n",
    "**Supervised Learning**\n",
    "- Classification algorithms (Logistic Regression, Random Forest, GBT)\n",
    "- Regression algorithms (Linear Regression, Random Forest Regression)\n",
    "- Model evaluation and cross-validation\n",
    "- Hyperparameter tuning with grid search\n",
    "\n",
    "**Unsupervised Learning**\n",
    "- Clustering algorithms (K-Means, Gaussian Mixture)\n",
    "- Dimensionality reduction (PCA)\n",
    "- Association rules and frequent pattern mining\n",
    "\n",
    "**Advanced Topics**\n",
    "- Pipeline construction and model persistence\n",
    "- Distributed model training strategies\n",
    "- Model deployment and serving\n",
    "- Performance optimization for ML workloads\n",
    "\n",
    "---\n",
    "\n",
    "## Module Structure\n",
    "1. **MLlib Setup & Data Preparation** - Environment and feature engineering\n",
    "2. **Supervised Learning** - Classification and regression algorithms\n",
    "3. **Unsupervised Learning** - Clustering and dimensionality reduction\n",
    "4. **Model Pipelines** - End-to-end ML pipeline construction\n",
    "5. **Model Evaluation & Tuning** - Cross-validation and hyperparameter optimization\n",
    "6. **Production ML** - Model deployment and serving strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42bf9f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up PySpark MLlib Environment...\n",
      "Spark MLlib Session Created\n",
      "Spark Version: 4.0.0\n",
      "Default Parallelism: 8\n",
      "\n",
      "MLlib Environment Configuration:\n",
      "   spark.sql.adaptive.enabled: true\n",
      "   spark.serializer: org.apache.spark.serializer.KryoSerializer\n",
      "   spark.sql.execution.arrow.pyspark.enabled: true\n",
      "   spark.default.parallelism: 8\n",
      "\n",
      "MLlib modules successfully imported and ready for machine learning!\n",
      "Available algorithms: Classification, Regression, Clustering, Feature Engineering\n"
     ]
    }
   ],
   "source": [
    "# Module 6: PySpark MLlib Setup and Environment\n",
    "print(\"Setting up PySpark MLlib Environment...\")\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# MLlib imports\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml.clustering import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.tuning import *\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "# Configure Spark for ML workloads\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark-MLlib-Comprehensive\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.default.parallelism\", \"8\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce noise\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"Spark MLlib Session Created\")\n",
    "print(\"Spark Version: {}\".format(spark.version))\n",
    "print(\"Default Parallelism: {}\".format(spark.sparkContext.defaultParallelism))\n",
    "\n",
    "# Display MLlib-specific configurations\n",
    "print(\"\\nMLlib Environment Configuration:\")\n",
    "ml_configs = [\n",
    "    \"spark.sql.adaptive.enabled\",\n",
    "    \"spark.serializer\", \n",
    "    \"spark.sql.execution.arrow.pyspark.enabled\",\n",
    "    \"spark.default.parallelism\"\n",
    "]\n",
    "\n",
    "for config in ml_configs:\n",
    "    value = spark.conf.get(config, \"Not Set\")\n",
    "    print(\"   {}: {}\".format(config, value))\n",
    "\n",
    "print(\"\\nMLlib modules successfully imported and ready for machine learning!\")\n",
    "print(\"Available algorithms: Classification, Regression, Clustering, Feature Engineering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cab847c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Machine Learning Datasets...\n",
      "Generating Customer Churn Dataset...\n",
      "Customer Dataset: 10,000 records\n",
      "Churn Distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/25 22:51:25 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|churn|count|\n",
      "+-----+-----+\n",
      "|    1| 1697|\n",
      "|    0| 8303|\n",
      "+-----+-----+\n",
      "\n",
      "\n",
      "Generating Sales Prediction Dataset...\n",
      "Sales Dataset: 15,000 records\n",
      "\n",
      "Generating Product Clustering Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/25 22:51:25 WARN CacheManager: Asked to cache already cached data.\n",
      "25/08/25 22:51:25 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Dataset: 5,000 records\n",
      "\n",
      "Sample Customer Data:\n",
      "+-----------+---+-------------+---------------+-------------+------------+--------------+--------------+----------------+------------+-------------+-----+\n",
      "|customer_id|age|tenure_months|monthly_charges|total_charges|num_services|contract_type |payment_method|internet_service|tech_support|online_backup|churn|\n",
      "+-----------+---+-------------+---------------+-------------+------------+--------------+--------------+----------------+------------+-------------+-----+\n",
      "|1          |48 |49           |83.88          |4110.12      |7           |Month-to-month|Mailed check  |DSL             |No          |Yes          |0    |\n",
      "|2          |43 |40           |89.44          |3577.60      |2           |Month-to-month|Mailed check  |Fiber optic     |Yes         |No           |0    |\n",
      "|3          |59 |16           |60.55          |968.80       |6           |Month-to-month|Bank transfer |Fiber optic     |Yes         |No           |0    |\n",
      "|4          |31 |13           |83.58          |1086.54      |4           |Two year      |Bank transfer |DSL             |No          |No           |1    |\n",
      "|5          |51 |39           |79.71          |3108.69      |3           |Month-to-month|Mailed check  |Fiber optic     |Yes         |Yes          |0    |\n",
      "+-----------+---+-------------+---------------+-------------+------------+--------------+--------------+----------------+------------+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Sample Sales Data:\n",
      "+-------+--------+-----------+-----+-----------+--------+----------+----------------+-----------------+----------+------------+\n",
      "|sale_id|store_id|day_of_week|month|temperature|humidity|is_holiday|promotion_active|competitor_nearby|store_size|sales_amount|\n",
      "+-------+--------+-----------+-----+-----------+--------+----------+----------------+-----------------+----------+------------+\n",
      "|1      |49      |4          |3    |30.16      |76.39   |0         |1               |0                |Medium    |2406.30     |\n",
      "|2      |31      |1          |12   |36.81      |65.96   |0         |0               |0                |Large     |2345.74     |\n",
      "|3      |13      |5          |10   |64.17      |74.62   |0         |0               |1                |Medium    |2475.48     |\n",
      "|4      |33      |5          |10   |36.62      |58.71   |0         |1               |1                |Medium    |2341.20     |\n",
      "|5      |7       |1          |2    |55.05      |74.78   |0         |0               |0                |Medium    |1866.00     |\n",
      "+-------+--------+-----------+-----+-----------+--------+----------+----------------+-----------------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Sample Product Data:\n",
      "+----------+------+------+-----------+---------+---------+--------+---------+-----------+\n",
      "|product_id|price |rating|num_reviews|weight_kg|length_cm|width_cm|height_cm|category   |\n",
      "+----------+------+------+-----------+---------+---------+--------+---------+-----------+\n",
      "|1         |367.09|3.42  |555        |19.47    |80.79    |48.71   |12.73    |Home       |\n",
      "|2         |103.99|4.47  |576        |16.07    |52.38    |33.62   |12.56    |Sports     |\n",
      "|3         |430.58|4.64  |694        |17.83    |31.94    |23.44   |29.59    |Electronics|\n",
      "|4         |45.99 |4.32  |161        |14.99    |84.58    |42.75   |11.26    |Clothing   |\n",
      "|5         |294.47|1.56  |289        |15.33    |5.16     |40.92   |19.01    |Clothing   |\n",
      "+----------+------+------+-----------+---------+---------+--------+---------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Generate Comprehensive ML Dataset for Demonstrations\n",
    "print(\"Creating Machine Learning Datasets...\")\n",
    "\n",
    "from datetime import date\n",
    "import random\n",
    "\n",
    "# Set seed for reproducibility\n",
    "spark.sparkContext.setCheckpointDir(\"/tmp/spark-checkpoint\")\n",
    "random.seed(42)\n",
    "\n",
    "# 1. Customer Dataset for Classification (Churn Prediction)\n",
    "print(\"Generating Customer Churn Dataset...\")\n",
    "customer_df = spark.range(1, 10001) \\\n",
    "    .withColumnRenamed(\"id\", \"customer_id\") \\\n",
    "    .withColumn(\"age\", (rand(42) * 50 + 18).cast(\"int\")) \\\n",
    "    .withColumn(\"tenure_months\", (rand(43) * 60 + 1).cast(\"int\")) \\\n",
    "    .withColumn(\"monthly_charges\", (rand(44) * 80 + 20).cast(\"decimal(8,2)\")) \\\n",
    "    .withColumn(\"total_charges\", col(\"monthly_charges\") * col(\"tenure_months\")) \\\n",
    "    .withColumn(\"num_services\", (rand(45) * 8 + 1).cast(\"int\")) \\\n",
    "    .withColumn(\"contract_type\", \n",
    "        when(rand(46) < 0.3, \"Month-to-month\")\n",
    "        .when(rand(47) < 0.6, \"One year\")\n",
    "        .otherwise(\"Two year\")) \\\n",
    "    .withColumn(\"payment_method\",\n",
    "        when(rand(48) < 0.25, \"Electronic check\")\n",
    "        .when(rand(49) < 0.5, \"Mailed check\") \n",
    "        .when(rand(50) < 0.75, \"Bank transfer\")\n",
    "        .otherwise(\"Credit card\")) \\\n",
    "    .withColumn(\"internet_service\",\n",
    "        when(rand(51) < 0.4, \"DSL\")\n",
    "        .when(rand(52) < 0.7, \"Fiber optic\")\n",
    "        .otherwise(\"No\")) \\\n",
    "    .withColumn(\"tech_support\", when(rand(53) < 0.6, \"Yes\").otherwise(\"No\")) \\\n",
    "    .withColumn(\"online_backup\", when(rand(54) < 0.5, \"Yes\").otherwise(\"No\"))\n",
    "\n",
    "# Create churn label based on business logic\n",
    "customer_df = customer_df.withColumn(\"churn\",\n",
    "    when(\n",
    "        (col(\"contract_type\") == \"Month-to-month\") & \n",
    "        (col(\"monthly_charges\") > 70) & \n",
    "        (col(\"tenure_months\") < 12) &\n",
    "        (col(\"tech_support\") == \"No\"), 1\n",
    "    ).when(\n",
    "        (col(\"age\") < 25) & \n",
    "        (col(\"monthly_charges\") > 60) &\n",
    "        (col(\"payment_method\") == \"Electronic check\"), 1\n",
    "    ).when(rand(55) < 0.15, 1)  # Random churn for complexity\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "customer_df.cache()\n",
    "print(\"Customer Dataset: {:,} records\".format(customer_df.count()))\n",
    "print(\"Churn Distribution:\")\n",
    "customer_df.groupBy(\"churn\").count().show()\n",
    "\n",
    "# 2. Sales Dataset for Regression (Sales Prediction)\n",
    "print(\"\\nGenerating Sales Prediction Dataset...\")\n",
    "sales_df = spark.range(1, 15001) \\\n",
    "    .withColumnRenamed(\"id\", \"sale_id\") \\\n",
    "    .withColumn(\"store_id\", (rand(56) * 50 + 1).cast(\"int\")) \\\n",
    "    .withColumn(\"day_of_week\", (rand(57) * 7 + 1).cast(\"int\")) \\\n",
    "    .withColumn(\"month\", (rand(58) * 12 + 1).cast(\"int\")) \\\n",
    "    .withColumn(\"temperature\", (rand(59) * 40 + 30).cast(\"decimal(5,2)\")) \\\n",
    "    .withColumn(\"humidity\", (rand(60) * 50 + 30).cast(\"decimal(5,2)\")) \\\n",
    "    .withColumn(\"is_holiday\", when(rand(61) < 0.1, 1).otherwise(0)) \\\n",
    "    .withColumn(\"promotion_active\", when(rand(62) < 0.3, 1).otherwise(0)) \\\n",
    "    .withColumn(\"competitor_nearby\", when(rand(63) < 0.4, 1).otherwise(0)) \\\n",
    "    .withColumn(\"store_size\", \n",
    "        when(rand(64) < 0.3, \"Small\")\n",
    "        .when(rand(65) < 0.7, \"Medium\")\n",
    "        .otherwise(\"Large\"))\n",
    "\n",
    "# Create sales amount based on realistic factors\n",
    "sales_df = sales_df.withColumn(\"sales_amount\",\n",
    "    (1000 + \n",
    "     col(\"temperature\") * 10 +\n",
    "     when(col(\"is_holiday\") == 1, 500).otherwise(0) +\n",
    "     when(col(\"promotion_active\") == 1, 300).otherwise(0) +\n",
    "     when(col(\"store_size\") == \"Large\", 800)\n",
    "     .when(col(\"store_size\") == \"Medium\", 400).otherwise(0) +\n",
    "     when(col(\"day_of_week\").isin([6, 7]), 200).otherwise(0) +\n",
    "     (rand(66) * 1000 - 500)  # Random variation\n",
    "    ).cast(\"decimal(10,2)\")\n",
    ")\n",
    "\n",
    "sales_df.cache()\n",
    "print(\"Sales Dataset: {:,} records\".format(sales_df.count()))\n",
    "\n",
    "# 3. Product Dataset for Clustering\n",
    "print(\"\\nGenerating Product Clustering Dataset...\")\n",
    "product_df = spark.range(1, 5001) \\\n",
    "    .withColumnRenamed(\"id\", \"product_id\") \\\n",
    "    .withColumn(\"price\", (rand(67) * 500 + 10).cast(\"decimal(8,2)\")) \\\n",
    "    .withColumn(\"rating\", (rand(68) * 4 + 1).cast(\"decimal(3,2)\")) \\\n",
    "    .withColumn(\"num_reviews\", (rand(69) * 1000 + 5).cast(\"int\")) \\\n",
    "    .withColumn(\"weight_kg\", (rand(70) * 20 + 0.1).cast(\"decimal(5,2)\")) \\\n",
    "    .withColumn(\"length_cm\", (rand(71) * 100 + 5).cast(\"decimal(5,2)\")) \\\n",
    "    .withColumn(\"width_cm\", (rand(72) * 50 + 3).cast(\"decimal(5,2)\")) \\\n",
    "    .withColumn(\"height_cm\", (rand(73) * 30 + 2).cast(\"decimal(5,2)\")) \\\n",
    "    .withColumn(\"category\",\n",
    "        when(rand(74) < 0.2, \"Electronics\")\n",
    "        .when(rand(75) < 0.4, \"Clothing\")\n",
    "        .when(rand(76) < 0.6, \"Home\")\n",
    "        .when(rand(77) < 0.8, \"Sports\")\n",
    "        .otherwise(\"Books\"))\n",
    "\n",
    "product_df.cache()\n",
    "print(\"Product Dataset: {:,} records\".format(product_df.count()))\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nSample Customer Data:\")\n",
    "customer_df.show(5, truncate=False)\n",
    "\n",
    "print(\"\\nSample Sales Data:\")\n",
    "sales_df.show(5, truncate=False)\n",
    "\n",
    "print(\"\\nSample Product Data:\")\n",
    "product_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889482d4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: Feature Engineering and Data Preparation\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "**Feature Engineering** is the process of transforming raw data into features suitable for machine learning:\n",
    "- **Categorical Encoding**: Converting text categories to numerical representations\n",
    "- **Feature Scaling**: Normalizing numerical features for algorithm performance\n",
    "- **Feature Selection**: Identifying the most relevant features\n",
    "- **Text Processing**: Converting text data into numerical vectors\n",
    "\n",
    "## Key MLlib Transformers\n",
    "\n",
    "### Categorical Features\n",
    "- **StringIndexer**: Converts string categories to numerical indices\n",
    "- **OneHotEncoder**: Creates binary columns for each category\n",
    "- **VectorIndexer**: Handles categorical features in vector columns\n",
    "\n",
    "### Numerical Features\n",
    "- **StandardScaler**: Standardizes features to have zero mean and unit variance\n",
    "- **MinMaxScaler**: Scales features to a fixed range [0,1]\n",
    "- **Normalizer**: Normalizes each row to have unit norm\n",
    "\n",
    "### Text Features\n",
    "- **Tokenizer**: Splits text into individual words\n",
    "- **StopWordsRemover**: Removes common stop words\n",
    "- **CountVectorizer**: Creates word count vectors\n",
    "- **TF-IDF**: Term Frequency-Inverse Document Frequency vectors\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09226f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demonstrating Feature Engineering Techniques...\n",
      "1. Categorical Feature Encoding\n",
      "Categorical variables indexed:\n",
      "+--------------+--------------+--------------+-------------+\n",
      "| contract_type|contract_index|payment_method|payment_index|\n",
      "+--------------+--------------+--------------+-------------+\n",
      "|Month-to-month|           1.0|  Mailed check|          0.0|\n",
      "|Month-to-month|           1.0|  Mailed check|          0.0|\n",
      "|Month-to-month|           1.0| Bank transfer|          1.0|\n",
      "|      Two year|           2.0| Bank transfer|          1.0|\n",
      "|Month-to-month|           1.0|  Mailed check|          0.0|\n",
      "+--------------+--------------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "One-hot encoded features (showing contract encoding):\n",
      "+--------------+----------------+\n",
      "|contract_type |contract_encoded|\n",
      "+--------------+----------------+\n",
      "|Month-to-month|(2,[1],[1.0])   |\n",
      "|Month-to-month|(2,[1],[1.0])   |\n",
      "|Month-to-month|(2,[1],[1.0])   |\n",
      "|Two year      |(2,[],[])       |\n",
      "|Month-to-month|(2,[1],[1.0])   |\n",
      "+--------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2. Numerical Feature Scaling\n",
      "Categorical variables indexed:\n",
      "+--------------+--------------+--------------+-------------+\n",
      "| contract_type|contract_index|payment_method|payment_index|\n",
      "+--------------+--------------+--------------+-------------+\n",
      "|Month-to-month|           1.0|  Mailed check|          0.0|\n",
      "|Month-to-month|           1.0|  Mailed check|          0.0|\n",
      "|Month-to-month|           1.0| Bank transfer|          1.0|\n",
      "|      Two year|           2.0| Bank transfer|          1.0|\n",
      "|Month-to-month|           1.0|  Mailed check|          0.0|\n",
      "+--------------+--------------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "One-hot encoded features (showing contract encoding):\n",
      "+--------------+----------------+\n",
      "|contract_type |contract_encoded|\n",
      "+--------------+----------------+\n",
      "|Month-to-month|(2,[1],[1.0])   |\n",
      "|Month-to-month|(2,[1],[1.0])   |\n",
      "|Month-to-month|(2,[1],[1.0])   |\n",
      "|Two year      |(2,[],[])       |\n",
      "|Month-to-month|(2,[1],[1.0])   |\n",
      "+--------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2. Numerical Feature Scaling\n",
      "Original vs Scaled Features:\n",
      "+-----------------------------+--------------------------------------------------------------------------------------------------+\n",
      "|numerical_features           |scaled_features                                                                                   |\n",
      "+-----------------------------+--------------------------------------------------------------------------------------------------+\n",
      "|[48.0,49.0,83.88,4110.12,7.0]|[0.39221008788076545,1.071384108693034,1.0385974481123896,1.7248154496904555,1.092161904561792]   |\n",
      "|[43.0,40.0,89.44,3577.6,2.0] |[0.04475868918056139,0.5512105650647605,1.278845694772319,1.3222956076130612,-1.0860628751785217] |\n",
      "|[59.0,16.0,60.55,968.8,6.0]  |[1.1566031650212143,-0.8359188846106357,0.030505434699412207,-0.64963739774168,0.6565169486137292]|\n",
      "+-----------------------------+--------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "MinMax scaled features:\n",
      "Original vs Scaled Features:\n",
      "+-----------------------------+--------------------------------------------------------------------------------------------------+\n",
      "|numerical_features           |scaled_features                                                                                   |\n",
      "+-----------------------------+--------------------------------------------------------------------------------------------------+\n",
      "|[48.0,49.0,83.88,4110.12,7.0]|[0.39221008788076545,1.071384108693034,1.0385974481123896,1.7248154496904555,1.092161904561792]   |\n",
      "|[43.0,40.0,89.44,3577.6,2.0] |[0.04475868918056139,0.5512105650647605,1.278845694772319,1.3222956076130612,-1.0860628751785217] |\n",
      "|[59.0,16.0,60.55,968.8,6.0]  |[1.1566031650212143,-0.8359188846106357,0.030505434699412207,-0.64963739774168,0.6565169486137292]|\n",
      "+-----------------------------+--------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "MinMax scaled features:\n",
      "+-------------------------------------------------------------------------------------------------+\n",
      "|minmax_features                                                                                  |\n",
      "+-------------------------------------------------------------------------------------------------+\n",
      "|[0.6122448979591836,0.8135593220338984,0.7985998249781222,0.6841275552468851,0.8571428571428571] |\n",
      "|[0.5102040816326531,0.6610169491525424,0.8681085135641955,0.5950095808683864,0.14285714285714285]|\n",
      "|[0.836734693877551,0.2542372881355932,0.5069383672959119,0.15842321498799253,0.7142857142857142] |\n",
      "+-------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "3. Final Feature Assembly\n",
      "Final feature vector shape and sample:\n",
      "Feature vector size: 12\n",
      "+------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                                                      |churn|\n",
      "+------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|[0.39221008788076545,1.071384108693034,1.0385974481123896,1.7248154496904555,1.092161904561792,0.0,1.0,1.0,0.0,0.0,0.0,1.0]   |0    |\n",
      "|[0.04475868918056139,0.5512105650647605,1.278845694772319,1.3222956076130612,-1.0860628751785217,0.0,1.0,1.0,0.0,0.0,1.0,0.0] |0    |\n",
      "|[1.1566031650212143,-0.8359188846106357,0.030505434699412207,-0.64963739774168,0.6565169486137292,0.0,1.0,0.0,1.0,0.0,1.0,0.0]|0    |\n",
      "+------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "4. Feature Analysis\n",
      "+-------------------------------------------------------------------------------------------------+\n",
      "|minmax_features                                                                                  |\n",
      "+-------------------------------------------------------------------------------------------------+\n",
      "|[0.6122448979591836,0.8135593220338984,0.7985998249781222,0.6841275552468851,0.8571428571428571] |\n",
      "|[0.5102040816326531,0.6610169491525424,0.8681085135641955,0.5950095808683864,0.14285714285714285]|\n",
      "|[0.836734693877551,0.2542372881355932,0.5069383672959119,0.15842321498799253,0.7142857142857142] |\n",
      "+-------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "3. Final Feature Assembly\n",
      "Final feature vector shape and sample:\n",
      "Feature vector size: 12\n",
      "+------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                                                      |churn|\n",
      "+------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|[0.39221008788076545,1.071384108693034,1.0385974481123896,1.7248154496904555,1.092161904561792,0.0,1.0,1.0,0.0,0.0,0.0,1.0]   |0    |\n",
      "|[0.04475868918056139,0.5512105650647605,1.278845694772319,1.3222956076130612,-1.0860628751785217,0.0,1.0,1.0,0.0,0.0,1.0,0.0] |0    |\n",
      "|[1.1566031650212143,-0.8359188846106357,0.030505434699412207,-0.64963739774168,0.6565169486137292,0.0,1.0,0.0,1.0,0.0,1.0,0.0]|0    |\n",
      "+------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "4. Feature Analysis\n",
      "Feature correlation matrix calculated\n",
      "Feature correlation matrix calculated\n",
      "+-------+------------------+-----------------+-----------------+------------------+-----------------+\n",
      "|summary|               age|    tenure_months|  monthly_charges|     total_charges|     num_services|\n",
      "+-------+------------------+-----------------+-----------------+------------------+-----------------+\n",
      "|  count|             10000|            10000|            10000|             10000|            10000|\n",
      "|   mean|           42.3559|           30.463|        59.844021|       1828.248084|            4.493|\n",
      "| stddev|14.390501862144507|17.30191800456422|23.14272872871435|1322.9658375390343|2.295447213026424|\n",
      "|    min|                18|                1|            20.00|             22.15|                1|\n",
      "|    max|                67|               60|            99.99|           5997.60|                8|\n",
      "+-------+------------------+-----------------+-----------------+------------------+-----------------+\n",
      "\n",
      "Feature engineering pipeline complete!\n",
      "Data ready for machine learning algorithms\n",
      "+-------+------------------+-----------------+-----------------+------------------+-----------------+\n",
      "|summary|               age|    tenure_months|  monthly_charges|     total_charges|     num_services|\n",
      "+-------+------------------+-----------------+-----------------+------------------+-----------------+\n",
      "|  count|             10000|            10000|            10000|             10000|            10000|\n",
      "|   mean|           42.3559|           30.463|        59.844021|       1828.248084|            4.493|\n",
      "| stddev|14.390501862144507|17.30191800456422|23.14272872871435|1322.9658375390343|2.295447213026424|\n",
      "|    min|                18|                1|            20.00|             22.15|                1|\n",
      "|    max|                67|               60|            99.99|           5997.60|                8|\n",
      "+-------+------------------+-----------------+-----------------+------------------+-----------------+\n",
      "\n",
      "Feature engineering pipeline complete!\n",
      "Data ready for machine learning algorithms\n",
      "Final ML dataset: 10,000 records with 12 features\n",
      "Final ML dataset: 10,000 records with 12 features\n"
     ]
    }
   ],
   "source": [
    "# Section 1.1: Feature Engineering Pipeline\n",
    "print(\"Demonstrating Feature Engineering Techniques...\")\n",
    "\n",
    "# 1. Categorical Feature Encoding\n",
    "print(\"1. Categorical Feature Encoding\")\n",
    "\n",
    "# String Indexer for categorical variables\n",
    "contract_indexer = StringIndexer(inputCol=\"contract_type\", outputCol=\"contract_index\")\n",
    "payment_indexer = StringIndexer(inputCol=\"payment_method\", outputCol=\"payment_index\")\n",
    "internet_indexer = StringIndexer(inputCol=\"internet_service\", outputCol=\"internet_index\")\n",
    "\n",
    "# Apply indexers\n",
    "customer_indexed = contract_indexer.fit(customer_df).transform(customer_df)\n",
    "customer_indexed = payment_indexer.fit(customer_indexed).transform(customer_indexed)\n",
    "customer_indexed = internet_indexer.fit(customer_indexed).transform(customer_indexed)\n",
    "\n",
    "print(\"Categorical variables indexed:\")\n",
    "customer_indexed.select(\"contract_type\", \"contract_index\", \"payment_method\", \"payment_index\").show(5)\n",
    "\n",
    "# One-Hot Encoding\n",
    "contract_encoder = OneHotEncoder(inputCol=\"contract_index\", outputCol=\"contract_encoded\")\n",
    "payment_encoder = OneHotEncoder(inputCol=\"payment_index\", outputCol=\"payment_encoded\")\n",
    "internet_encoder = OneHotEncoder(inputCol=\"internet_index\", outputCol=\"internet_encoded\")\n",
    "\n",
    "customer_encoded = contract_encoder.fit(customer_indexed).transform(customer_indexed)\n",
    "customer_encoded = payment_encoder.fit(customer_encoded).transform(customer_encoded)\n",
    "customer_encoded = internet_encoder.fit(customer_encoded).transform(customer_encoded)\n",
    "\n",
    "print(\"\\nOne-hot encoded features (showing contract encoding):\")\n",
    "customer_encoded.select(\"contract_type\", \"contract_encoded\").show(5, truncate=False)\n",
    "\n",
    "# 2. Numerical Feature Scaling\n",
    "print(\"\\n2. Numerical Feature Scaling\")\n",
    "\n",
    "# Assemble numerical features into a vector\n",
    "numerical_features = [\"age\", \"tenure_months\", \"monthly_charges\", \"total_charges\", \"num_services\"]\n",
    "num_assembler = VectorAssembler(inputCols=numerical_features, outputCol=\"numerical_features\")\n",
    "customer_vector = num_assembler.transform(customer_encoded)\n",
    "\n",
    "# Standard Scaling\n",
    "scaler = StandardScaler(inputCol=\"numerical_features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "scaler_model = scaler.fit(customer_vector)\n",
    "customer_scaled = scaler_model.transform(customer_vector)\n",
    "\n",
    "print(\"Original vs Scaled Features:\")\n",
    "customer_scaled.select(\"numerical_features\", \"scaled_features\").show(3, truncate=False)\n",
    "\n",
    "# MinMax Scaling demonstration\n",
    "minmax_scaler = MinMaxScaler(inputCol=\"numerical_features\", outputCol=\"minmax_features\")\n",
    "minmax_model = minmax_scaler.fit(customer_vector)\n",
    "customer_minmax = minmax_model.transform(customer_vector)\n",
    "\n",
    "print(\"MinMax scaled features:\")\n",
    "customer_minmax.select(\"minmax_features\").show(3, truncate=False)\n",
    "\n",
    "# 3. Feature Assembly for ML\n",
    "print(\"\\n3. Final Feature Assembly\")\n",
    "\n",
    "# Combine all features into a single vector\n",
    "feature_cols = [\"scaled_features\", \"contract_encoded\", \"payment_encoded\", \"internet_encoded\"]\n",
    "final_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "ml_ready_data = final_assembler.transform(customer_scaled)\n",
    "\n",
    "print(\"Final feature vector shape and sample:\")\n",
    "print(\"Feature vector size: {}\".format(ml_ready_data.select(\"features\").first()[\"features\"].size))\n",
    "ml_ready_data.select(\"features\", \"churn\").show(3, truncate=False)\n",
    "\n",
    "# 4. Feature Statistics and Correlation\n",
    "print(\"\\n4. Feature Analysis\")\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = Correlation.corr(ml_ready_data, \"features\").head()\n",
    "print(\"Feature correlation matrix calculated\")\n",
    "\n",
    "# Basic feature statistics\n",
    "ml_ready_data.describe(numerical_features).show()\n",
    "\n",
    "print(\"Feature engineering pipeline complete!\")\n",
    "print(\"Data ready for machine learning algorithms\")\n",
    "\n",
    "# Cache the final dataset for reuse\n",
    "ml_ready_data.cache()\n",
    "final_count = ml_ready_data.count()\n",
    "print(\"Final ML dataset: {:,} records with {} features\".format(\n",
    "    final_count, ml_ready_data.select(\"features\").first()[\"features\"].size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8e47a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing datasets for ML algorithms...\n",
      "customers_features: 10000 records\n",
      "Classification target distribution:\n",
      "+-----------+-----+\n",
      "|churn_label|count|\n",
      "+-----------+-----+\n",
      "|          1| 1697|\n",
      "|          0| 8303|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/25 22:51:49 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales_features: 15000 records\n",
      "products_features: 5000 records\n",
      "\n",
      "All ML datasets prepared successfully!\n",
      "Available datasets:\n",
      "- customers_features: Classification (churn prediction)\n",
      "- sales_features: Regression (sales prediction)\n",
      "- products_features: Clustering (product segmentation)\n",
      "products_features: 5000 records\n",
      "\n",
      "All ML datasets prepared successfully!\n",
      "Available datasets:\n",
      "- customers_features: Classification (churn prediction)\n",
      "- sales_features: Regression (sales prediction)\n",
      "- products_features: Clustering (product segmentation)\n"
     ]
    }
   ],
   "source": [
    "# Prepare ML-ready datasets with proper naming\n",
    "print(\"\\nPreparing datasets for ML algorithms...\")\n",
    "\n",
    "# 1. Customer Features for Classification (rename churn to churn_label)\n",
    "customers_features = ml_ready_data.withColumnRenamed(\"churn\", \"churn_label\")\n",
    "customers_features.cache()\n",
    "\n",
    "print(f\"customers_features: {customers_features.count()} records\")\n",
    "print(\"Classification target distribution:\")\n",
    "customers_features.groupBy(\"churn_label\").count().show()\n",
    "\n",
    "# 2. Sales Features for Regression\n",
    "# Create features for sales dataset\n",
    "sales_categorical_cols = [\"store_size\"]\n",
    "sales_numerical_cols = [\"store_id\", \"day_of_week\", \"month\", \"temperature\", \"humidity\", \n",
    "                       \"is_holiday\", \"promotion_active\", \"competitor_nearby\"]\n",
    "\n",
    "# Index categorical variables\n",
    "sales_indexed = sales_df\n",
    "for col_name in sales_categorical_cols:\n",
    "    indexer = StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_indexed\")\n",
    "    sales_indexed = indexer.fit(sales_indexed).transform(sales_indexed)\n",
    "\n",
    "# One-hot encode categorical variables  \n",
    "sales_encoded = sales_indexed\n",
    "for col_name in sales_categorical_cols:\n",
    "    encoder = OneHotEncoder(inputCol=f\"{col_name}_indexed\", outputCol=f\"{col_name}_encoded\")\n",
    "    sales_encoded = encoder.fit(sales_encoded).transform(sales_encoded)\n",
    "\n",
    "# Assemble features\n",
    "sales_feature_cols = sales_numerical_cols + [f\"{col}_encoded\" for col in sales_categorical_cols]\n",
    "sales_assembler = VectorAssembler(inputCols=sales_feature_cols, outputCol=\"features\")\n",
    "sales_features = sales_assembler.transform(sales_encoded).withColumnRenamed(\"sales_amount\", \"total_amount\")\n",
    "sales_features.cache()\n",
    "\n",
    "print(f\"sales_features: {sales_features.count()} records\")\n",
    "\n",
    "# 3. Product Features for Clustering  \n",
    "# Create features for products dataset\n",
    "product_categorical_cols = [\"category\"]\n",
    "product_numerical_cols = [\"price\", \"rating\", \"num_reviews\", \"weight_kg\", \"length_cm\", \"width_cm\", \"height_cm\"]\n",
    "\n",
    "# Index categorical variables\n",
    "products_indexed = product_df\n",
    "for col_name in product_categorical_cols:\n",
    "    indexer = StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_indexed\")\n",
    "    products_indexed = indexer.fit(products_indexed).transform(products_indexed)\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "products_encoded = products_indexed  \n",
    "for col_name in product_categorical_cols:\n",
    "    encoder = OneHotEncoder(inputCol=f\"{col_name}_indexed\", outputCol=f\"{col_name}_encoded\")\n",
    "    products_encoded = encoder.fit(products_encoded).transform(products_encoded)\n",
    "\n",
    "# Assemble features\n",
    "product_feature_cols = product_numerical_cols + [f\"{col}_encoded\" for col in product_categorical_cols]\n",
    "products_assembler = VectorAssembler(inputCols=product_feature_cols, outputCol=\"features\")\n",
    "products_features = products_assembler.transform(products_encoded)\n",
    "products_features.cache()\n",
    "\n",
    "print(f\"products_features: {products_features.count()} records\")\n",
    "\n",
    "print(\"\\nAll ML datasets prepared successfully!\")\n",
    "print(\"Available datasets:\")\n",
    "print(\"- customers_features: Classification (churn prediction)\")  \n",
    "print(\"- sales_features: Regression (sales prediction)\")\n",
    "print(\"- products_features: Clustering (product segmentation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee600b1e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Supervised Learning Algorithms\n",
    "\n",
    "## Classification Algorithms\n",
    "\n",
    "**Classification** predicts discrete categories or classes:\n",
    "- **Logistic Regression**: Linear model for binary/multiclass classification\n",
    "- **Decision Trees**: Rule-based models with high interpretability\n",
    "- **Random Forest**: Ensemble of decision trees for improved accuracy\n",
    "- **Gradient Boosted Trees**: Sequential ensemble with error correction\n",
    "- **Naive Bayes**: Probabilistic classifier based on Bayes' theorem\n",
    "\n",
    "## Regression Algorithms\n",
    "\n",
    "**Regression** predicts continuous numerical values:\n",
    "- **Linear Regression**: Linear relationship modeling\n",
    "- **Decision Tree Regression**: Non-linear regression with rules\n",
    "- **Random Forest Regression**: Ensemble regression for better predictions\n",
    "- **Gradient Boosted Tree Regression**: Sequential improvement regression\n",
    "\n",
    "## Model Evaluation Metrics\n",
    "\n",
    "### Classification Metrics\n",
    "- **Accuracy**: Overall correct predictions\n",
    "- **Precision**: True positives / (True positives + False positives)\n",
    "- **Recall**: True positives / (True positives + False negatives)\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "- **AUC-ROC**: Area under the receiver operating characteristic curve\n",
    "\n",
    "### Regression Metrics\n",
    "- **RMSE**: Root Mean Square Error\n",
    "- **MAE**: Mean Absolute Error\n",
    "- **R²**: Coefficient of determination\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4acf1f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ML dataset variables...\n",
      "customers_features: 10000 records\n",
      "sales_features: 15000 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/25 22:51:56 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "products_features: 5000 records\n",
      "All ML datasets ready!\n"
     ]
    }
   ],
   "source": [
    "# Quick fix: Create ML-ready datasets for algorithms\n",
    "print(\"Creating ML dataset variables...\")\n",
    "\n",
    "# 1. Customer features for classification\n",
    "customers_features = ml_ready_data.withColumnRenamed(\"churn\", \"churn_label\")\n",
    "customers_features.cache()\n",
    "print(f\"customers_features: {customers_features.count()} records\")\n",
    "\n",
    "# 2. Sales features for regression  \n",
    "sales_assembler = VectorAssembler(\n",
    "    inputCols=[\"store_id\", \"day_of_week\", \"month\", \"temperature\", \"humidity\", \n",
    "              \"is_holiday\", \"promotion_active\", \"competitor_nearby\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "sales_features = sales_assembler.transform(sales_df).withColumnRenamed(\"sales_amount\", \"total_amount\")\n",
    "sales_features.cache()\n",
    "print(f\"sales_features: {sales_features.count()} records\")\n",
    "\n",
    "# 3. Product features for clustering\n",
    "products_assembler = VectorAssembler(\n",
    "    inputCols=[\"price\", \"rating\", \"num_reviews\", \"weight_kg\", \"length_cm\", \"width_cm\", \"height_cm\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "products_features = products_assembler.transform(product_df)\n",
    "products_features.cache()\n",
    "print(f\"products_features: {products_features.count()} records\")\n",
    "\n",
    "print(\"All ML datasets ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b64a4f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Algorithm Comparison\n",
      "==================================================\n",
      "Training set: 8046 records\n",
      "Test set: 1954 records\n",
      "\n",
      "Logistic Regression:\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/25 22:53:27 WARN CacheManager: Asked to cache already cached data.\n",
      "25/08/25 22:53:27 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.5210\n",
      "Accuracy: 0.8557\n",
      "Precision: 0.7322\n",
      "Recall: 0.8557\n",
      "F1-Score: 0.7891\n",
      "Training Time: 1.29 seconds\n",
      "\n",
      "Decision Tree:\n",
      "------------------------------\n",
      "AUC-ROC: 0.4687\n",
      "Accuracy: 0.7958\n",
      "Precision: 0.7451\n",
      "Recall: 0.7958\n",
      "F1-Score: 0.7682\n",
      "Training Time: 0.83 seconds\n",
      "\n",
      "Random Forest:\n",
      "------------------------------\n",
      "AUC-ROC: 0.4687\n",
      "Accuracy: 0.7958\n",
      "Precision: 0.7451\n",
      "Recall: 0.7958\n",
      "F1-Score: 0.7682\n",
      "Training Time: 0.83 seconds\n",
      "\n",
      "Random Forest:\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/25 22:53:31 WARN DAGScheduler: Broadcasting large task binary with size 1152.8 KiB\n",
      "25/08/25 22:53:32 WARN DAGScheduler: Broadcasting large task binary with size 1644.4 KiB\n",
      "25/08/25 22:53:32 WARN DAGScheduler: Broadcasting large task binary with size 1644.4 KiB\n",
      "25/08/25 22:53:32 WARN DAGScheduler: Broadcasting large task binary with size 1102.9 KiB\n",
      "25/08/25 22:53:32 WARN DAGScheduler: Broadcasting large task binary with size 1102.9 KiB\n",
      "25/08/25 22:53:32 WARN DAGScheduler: Broadcasting large task binary with size 1114.7 KiB\n",
      "25/08/25 22:53:33 WARN DAGScheduler: Broadcasting large task binary with size 1114.7 KiB\n",
      "25/08/25 22:53:32 WARN DAGScheduler: Broadcasting large task binary with size 1114.7 KiB\n",
      "25/08/25 22:53:33 WARN DAGScheduler: Broadcasting large task binary with size 1114.7 KiB\n",
      "25/08/25 22:53:33 WARN DAGScheduler: Broadcasting large task binary with size 1114.7 KiB\n",
      "25/08/25 22:53:33 WARN DAGScheduler: Broadcasting large task binary with size 1114.7 KiB\n",
      "25/08/25 22:53:33 WARN DAGScheduler: Broadcasting large task binary with size 1114.7 KiB\n",
      "25/08/25 22:53:33 WARN DAGScheduler: Broadcasting large task binary with size 1114.7 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.5025\n",
      "Accuracy: 0.8552\n",
      "Precision: 0.7321\n",
      "Recall: 0.8552\n",
      "F1-Score: 0.7889\n",
      "Training Time: 1.98 seconds\n",
      "\n",
      "Gradient Boosted Trees:\n",
      "------------------------------\n",
      "AUC-ROC: 0.5110\n",
      "Accuracy: 0.8547\n",
      "Precision: 0.7808\n",
      "Recall: 0.8547\n",
      "F1-Score: 0.7906\n",
      "Training Time: 2.64 seconds\n",
      "\n",
      "================================================================================\n",
      "CLASSIFICATION ALGORITHM COMPARISON SUMMARY\n",
      "================================================================================\n",
      "+------+--------+----------------------+--------+---------+------+-------------+\n",
      "|AUC   |Accuracy|Algorithm             |F1-Score|Precision|Recall|Training Time|\n",
      "+------+--------+----------------------+--------+---------+------+-------------+\n",
      "|0.521 |0.8557  |Logistic Regression   |0.7891  |0.7322   |0.8557|1.29         |\n",
      "|0.4687|0.7958  |Decision Tree         |0.7682  |0.7451   |0.7958|0.83         |\n",
      "|0.5025|0.8552  |Random Forest         |0.7889  |0.7321   |0.8552|1.98         |\n",
      "|0.511 |0.8547  |Gradient Boosted Trees|0.7906  |0.7808   |0.8547|2.64         |\n",
      "+------+--------+----------------------+--------+---------+------+-------------+\n",
      "\n",
      "\n",
      "Best F1-Score: Gradient Boosted Trees (0.7906)\n",
      "Fastest Training: Decision Tree (0.83 seconds)\n",
      "\n",
      "Classification analysis complete!\n",
      "AUC-ROC: 0.5110\n",
      "Accuracy: 0.8547\n",
      "Precision: 0.7808\n",
      "Recall: 0.8547\n",
      "F1-Score: 0.7906\n",
      "Training Time: 2.64 seconds\n",
      "\n",
      "================================================================================\n",
      "CLASSIFICATION ALGORITHM COMPARISON SUMMARY\n",
      "================================================================================\n",
      "+------+--------+----------------------+--------+---------+------+-------------+\n",
      "|AUC   |Accuracy|Algorithm             |F1-Score|Precision|Recall|Training Time|\n",
      "+------+--------+----------------------+--------+---------+------+-------------+\n",
      "|0.521 |0.8557  |Logistic Regression   |0.7891  |0.7322   |0.8557|1.29         |\n",
      "|0.4687|0.7958  |Decision Tree         |0.7682  |0.7451   |0.7958|0.83         |\n",
      "|0.5025|0.8552  |Random Forest         |0.7889  |0.7321   |0.8552|1.98         |\n",
      "|0.511 |0.8547  |Gradient Boosted Trees|0.7906  |0.7808   |0.8547|2.64         |\n",
      "+------+--------+----------------------+--------+---------+------+-------------+\n",
      "\n",
      "\n",
      "Best F1-Score: Gradient Boosted Trees (0.7906)\n",
      "Fastest Training: Decision Tree (0.83 seconds)\n",
      "\n",
      "Classification analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Section 2.1: Classification Algorithms\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "import time\n",
    "import builtins  # Add this to ensure we use Python's built-in functions\n",
    "\n",
    "print(\"Classification Algorithm Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Prepare training and test sets\n",
    "train_data, test_data = customers_features.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training set: {train_data.count()} records\")\n",
    "print(f\"Test set: {test_data.count()} records\")\n",
    "\n",
    "# Cache datasets for repeated use\n",
    "train_data.cache()\n",
    "test_data.cache()\n",
    "\n",
    "# Initialize classification algorithms\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(\n",
    "        featuresCol=\"features\", \n",
    "        labelCol=\"churn_label\",\n",
    "        maxIter=20\n",
    "    ),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(\n",
    "        featuresCol=\"features\", \n",
    "        labelCol=\"churn_label\",\n",
    "        maxDepth=10\n",
    "    ),\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        featuresCol=\"features\", \n",
    "        labelCol=\"churn_label\",\n",
    "        numTrees=20,\n",
    "        maxDepth=10\n",
    "    ),\n",
    "    \"Gradient Boosted Trees\": GBTClassifier(\n",
    "        featuresCol=\"features\", \n",
    "        labelCol=\"churn_label\",\n",
    "        maxIter=10,\n",
    "        maxDepth=5\n",
    "    )\n",
    "}\n",
    "\n",
    "# Evaluation metrics\n",
    "binary_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"churn_label\", \n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "multiclass_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"churn_label\", \n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "results = []\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Measure training time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train model\n",
    "    model = classifier.fit(train_data)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.transform(test_data)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc = binary_evaluator.evaluate(predictions)\n",
    "    accuracy = multiclass_evaluator.evaluate(predictions, {multiclass_evaluator.metricName: \"accuracy\"})\n",
    "    precision = multiclass_evaluator.evaluate(predictions, {multiclass_evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = multiclass_evaluator.evaluate(predictions, {multiclass_evaluator.metricName: \"weightedRecall\"})\n",
    "    f1 = multiclass_evaluator.evaluate(predictions, {multiclass_evaluator.metricName: \"f1\"})\n",
    "    \n",
    "    # Store results\n",
    "    import builtins\n",
    "    result = {\n",
    "        \"Algorithm\": name,\n",
    "        \"AUC\": builtins.round(auc, 4),\n",
    "        \"Accuracy\": builtins.round(accuracy, 4),\n",
    "        \"Precision\": builtins.round(precision, 4),\n",
    "        \"Recall\": builtins.round(recall, 4),\n",
    "        \"F1-Score\": builtins.round(f1, 4),\n",
    "        \"Training Time\": builtins.round(training_time, 2)\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"AUC-ROC: {auc:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLASSIFICATION ALGORITHM COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "results_df = spark.createDataFrame(results)\n",
    "results_df.show(truncate=False)\n",
    "\n",
    "# Find best performing algorithm by F1-Score\n",
    "best_f1 = builtins.max(results, key=lambda x: x[\"F1-Score\"])\n",
    "print(f\"\\nBest F1-Score: {best_f1['Algorithm']} ({best_f1['F1-Score']})\")\n",
    "\n",
    "# Find fastest training algorithm\n",
    "fastest = builtins.min(results, key=lambda x: x[\"Training Time\"])\n",
    "print(f\"Fastest Training: {fastest['Algorithm']} ({fastest['Training Time']} seconds)\")\n",
    "\n",
    "print(\"\\nClassification analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0605acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Algorithm Comparison\n",
      "==================================================\n",
      "Training set: 12085 records\n",
      "Test set: 2915 records\n",
      "\n",
      "Linear Regression:\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/25 22:54:09 WARN Instrumentation: [89a8dc0b] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/08/25 22:54:09 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "25/08/25 22:54:09 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 395.56\n",
      "MAE: 319.93\n",
      "R²: 0.2650\n",
      "Training Time: 0.43 seconds\n",
      "\n",
      "Sample Predictions:\n",
      "+------------+------------------+\n",
      "|total_amount|        prediction|\n",
      "+------------+------------------+\n",
      "|     2475.48| 2094.593521200074|\n",
      "|     2433.94| 2027.285355126064|\n",
      "|     2982.46|  2412.30674820032|\n",
      "|     2204.51|1943.2882513798668|\n",
      "|     2112.11|2389.5013394502184|\n",
      "+------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Decision Tree:\n",
      "------------------------------\n",
      "RMSE: 433.31\n",
      "MAE: 346.87\n",
      "R²: 0.1180\n",
      "Training Time: 0.68 seconds\n",
      "\n",
      "Sample Predictions:\n",
      "+------------+------------------+\n",
      "|total_amount|        prediction|\n",
      "+------------+------------------+\n",
      "|     2475.48|2150.1538095238093|\n",
      "|     2433.94| 2220.969122807017|\n",
      "|     2982.46|2356.3030769230772|\n",
      "|     2204.51|1959.1377628032344|\n",
      "|     2112.11|2356.3030769230772|\n",
      "+------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Random Forest:\n",
      "------------------------------\n",
      "RMSE: 433.31\n",
      "MAE: 346.87\n",
      "R²: 0.1180\n",
      "Training Time: 0.68 seconds\n",
      "\n",
      "Sample Predictions:\n",
      "+------------+------------------+\n",
      "|total_amount|        prediction|\n",
      "+------------+------------------+\n",
      "|     2475.48|2150.1538095238093|\n",
      "|     2433.94| 2220.969122807017|\n",
      "|     2982.46|2356.3030769230772|\n",
      "|     2204.51|1959.1377628032344|\n",
      "|     2112.11|2356.3030769230772|\n",
      "+------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Random Forest:\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/25 22:54:11 WARN DAGScheduler: Broadcasting large task binary with size 1469.4 KiB\n",
      "25/08/25 22:54:12 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/08/25 22:54:12 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 398.59\n",
      "MAE: 321.57\n",
      "R²: 0.2537\n",
      "Training Time: 2.12 seconds\n",
      "\n",
      "Sample Predictions:\n",
      "+------------+------------------+\n",
      "|total_amount|        prediction|\n",
      "+------------+------------------+\n",
      "|     2475.48| 1999.548516777675|\n",
      "|     2433.94| 2012.909331753898|\n",
      "|     2982.46|2499.4297793143855|\n",
      "|     2204.51|1911.5751781192134|\n",
      "|     2112.11|2368.3664765789536|\n",
      "+------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Gradient Boosted Trees:\n",
      "------------------------------\n",
      "RMSE: 396.88\n",
      "MAE: 320.55\n",
      "R²: 0.2601\n",
      "Training Time: 2.54 seconds\n",
      "\n",
      "Sample Predictions:\n",
      "+------------+------------------+\n",
      "|total_amount|        prediction|\n",
      "+------------+------------------+\n",
      "|     2475.48| 2018.434365016623|\n",
      "|     2433.94|2122.8653830757153|\n",
      "|     2982.46| 2434.429295146346|\n",
      "|     2204.51| 1913.953447206258|\n",
      "|     2112.11| 2451.212015955912|\n",
      "+------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "================================================================================\n",
      "REGRESSION ALGORITHM COMPARISON SUMMARY\n",
      "================================================================================\n",
      "+----------------------+------+------+------+-------------+\n",
      "|Algorithm             |MAE   |RMSE  |R²    |Training Time|\n",
      "+----------------------+------+------+------+-------------+\n",
      "|Linear Regression     |319.93|395.56|0.265 |0.43         |\n",
      "|Decision Tree         |346.87|433.31|0.118 |0.68         |\n",
      "|Random Forest         |321.57|398.59|0.2537|2.12         |\n",
      "|Gradient Boosted Trees|320.55|396.88|0.2601|2.54         |\n",
      "+----------------------+------+------+------+-------------+\n",
      "\n",
      "\n",
      "Best R²: Linear Regression (0.265)\n",
      "Lowest RMSE: Linear Regression (395.56)\n",
      "\n",
      "Regression analysis complete!\n",
      "RMSE: 396.88\n",
      "MAE: 320.55\n",
      "R²: 0.2601\n",
      "Training Time: 2.54 seconds\n",
      "\n",
      "Sample Predictions:\n",
      "+------------+------------------+\n",
      "|total_amount|        prediction|\n",
      "+------------+------------------+\n",
      "|     2475.48| 2018.434365016623|\n",
      "|     2433.94|2122.8653830757153|\n",
      "|     2982.46| 2434.429295146346|\n",
      "|     2204.51| 1913.953447206258|\n",
      "|     2112.11| 2451.212015955912|\n",
      "+------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "================================================================================\n",
      "REGRESSION ALGORITHM COMPARISON SUMMARY\n",
      "================================================================================\n",
      "+----------------------+------+------+------+-------------+\n",
      "|Algorithm             |MAE   |RMSE  |R²    |Training Time|\n",
      "+----------------------+------+------+------+-------------+\n",
      "|Linear Regression     |319.93|395.56|0.265 |0.43         |\n",
      "|Decision Tree         |346.87|433.31|0.118 |0.68         |\n",
      "|Random Forest         |321.57|398.59|0.2537|2.12         |\n",
      "|Gradient Boosted Trees|320.55|396.88|0.2601|2.54         |\n",
      "+----------------------+------+------+------+-------------+\n",
      "\n",
      "\n",
      "Best R²: Linear Regression (0.265)\n",
      "Lowest RMSE: Linear Regression (395.56)\n",
      "\n",
      "Regression analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Section 2.2: Regression Algorithms\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import math\n",
    "import builtins  # Add this to ensure we use Python's built-in functions\n",
    "\n",
    "print(\"Regression Algorithm Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Prepare sales prediction data\n",
    "sales_train, sales_test = sales_features.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training set: {sales_train.count()} records\")\n",
    "print(f\"Test set: {sales_test.count()} records\")\n",
    "\n",
    "# Cache datasets\n",
    "sales_train.cache()\n",
    "sales_test.cache()\n",
    "\n",
    "# Initialize regression algorithms\n",
    "regressors = {\n",
    "    \"Linear Regression\": LinearRegression(\n",
    "        featuresCol=\"features\", \n",
    "        labelCol=\"total_amount\",\n",
    "        maxIter=20\n",
    "    ),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(\n",
    "        featuresCol=\"features\", \n",
    "        labelCol=\"total_amount\",\n",
    "        maxDepth=10\n",
    "    ),\n",
    "    \"Random Forest\": RandomForestRegressor(\n",
    "        featuresCol=\"features\", \n",
    "        labelCol=\"total_amount\",\n",
    "        numTrees=20,\n",
    "        maxDepth=10\n",
    "    ),\n",
    "    \"Gradient Boosted Trees\": GBTRegressor(\n",
    "        featuresCol=\"features\", \n",
    "        labelCol=\"total_amount\",\n",
    "        maxIter=10,\n",
    "        maxDepth=5\n",
    "    )\n",
    "}\n",
    "\n",
    "# Evaluation metrics\n",
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"total_amount\", \n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "mae_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"total_amount\", \n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"mae\"\n",
    ")\n",
    "\n",
    "r2_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"total_amount\", \n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "\n",
    "# Train and evaluate each regressor\n",
    "regression_results = []\n",
    "\n",
    "for name, regressor in regressors.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Measure training time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train model\n",
    "    model = regressor.fit(sales_train)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.transform(sales_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = rmse_evaluator.evaluate(predictions)\n",
    "    mae = mae_evaluator.evaluate(predictions)\n",
    "    r2 = r2_evaluator.evaluate(predictions)\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        \"Algorithm\": name,\n",
    "        \"RMSE\": builtins.round(rmse, 2),\n",
    "        \"MAE\": builtins.round(mae, 2),\n",
    "        \"R²\": builtins.round(r2, 4),\n",
    "        \"Training Time\": builtins.round(training_time, 2)\n",
    "    }\n",
    "    regression_results.append(result)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Show sample predictions\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    predictions.select(\"total_amount\", \"prediction\").show(5)\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"REGRESSION ALGORITHM COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "regression_results_df = spark.createDataFrame(regression_results)\n",
    "regression_results_df.show(truncate=False)\n",
    "\n",
    "# Find best performing algorithm by R²\n",
    "best_r2 = builtins.max(regression_results, key=lambda x: x[\"R²\"])\n",
    "print(f\"\\nBest R²: {best_r2['Algorithm']} ({best_r2['R²']})\")\n",
    "\n",
    "# Find lowest RMSE\n",
    "lowest_rmse = builtins.min(regression_results, key=lambda x: x[\"RMSE\"])\n",
    "print(f\"Lowest RMSE: {lowest_rmse['Algorithm']} ({lowest_rmse['RMSE']})\")\n",
    "\n",
    "print(\"\\nRegression analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e94de90",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: Unsupervised Learning\n",
    "\n",
    "## Clustering Algorithms\n",
    "\n",
    "**Clustering** groups similar data points without predefined labels:\n",
    "- **K-Means**: Partitions data into k clusters based on centroids\n",
    "- **Gaussian Mixture Model**: Probabilistic clustering with soft assignments\n",
    "- **Bisecting K-Means**: Hierarchical variant of K-Means\n",
    "\n",
    "## Dimensionality Reduction\n",
    "\n",
    "**Dimensionality Reduction** reduces feature space while preserving information:\n",
    "- **Principal Component Analysis (PCA)**: Linear transformation to principal components\n",
    "- **Feature Selection**: Selecting most relevant features\n",
    "\n",
    "## Clustering Evaluation\n",
    "\n",
    "- **Silhouette Score**: Measures cluster cohesion and separation\n",
    "- **Within Set Sum of Squared Errors (WSSSE)**: Measures cluster compactness\n",
    "- **Calinski-Harabasz Index**: Ratio of between-cluster to within-cluster variance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41de7dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Algorithm Comparison\n",
      "==================================================\n",
      "Products for clustering: 1504 records\n",
      "\n",
      "K-Means:\n",
      "------------------------------\n",
      "Silhouette Score: 0.5307\n",
      "Training Time: 1.26 seconds\n",
      "Cluster Distribution:\n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         0|  291|\n",
      "|         1|  239|\n",
      "|         2|  400|\n",
      "|         3|  255|\n",
      "|         4|  319|\n",
      "+----------+-----+\n",
      "\n",
      "Within Set Sum of Squared Errors: 29640786.48\n",
      "\n",
      "Gaussian Mixture:\n",
      "------------------------------\n",
      "Silhouette Score: 0.5307\n",
      "Training Time: 1.26 seconds\n",
      "Cluster Distribution:\n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         0|  291|\n",
      "|         1|  239|\n",
      "|         2|  400|\n",
      "|         3|  255|\n",
      "|         4|  319|\n",
      "+----------+-----+\n",
      "\n",
      "Within Set Sum of Squared Errors: 29640786.48\n",
      "\n",
      "Gaussian Mixture:\n",
      "------------------------------\n",
      "Silhouette Score: -0.0273\n",
      "Training Time: 4.20 seconds\n",
      "Cluster Distribution:\n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         0|  274|\n",
      "|         1|  398|\n",
      "|         2|  374|\n",
      "|         3|  225|\n",
      "|         4|  233|\n",
      "+----------+-----+\n",
      "\n",
      "Log Likelihood: -45774.49\n",
      "\n",
      "Bisecting K-Means:\n",
      "------------------------------\n",
      "Silhouette Score: -0.0273\n",
      "Training Time: 4.20 seconds\n",
      "Cluster Distribution:\n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         0|  274|\n",
      "|         1|  398|\n",
      "|         2|  374|\n",
      "|         3|  225|\n",
      "|         4|  233|\n",
      "+----------+-----+\n",
      "\n",
      "Log Likelihood: -45774.49\n",
      "\n",
      "Bisecting K-Means:\n",
      "------------------------------\n",
      "Silhouette Score: 0.4234\n",
      "Training Time: 2.45 seconds\n",
      "Cluster Distribution:\n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         0|  223|\n",
      "|         1|  175|\n",
      "|         2|  348|\n",
      "|         3|  374|\n",
      "|         4|  384|\n",
      "+----------+-----+\n",
      "\n",
      "\n",
      "============================================================\n",
      "CLUSTERING ALGORITHM COMPARISON SUMMARY\n",
      "============================================================\n",
      "+-----------------+----------------+-------------+\n",
      "|Algorithm        |Silhouette Score|Training Time|\n",
      "+-----------------+----------------+-------------+\n",
      "|K-Means          |0.5307          |1.26         |\n",
      "|Gaussian Mixture |-0.0273         |4.2          |\n",
      "|Bisecting K-Means|0.4234          |2.45         |\n",
      "+-----------------+----------------+-------------+\n",
      "\n",
      "\n",
      "Best Silhouette Score: K-Means (0.5307)\n",
      "\n",
      "Clustering analysis complete!\n",
      "Silhouette Score: 0.4234\n",
      "Training Time: 2.45 seconds\n",
      "Cluster Distribution:\n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         0|  223|\n",
      "|         1|  175|\n",
      "|         2|  348|\n",
      "|         3|  374|\n",
      "|         4|  384|\n",
      "+----------+-----+\n",
      "\n",
      "\n",
      "============================================================\n",
      "CLUSTERING ALGORITHM COMPARISON SUMMARY\n",
      "============================================================\n",
      "+-----------------+----------------+-------------+\n",
      "|Algorithm        |Silhouette Score|Training Time|\n",
      "+-----------------+----------------+-------------+\n",
      "|K-Means          |0.5307          |1.26         |\n",
      "|Gaussian Mixture |-0.0273         |4.2          |\n",
      "|Bisecting K-Means|0.4234          |2.45         |\n",
      "+-----------------+----------------+-------------+\n",
      "\n",
      "\n",
      "Best Silhouette Score: K-Means (0.5307)\n",
      "\n",
      "Clustering analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Section 3.1: Clustering Algorithms\n",
    "\n",
    "from pyspark.ml.clustering import KMeans, GaussianMixture, BisectingKMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import builtins  # Add this to ensure we use Python's built-in functions\n",
    "\n",
    "print(\"Clustering Algorithm Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Use product features for clustering\n",
    "products_sample = products_features.sample(0.3, seed=42)  # Sample for faster computation\n",
    "products_sample.cache()\n",
    "\n",
    "print(f\"Products for clustering: {products_sample.count()} records\")\n",
    "\n",
    "# Initialize clustering algorithms\n",
    "clusterers = {\n",
    "    \"K-Means\": KMeans(featuresCol=\"features\", k=5, seed=42),\n",
    "    \"Gaussian Mixture\": GaussianMixture(featuresCol=\"features\", k=5, seed=42),\n",
    "    \"Bisecting K-Means\": BisectingKMeans(featuresCol=\"features\", k=5, seed=42)\n",
    "}\n",
    "\n",
    "# Clustering evaluator\n",
    "silhouette_evaluator = ClusteringEvaluator(\n",
    "    predictionCol=\"prediction\",\n",
    "    featuresCol=\"features\",\n",
    "    metricName=\"silhouette\",\n",
    "    distanceMeasure=\"squaredEuclidean\"\n",
    ")\n",
    "\n",
    "# Train and evaluate each clustering algorithm\n",
    "clustering_results = []\n",
    "\n",
    "for name, clusterer in clusterers.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Measure training time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train model\n",
    "    model = clusterer.fit(products_sample)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.transform(products_sample)\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    silhouette = silhouette_evaluator.evaluate(predictions)\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        \"Algorithm\": name,\n",
    "        \"Silhouette Score\": builtins.round(silhouette, 4),\n",
    "        \"Training Time\": builtins.round(training_time, 2)\n",
    "    }\n",
    "    clustering_results.append(result)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Silhouette Score: {silhouette:.4f}\")\n",
    "    print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Show cluster distribution\n",
    "    cluster_counts = predictions.groupBy(\"prediction\").count().orderBy(\"prediction\")\n",
    "    print(\"Cluster Distribution:\")\n",
    "    cluster_counts.show()\n",
    "    \n",
    "    # Additional algorithm-specific metrics\n",
    "    if name == \"K-Means\":\n",
    "        print(f\"Within Set Sum of Squared Errors: {model.summary.trainingCost:.2f}\")\n",
    "    elif name == \"Gaussian Mixture\":\n",
    "        print(f\"Log Likelihood: {model.summary.logLikelihood:.2f}\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLUSTERING ALGORITHM COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "clustering_results_df = spark.createDataFrame(clustering_results)\n",
    "clustering_results_df.show(truncate=False)\n",
    "\n",
    "# Find best performing algorithm by silhouette score\n",
    "best_silhouette = builtins.max(clustering_results, key=lambda x: x[\"Silhouette Score\"])\n",
    "print(f\"\\nBest Silhouette Score: {best_silhouette['Algorithm']} ({best_silhouette['Silhouette Score']})\")\n",
    "\n",
    "print(\"\\nClustering analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8eb1b5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality Reduction Techniques\n",
      "==================================================\n",
      "Original Feature Analysis:\n",
      "------------------------------\n",
      "Original feature dimensions: 12\n",
      "Feature correlation matrix shape: (12, 12)\n",
      "Highly correlated feature pairs (|r| > 0.8): 0\n",
      "\n",
      "Principal Component Analysis:\n",
      "------------------------------\n",
      "\n",
      "PCA with 3 components:\n",
      "Training time: 0.14 seconds\n",
      "Explained variance ratio: [0.29955737 0.15449658 0.15220181]\n",
      "Cumulative explained variance: 0.6063\n",
      "Sample PCA features:\n",
      "+-------------------------------------------------------------+\n",
      "|pca_features                                                 |\n",
      "+-------------------------------------------------------------+\n",
      "|[-2.2892407617035317,-0.8474004717264122,-0.6739176158414207]|\n",
      "|[-1.7528877435904526,0.4881883879713173,0.16408615342744615] |\n",
      "|[0.9163851628851283,-0.4456221025432198,-1.323281603589831]  |\n",
      "+-------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "PCA with 6 components:\n",
      "Training time: 0.14 seconds\n",
      "Explained variance ratio: [0.29955737 0.15449658 0.15220181 0.14858285 0.06279039 0.05425693]\n",
      "Cumulative explained variance: 0.8719\n",
      "Sample PCA features:\n",
      "Training time: 0.14 seconds\n",
      "Explained variance ratio: [0.29955737 0.15449658 0.15220181]\n",
      "Cumulative explained variance: 0.6063\n",
      "Sample PCA features:\n",
      "+-------------------------------------------------------------+\n",
      "|pca_features                                                 |\n",
      "+-------------------------------------------------------------+\n",
      "|[-2.2892407617035317,-0.8474004717264122,-0.6739176158414207]|\n",
      "|[-1.7528877435904526,0.4881883879713173,0.16408615342744615] |\n",
      "|[0.9163851628851283,-0.4456221025432198,-1.323281603589831]  |\n",
      "+-------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "PCA with 6 components:\n",
      "Training time: 0.14 seconds\n",
      "Explained variance ratio: [0.29955737 0.15449658 0.15220181 0.14858285 0.06279039 0.05425693]\n",
      "Cumulative explained variance: 0.8719\n",
      "Sample PCA features:\n",
      "+-------------------------------------------------------------------------------------------------------------------------+\n",
      "|pca_features                                                                                                             |\n",
      "+-------------------------------------------------------------------------------------------------------------------------+\n",
      "|[-2.2892407617035317,-0.8474004717264122,-0.6739176158414207,-0.39910975854678904,0.6419672780114725,0.44860545108235206]|\n",
      "|[-1.7528877435904526,0.4881883879713173,0.16408615342744615,1.2482818098589643,-0.7194033500242449,0.3664605140969539]   |\n",
      "|[0.9163851628851283,-0.4456221025432198,-1.323281603589831,0.09542024993784326,-0.7552478285159353,0.72724035266366]     |\n",
      "+-------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "PCA with 10 components:\n",
      "Training time: 0.14 seconds\n",
      "Explained variance ratio: [0.29955737 0.15449658 0.15220181 0.14858285 0.06279039 0.05425693\n",
      " 0.05060113 0.04013905 0.0151766  0.01100897]\n",
      "Cumulative explained variance: 0.9888\n",
      "Sample PCA features:\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|pca_features                                                                                                                                                                                            |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[-2.2892407617035317,-0.8474004717264122,-0.6739176158414207,-0.39910975854678904,0.6419672780114725,0.44860545108235206,-0.94327110921341,-0.18668753402448096,-0.7478349054858914,-0.7270877215008453]|\n",
      "|[-1.7528877435904526,0.4881883879713173,0.16408615342744615,1.2482818098589643,-0.7194033500242449,0.3664605140969539,-0.9590508069621309,-0.16263822815341625,-0.7324444046421904,-0.7153960266404613] |\n",
      "|[0.9163851628851283,-0.4456221025432198,-1.323281603589831,0.09542024993784326,-0.7552478285159353,0.72724035266366,0.3517683866830219,-0.6561534227079878,-0.7669267293955772,-0.7084392347190002]     |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Feature Selection:\n",
      "------------------------------\n",
      "+-------------------------------------------------------------------------------------------------------------------------+\n",
      "|pca_features                                                                                                             |\n",
      "+-------------------------------------------------------------------------------------------------------------------------+\n",
      "|[-2.2892407617035317,-0.8474004717264122,-0.6739176158414207,-0.39910975854678904,0.6419672780114725,0.44860545108235206]|\n",
      "|[-1.7528877435904526,0.4881883879713173,0.16408615342744615,1.2482818098589643,-0.7194033500242449,0.3664605140969539]   |\n",
      "|[0.9163851628851283,-0.4456221025432198,-1.323281603589831,0.09542024993784326,-0.7552478285159353,0.72724035266366]     |\n",
      "+-------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "PCA with 10 components:\n",
      "Training time: 0.14 seconds\n",
      "Explained variance ratio: [0.29955737 0.15449658 0.15220181 0.14858285 0.06279039 0.05425693\n",
      " 0.05060113 0.04013905 0.0151766  0.01100897]\n",
      "Cumulative explained variance: 0.9888\n",
      "Sample PCA features:\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|pca_features                                                                                                                                                                                            |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[-2.2892407617035317,-0.8474004717264122,-0.6739176158414207,-0.39910975854678904,0.6419672780114725,0.44860545108235206,-0.94327110921341,-0.18668753402448096,-0.7478349054858914,-0.7270877215008453]|\n",
      "|[-1.7528877435904526,0.4881883879713173,0.16408615342744615,1.2482818098589643,-0.7194033500242449,0.3664605140969539,-0.9590508069621309,-0.16263822815341625,-0.7324444046421904,-0.7153960266404613] |\n",
      "|[0.9163851628851283,-0.4456221025432198,-1.323281603589831,0.09542024993784326,-0.7552478285159353,0.72724035266366,0.3517683866830219,-0.6561534227079878,-0.7669267293955772,-0.7084392347190002]     |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Feature Selection:\n",
      "------------------------------\n",
      "Original features: 12\n",
      "High variance features: 6\n",
      "Variance threshold: 0.2433\n",
      "Sample selected features:\n",
      "+------------------------------------------------------------------------------------------------------+\n",
      "|selected_features                                                                                     |\n",
      "+------------------------------------------------------------------------------------------------------+\n",
      "|[0.39221008788076545,1.071384108693034,1.0385974481123896,1.7248154496904555,1.092161904561792,0.0]   |\n",
      "|[0.04475868918056139,0.5512105650647605,1.278845694772319,1.3222956076130612,-1.0860628751785217,1.0] |\n",
      "|[1.1566031650212143,-0.8359188846106357,0.030505434699412207,-0.64963739774168,0.6565169486137292,1.0]|\n",
      "+------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "======================================================================\n",
      "DIMENSIONALITY REDUCTION SUMMARY\n",
      "======================================================================\n",
      "+----------+-----------------+-------------------+-------------------+\n",
      "|Components|Compression Ratio|Cumulative Variance|Dimension Reduction|\n",
      "+----------+-----------------+-------------------+-------------------+\n",
      "|3         |0.25             |0.6063             |12 -> 3            |\n",
      "|6         |0.5              |0.8719             |12 -> 6            |\n",
      "|10        |0.833            |0.9888             |12 -> 10           |\n",
      "+----------+-----------------+-------------------+-------------------+\n",
      "\n",
      "\n",
      "Feature Selection Results:\n",
      "Original dimensions: 12\n",
      "Selected dimensions: 6\n",
      "Reduction ratio: 0.500\n",
      "\n",
      "Recommendations:\n",
      "- PCA with 10 components retains 87.2% of variance\n",
      "- Feature selection reduces dimensions by 50.0%\n",
      "- Consider PCA for linear dimensionality reduction\n",
      "- Consider feature selection for interpretable models\n",
      "\n",
      "Dimensionality reduction analysis complete!\n",
      "Original features: 12\n",
      "High variance features: 6\n",
      "Variance threshold: 0.2433\n",
      "Sample selected features:\n",
      "+------------------------------------------------------------------------------------------------------+\n",
      "|selected_features                                                                                     |\n",
      "+------------------------------------------------------------------------------------------------------+\n",
      "|[0.39221008788076545,1.071384108693034,1.0385974481123896,1.7248154496904555,1.092161904561792,0.0]   |\n",
      "|[0.04475868918056139,0.5512105650647605,1.278845694772319,1.3222956076130612,-1.0860628751785217,1.0] |\n",
      "|[1.1566031650212143,-0.8359188846106357,0.030505434699412207,-0.64963739774168,0.6565169486137292,1.0]|\n",
      "+------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "======================================================================\n",
      "DIMENSIONALITY REDUCTION SUMMARY\n",
      "======================================================================\n",
      "+----------+-----------------+-------------------+-------------------+\n",
      "|Components|Compression Ratio|Cumulative Variance|Dimension Reduction|\n",
      "+----------+-----------------+-------------------+-------------------+\n",
      "|3         |0.25             |0.6063             |12 -> 3            |\n",
      "|6         |0.5              |0.8719             |12 -> 6            |\n",
      "|10        |0.833            |0.9888             |12 -> 10           |\n",
      "+----------+-----------------+-------------------+-------------------+\n",
      "\n",
      "\n",
      "Feature Selection Results:\n",
      "Original dimensions: 12\n",
      "Selected dimensions: 6\n",
      "Reduction ratio: 0.500\n",
      "\n",
      "Recommendations:\n",
      "- PCA with 10 components retains 87.2% of variance\n",
      "- Feature selection reduces dimensions by 50.0%\n",
      "- Consider PCA for linear dimensionality reduction\n",
      "- Consider feature selection for interpretable models\n",
      "\n",
      "Dimensionality reduction analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Section 3.2: Dimensionality Reduction\n",
    "\n",
    "from pyspark.ml.feature import PCA, VectorSlicer\n",
    "from pyspark.ml.stat import Correlation\n",
    "import numpy as np\n",
    "import builtins  # Add this to ensure we use Python's built-in functions\n",
    "\n",
    "print(\"Dimensionality Reduction Techniques\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analyze original feature dimensions\n",
    "print(\"Original Feature Analysis:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Get feature vector size\n",
    "sample_features = customers_features.select(\"features\").first()[\"features\"]\n",
    "original_dimensions = builtins.len(sample_features.toArray())\n",
    "print(f\"Original feature dimensions: {original_dimensions}\")\n",
    "\n",
    "# Compute correlation matrix for feature analysis\n",
    "correlation_matrix = Correlation.corr(customers_features, \"features\").head()[0]\n",
    "correlation_array = correlation_matrix.toArray()\n",
    "print(f\"Feature correlation matrix shape: {correlation_array.shape}\")\n",
    "\n",
    "# Find highly correlated features\n",
    "high_correlation_pairs = []\n",
    "threshold = 0.8\n",
    "for i in range(builtins.len(correlation_array)):\n",
    "    for j in range(i+1, builtins.len(correlation_array)):\n",
    "        if builtins.abs(correlation_array[i][j]) > threshold:\n",
    "            high_correlation_pairs.append((i, j, correlation_array[i][j]))\n",
    "\n",
    "print(f\"Highly correlated feature pairs (|r| > {threshold}): {builtins.len(high_correlation_pairs)}\")\n",
    "\n",
    "# Principal Component Analysis (PCA)\n",
    "print(f\"\\nPrincipal Component Analysis:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Apply PCA with different numbers of components\n",
    "pca_components = [3, 6, 10]  # Reduced to fit within 12 dimensions\n",
    "pca_results = []\n",
    "\n",
    "for n_components in pca_components:\n",
    "    print(f\"\\nPCA with {n_components} components:\")\n",
    "    \n",
    "    # Initialize PCA\n",
    "    pca = PCA(k=n_components, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "    \n",
    "    # Fit PCA model\n",
    "    start_time = time.time()\n",
    "    pca_model = pca.fit(customers_features)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Transform data\n",
    "    pca_result = pca_model.transform(customers_features)\n",
    "    \n",
    "    # Get explained variance\n",
    "    explained_variance = pca_model.explainedVariance.toArray()\n",
    "    cumulative_variance = np.cumsum(explained_variance)\n",
    "    \n",
    "    print(f\"Training time: {training_time:.2f} seconds\")\n",
    "    print(f\"Explained variance ratio: {explained_variance}\")\n",
    "    print(f\"Cumulative explained variance: {cumulative_variance[-1]:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    pca_results.append({\n",
    "        \"Components\": n_components,\n",
    "        \"Cumulative Variance\": float(builtins.round(cumulative_variance[-1], 4)),\n",
    "        \"Dimension Reduction\": f\"{original_dimensions} -> {n_components}\",\n",
    "        \"Compression Ratio\": float(builtins.round(n_components / original_dimensions, 3))\n",
    "    })\n",
    "    \n",
    "    # Show sample transformed data\n",
    "    print(\"Sample PCA features:\")\n",
    "    pca_result.select(\"pca_features\").show(3, truncate=False)\n",
    "\n",
    "# Feature Selection using variance threshold\n",
    "print(f\"\\nFeature Selection:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Calculate feature variances (approximate using statistics)\n",
    "feature_stats = customers_features.select(\"features\").rdd.map(\n",
    "    lambda row: row.features.toArray()\n",
    ").collect()\n",
    "\n",
    "# Convert to numpy array for easier computation\n",
    "feature_array = np.array(feature_stats)\n",
    "feature_variances = np.var(feature_array, axis=0)\n",
    "\n",
    "# Select features with high variance\n",
    "variance_threshold = np.percentile(feature_variances, 50)  # Top 50% by variance\n",
    "high_variance_indices = [i for i, var in enumerate(feature_variances) if var > variance_threshold]\n",
    "\n",
    "print(f\"Original features: {builtins.len(feature_variances)}\")\n",
    "print(f\"High variance features: {builtins.len(high_variance_indices)}\")\n",
    "print(f\"Variance threshold: {float(variance_threshold):.4f}\")\n",
    "\n",
    "# Apply feature selection\n",
    "vector_slicer = VectorSlicer(\n",
    "    inputCol=\"features\", \n",
    "    outputCol=\"selected_features\", \n",
    "    indices=high_variance_indices\n",
    ")\n",
    "\n",
    "selected_features_df = vector_slicer.transform(customers_features)\n",
    "\n",
    "print(\"Sample selected features:\")\n",
    "selected_features_df.select(\"selected_features\").show(3, truncate=False)\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DIMENSIONALITY REDUCTION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# PCA results summary\n",
    "pca_summary_df = spark.createDataFrame(pca_results)\n",
    "pca_summary_df.show(truncate=False)\n",
    "\n",
    "print(f\"\\nFeature Selection Results:\")\n",
    "print(f\"Original dimensions: {original_dimensions}\")\n",
    "print(f\"Selected dimensions: {builtins.len(high_variance_indices)}\")\n",
    "print(f\"Reduction ratio: {float(builtins.len(high_variance_indices)/original_dimensions):.3f}\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\nRecommendations:\")\n",
    "print(f\"- PCA with 10 components retains {pca_results[1]['Cumulative Variance']:.1%} of variance\")\n",
    "print(f\"- Feature selection reduces dimensions by {float(1-builtins.len(high_variance_indices)/original_dimensions):.1%}\")\n",
    "print(f\"- Consider PCA for linear dimensionality reduction\")\n",
    "print(f\"- Consider feature selection for interpretable models\")\n",
    "\n",
    "print(\"\\nDimensionality reduction analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84262e1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 4: Model Pipelines and Advanced Topics\n",
    "\n",
    "## ML Pipelines\n",
    "\n",
    "**ML Pipelines** provide a high-level API for building machine learning workflows:\n",
    "- **Pipeline**: Sequence of stages (transformers and estimators)\n",
    "- **Transformer**: Transforms one DataFrame to another\n",
    "- **Estimator**: Fits a model to data and produces a transformer\n",
    "- **Parameter**: Named parameter for ML algorithms\n",
    "\n",
    "## Model Selection and Tuning\n",
    "\n",
    "**Hyperparameter Tuning** optimizes model performance:\n",
    "- **Cross Validation**: Evaluates model performance across multiple data splits\n",
    "- **Train-Validation Split**: Single train/validation split for faster tuning\n",
    "- **Parameter Grid**: Defines hyperparameter search space\n",
    "- **Model Selection**: Chooses best model based on evaluation metrics\n",
    "\n",
    "## Model Persistence\n",
    "\n",
    "**Model Saving and Loading**:\n",
    "- **Save**: Persist trained models to disk\n",
    "- **Load**: Reload models for prediction\n",
    "- **Versioning**: Track model versions and metadata\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6043f2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML Pipeline Construction\n",
      "==================================================\n",
      "Building end-to-end ML pipeline for churn prediction...\n",
      "Pipeline stages:\n",
      "  1. VectorAssembler\n",
      "  2. RandomForestClassifier\n",
      "\n",
      "Training set: 8046 records\n",
      "Test set: 1954 records\n",
      "\n",
      "Training complete pipeline...\n",
      "Test set: 1954 records\n",
      "\n",
      "Training complete pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/25 23:01:29 WARN DAGScheduler: Broadcasting large task binary with size 1457.3 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline training completed in 1.93 seconds\n",
      "\n",
      "Making predictions...\n",
      "Pipeline AUC: 0.4912\n",
      "\n",
      "Pipeline prediction results:\n",
      "+-----------+---+---------------+-----+----------+--------------------+\n",
      "|customer_id|age|monthly_charges|churn|prediction|         probability|\n",
      "+-----------+---+---------------+-----+----------+--------------------+\n",
      "|          3| 59|          60.55|    0|       0.0|[0.72964012466928...|\n",
      "|          7| 67|          41.93|    0|       0.0|[0.80236576190773...|\n",
      "|          9| 66|          39.67|    1|       0.0|[0.90835550218053...|\n",
      "|         14| 59|          55.40|    0|       0.0|[0.77361856841920...|\n",
      "|         20| 59|          22.66|    0|       0.0|[0.80007942897096...|\n",
      "|         24| 62|          48.09|    0|       0.0|[0.92954412174996...|\n",
      "|         30| 58|          28.61|    0|       0.0|[0.82260937249839...|\n",
      "|         36| 61|          26.65|    0|       0.0|[0.83465681163558...|\n",
      "|         46| 58|          62.18|    0|       0.0|[0.86369507753931...|\n",
      "|         47| 61|          58.44|    0|       0.0|[0.76228185052339...|\n",
      "+-----------+---+---------------+-----+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Pipeline intermediate transformations:\n",
      "Raw features -> Indexed features -> Encoded features -> Final features\n",
      "Pipeline AUC: 0.4912\n",
      "\n",
      "Pipeline prediction results:\n",
      "+-----------+---+---------------+-----+----------+--------------------+\n",
      "|customer_id|age|monthly_charges|churn|prediction|         probability|\n",
      "+-----------+---+---------------+-----+----------+--------------------+\n",
      "|          3| 59|          60.55|    0|       0.0|[0.72964012466928...|\n",
      "|          7| 67|          41.93|    0|       0.0|[0.80236576190773...|\n",
      "|          9| 66|          39.67|    1|       0.0|[0.90835550218053...|\n",
      "|         14| 59|          55.40|    0|       0.0|[0.77361856841920...|\n",
      "|         20| 59|          22.66|    0|       0.0|[0.80007942897096...|\n",
      "|         24| 62|          48.09|    0|       0.0|[0.92954412174996...|\n",
      "|         30| 58|          28.61|    0|       0.0|[0.82260937249839...|\n",
      "|         36| 61|          26.65|    0|       0.0|[0.83465681163558...|\n",
      "|         46| 58|          62.18|    0|       0.0|[0.86369507753931...|\n",
      "|         47| 61|          58.44|    0|       0.0|[0.76228185052339...|\n",
      "+-----------+---+---------------+-----+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Pipeline intermediate transformations:\n",
      "Raw features -> Indexed features -> Encoded features -> Final features\n",
      "+---+---------------+----------------------------+\n",
      "|age|monthly_charges|features                    |\n",
      "+---+---------------+----------------------------+\n",
      "|59 |60.55          |[59.0,16.0,60.55,968.8,6.0] |\n",
      "|67 |41.93          |[67.0,50.0,41.93,2096.5,7.0]|\n",
      "|66 |39.67          |[66.0,21.0,39.67,833.07,7.0]|\n",
      "+---+---------------+----------------------------+\n",
      "\n",
      "\n",
      "ML Pipeline construction complete!\n",
      "+---+---------------+----------------------------+\n",
      "|age|monthly_charges|features                    |\n",
      "+---+---------------+----------------------------+\n",
      "|59 |60.55          |[59.0,16.0,60.55,968.8,6.0] |\n",
      "|67 |41.93          |[67.0,50.0,41.93,2096.5,7.0]|\n",
      "|66 |39.67          |[66.0,21.0,39.67,833.07,7.0]|\n",
      "+---+---------------+----------------------------+\n",
      "\n",
      "\n",
      "ML Pipeline construction complete!\n"
     ]
    }
   ],
   "source": [
    "# Section 4.1: ML Pipelines\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "print(\"ML Pipeline Construction\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Start with existing processed data for pipeline demonstration\n",
    "print(\"Building end-to-end ML pipeline for churn prediction...\")\n",
    "\n",
    "# Use a simpler pipeline with existing features\n",
    "final_assembler_simple = VectorAssembler(\n",
    "    inputCols=[\"age\", \"tenure_months\", \"monthly_charges\", \"total_charges\", \"num_services\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Stage: Machine Learning Algorithm\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"churn\",\n",
    "    numTrees=20,\n",
    "    maxDepth=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Create Pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    final_assembler_simple,\n",
    "    rf_classifier\n",
    "])\n",
    "\n",
    "print(\"Pipeline stages:\")\n",
    "for i, stage in enumerate(pipeline.getStages()):\n",
    "    print(f\"  {i+1}. {type(stage).__name__}\")\n",
    "\n",
    "# Prepare data (use existing customer DataFrame)\n",
    "pipeline_train, pipeline_test = customer_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"\\nTraining set: {pipeline_train.count()} records\")\n",
    "print(f\"Test set: {pipeline_test.count()} records\")\n",
    "\n",
    "# Train Pipeline\n",
    "print(\"\\nTraining complete pipeline...\")\n",
    "start_time = time.time()\n",
    "\n",
    "pipeline_model = pipeline.fit(pipeline_train)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Pipeline training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Make Predictions\n",
    "print(\"\\nMaking predictions...\")\n",
    "pipeline_predictions = pipeline_model.transform(pipeline_test)\n",
    "\n",
    "# Evaluate Pipeline Performance\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"churn\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "auc = evaluator.evaluate(pipeline_predictions)\n",
    "print(f\"Pipeline AUC: {auc:.4f}\")\n",
    "\n",
    "# Show pipeline results\n",
    "print(\"\\nPipeline prediction results:\")\n",
    "pipeline_predictions.select(\n",
    "    \"customer_id\", \"age\", \"monthly_charges\", \n",
    "    \"churn\", \"prediction\", \"probability\"\n",
    ").show(10)\n",
    "\n",
    "# Inspect intermediate transformations\n",
    "print(\"\\nPipeline intermediate transformations:\")\n",
    "print(\"Raw features -> Indexed features -> Encoded features -> Final features\")\n",
    "\n",
    "# Show feature transformation stages\n",
    "sample_transformed = pipeline_model.transform(pipeline_test.limit(3))\n",
    "sample_transformed.select(\n",
    "    \"age\", \"monthly_charges\", \"features\"\n",
    ").show(3, truncate=False)\n",
    "\n",
    "print(\"\\nML Pipeline construction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0717699f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning and Model Selection\n",
      "==================================================\n",
      "Parameter grid size: 18 combinations\n",
      "Parameter combinations:\n",
      "  1. regParam=0.01, elasticNetParam=0.0, maxIter=10\n",
      "  2. regParam=0.01, elasticNetParam=0.0, maxIter=20\n",
      "  3. regParam=0.01, elasticNetParam=0.5, maxIter=10\n",
      "  4. regParam=0.01, elasticNetParam=0.5, maxIter=20\n",
      "  5. regParam=0.01, elasticNetParam=1.0, maxIter=10\n",
      "  6. regParam=0.01, elasticNetParam=1.0, maxIter=20\n",
      "  ... and 12 more combinations\n",
      "\n",
      "Running 3-fold cross validation...\n",
      "This may take a few minutes...\n",
      "Cross validation completed in 30.22 seconds\n",
      "Best model AUC: 0.4992\n",
      "\n",
      "Best hyperparameters:\n",
      "  regularization parameter: 0.01\n",
      "  elastic net parameter: 1.0\n",
      "  max iterations: 20\n",
      "\n",
      "Cross validation metrics:\n",
      "Top 5 parameter combinations:\n",
      "Cross validation completed in 30.22 seconds\n",
      "Best model AUC: 0.4992\n",
      "\n",
      "Best hyperparameters:\n",
      "  regularization parameter: 0.01\n",
      "  elastic net parameter: 1.0\n",
      "  max iterations: 20\n",
      "\n",
      "Cross validation metrics:\n",
      "Top 5 parameter combinations:\n",
      "+------+---------------+-------+--------+\n",
      "|CV_AUC|elasticNetParam|maxIter|regParam|\n",
      "+------+---------------+-------+--------+\n",
      "|0.5685|            1.0|     10|    0.01|\n",
      "|0.5685|            1.0|     20|    0.01|\n",
      "|0.5674|            0.5|     10|    0.01|\n",
      "|0.5674|            0.5|     20|    0.01|\n",
      "| 0.567|            0.0|     20|    0.01|\n",
      "+------+---------------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Train-Validation Split Alternative:\n",
      "----------------------------------------\n",
      "Running train-validation split...\n",
      "+------+---------------+-------+--------+\n",
      "|CV_AUC|elasticNetParam|maxIter|regParam|\n",
      "+------+---------------+-------+--------+\n",
      "|0.5685|            1.0|     10|    0.01|\n",
      "|0.5685|            1.0|     20|    0.01|\n",
      "|0.5674|            0.5|     10|    0.01|\n",
      "|0.5674|            0.5|     20|    0.01|\n",
      "| 0.567|            0.0|     20|    0.01|\n",
      "+------+---------------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Train-Validation Split Alternative:\n",
      "----------------------------------------\n",
      "Running train-validation split...\n",
      "Train-validation split completed in 5.19 seconds\n",
      "Train-validation split completed in 5.19 seconds\n",
      "Train-validation split AUC: 0.4992\n",
      "\n",
      "============================================================\n",
      "HYPERPARAMETER TUNING COMPARISON\n",
      "============================================================\n",
      "+------+-------------------------+----------------------+-------------+\n",
      "|AUC   |Method                   |Parameter Combinations|Training Time|\n",
      "+------+-------------------------+----------------------+-------------+\n",
      "|0.4992|Cross Validation (3-fold)|18                    |30.22        |\n",
      "|0.4992|Train-Validation Split   |6                     |5.19         |\n",
      "+------+-------------------------+----------------------+-------------+\n",
      "\n",
      "\n",
      "Recommendations:\n",
      "- Use Cross Validation for robust model selection\n",
      "- Use Train-Validation Split for faster hyperparameter tuning\n",
      "- Cross Validation provides more reliable performance estimates\n",
      "- Train-Validation Split is 5.8x faster\n",
      "\n",
      "Hyperparameter tuning complete!\n",
      "Train-validation split AUC: 0.4992\n",
      "\n",
      "============================================================\n",
      "HYPERPARAMETER TUNING COMPARISON\n",
      "============================================================\n",
      "+------+-------------------------+----------------------+-------------+\n",
      "|AUC   |Method                   |Parameter Combinations|Training Time|\n",
      "+------+-------------------------+----------------------+-------------+\n",
      "|0.4992|Cross Validation (3-fold)|18                    |30.22        |\n",
      "|0.4992|Train-Validation Split   |6                     |5.19         |\n",
      "+------+-------------------------+----------------------+-------------+\n",
      "\n",
      "\n",
      "Recommendations:\n",
      "- Use Cross Validation for robust model selection\n",
      "- Use Train-Validation Split for faster hyperparameter tuning\n",
      "- Cross Validation provides more reliable performance estimates\n",
      "- Train-Validation Split is 5.8x faster\n",
      "\n",
      "Hyperparameter tuning complete!\n"
     ]
    }
   ],
   "source": [
    "# Section 4.2: Hyperparameter Tuning and Cross Validation\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import functions as F\n",
    "import builtins  # Add this to ensure we use Python's built-in functions\n",
    "\n",
    "print(\"Hyperparameter Tuning and Model Selection\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a simpler pipeline for tuning demonstration\n",
    "tuning_pipeline = Pipeline(stages=[\n",
    "    final_assembler_simple,\n",
    "    LogisticRegression(featuresCol=\"features\", labelCol=\"churn\")\n",
    "])\n",
    "\n",
    "# Create Parameter Grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(tuning_pipeline.getStages()[-1].regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(tuning_pipeline.getStages()[-1].elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .addGrid(tuning_pipeline.getStages()[-1].maxIter, [10, 20]) \\\n",
    "    .build()\n",
    "\n",
    "print(f\"Parameter grid size: {builtins.len(param_grid)} combinations\")\n",
    "print(\"Parameter combinations:\")\n",
    "for i, params in enumerate(param_grid[:6]):  # Show first 6 combinations\n",
    "    print(f\"  {i+1}. regParam={params[tuning_pipeline.getStages()[-1].regParam]}, \"\n",
    "          f\"elasticNetParam={params[tuning_pipeline.getStages()[-1].elasticNetParam]}, \"\n",
    "          f\"maxIter={params[tuning_pipeline.getStages()[-1].maxIter]}\")\n",
    "if builtins.len(param_grid) > 6:\n",
    "    print(f\"  ... and {builtins.len(param_grid) - 6} more combinations\")\n",
    "\n",
    "# Cross Validation Setup\n",
    "cv_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"churn\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "# Cross Validator\n",
    "cv = CrossValidator(\n",
    "    estimator=tuning_pipeline,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=cv_evaluator,\n",
    "    numFolds=3,  # 3-fold cross validation\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\nRunning {cv.getNumFolds()}-fold cross validation...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# Train with Cross Validation\n",
    "start_time = time.time()\n",
    "cv_model = cv.fit(pipeline_train)\n",
    "cv_training_time = time.time() - start_time\n",
    "\n",
    "print(f\"Cross validation completed in {cv_training_time:.2f} seconds\")\n",
    "\n",
    "# Get best model and its performance\n",
    "best_model = cv_model.bestModel\n",
    "cv_predictions = best_model.transform(pipeline_test)\n",
    "cv_auc = cv_evaluator.evaluate(cv_predictions)\n",
    "\n",
    "print(f\"Best model AUC: {cv_auc:.4f}\")\n",
    "\n",
    "# Extract best parameters\n",
    "best_lr_stage = best_model.stages[-1]  # Last stage is LogisticRegression\n",
    "print(f\"\\nBest hyperparameters:\")\n",
    "print(f\"  regularization parameter: {best_lr_stage.getRegParam()}\")\n",
    "print(f\"  elastic net parameter: {best_lr_stage.getElasticNetParam()}\")\n",
    "print(f\"  max iterations: {best_lr_stage.getMaxIter()}\")\n",
    "\n",
    "# Show cross validation metrics for all parameter combinations\n",
    "print(f\"\\nCross validation metrics:\")\n",
    "cv_metrics = cv_model.avgMetrics\n",
    "param_performance = []\n",
    "\n",
    "for i, (params, metric) in enumerate(zip(param_grid, cv_metrics)):\n",
    "    lr_stage = tuning_pipeline.getStages()[-1]\n",
    "    param_performance.append({\n",
    "        \"regParam\": float(params[lr_stage.regParam]),\n",
    "        \"elasticNetParam\": float(params[lr_stage.elasticNetParam]),\n",
    "        \"maxIter\": int(params[lr_stage.maxIter]),\n",
    "        \"CV_AUC\": float(builtins.round(metric, 4))\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame and show top results\n",
    "param_df = spark.createDataFrame(param_performance)\n",
    "print(\"Top 5 parameter combinations:\")\n",
    "param_df.orderBy(F.desc(\"CV_AUC\")).show(5)\n",
    "\n",
    "# Train-Validation Split Alternative (faster than CV)\n",
    "print(f\"\\nTrain-Validation Split Alternative:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Train-Validation Split (faster alternative to cross validation)\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=tuning_pipeline,\n",
    "    estimatorParamMaps=param_grid[:6],  # Use subset for faster execution\n",
    "    evaluator=cv_evaluator,\n",
    "    trainRatio=0.8,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Running train-validation split...\")\n",
    "start_time = time.time()\n",
    "tvs_model = tvs.fit(pipeline_train)\n",
    "tvs_training_time = time.time() - start_time\n",
    "\n",
    "print(f\"Train-validation split completed in {tvs_training_time:.2f} seconds\")\n",
    "\n",
    "# Compare performance\n",
    "tvs_predictions = tvs_model.bestModel.transform(pipeline_test)\n",
    "tvs_auc = cv_evaluator.evaluate(tvs_predictions)\n",
    "\n",
    "print(f\"Train-validation split AUC: {tvs_auc:.4f}\")\n",
    "\n",
    "# Performance comparison\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"HYPERPARAMETER TUNING COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison_results = [\n",
    "    {\n",
    "        \"Method\": \"Cross Validation (3-fold)\",\n",
    "        \"AUC\": float(builtins.round(cv_auc, 4)),\n",
    "        \"Training Time\": float(builtins.round(cv_training_time, 2)),\n",
    "        \"Parameter Combinations\": builtins.len(param_grid)\n",
    "    },\n",
    "    {\n",
    "        \"Method\": \"Train-Validation Split\",\n",
    "        \"AUC\": float(builtins.round(tvs_auc, 4)),\n",
    "        \"Training Time\": float(builtins.round(tvs_training_time, 2)),\n",
    "        \"Parameter Combinations\": 6\n",
    "    }\n",
    "]\n",
    "\n",
    "comparison_df = spark.createDataFrame(comparison_results)\n",
    "comparison_df.show(truncate=False)\n",
    "\n",
    "print(f\"\\nRecommendations:\")\n",
    "print(f\"- Use Cross Validation for robust model selection\")\n",
    "print(f\"- Use Train-Validation Split for faster hyperparameter tuning\")\n",
    "print(f\"- Cross Validation provides more reliable performance estimates\")\n",
    "print(f\"- Train-Validation Split is {cv_training_time/tvs_training_time:.1f}x faster\")\n",
    "\n",
    "print(\"\\nHyperparameter tuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf795802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Persistence and Deployment\n",
      "==================================================\n",
      "Model storage directory: /var/folders/kv/9k05f6hn2lx0dq4ylk1spdqc0000gn/T/tmp7i16z5li\n",
      "\n",
      "Saving trained model...\n",
      "Pipeline model saved to: /var/folders/kv/9k05f6hn2lx0dq4ylk1spdqc0000gn/T/tmp7i16z5li/churn_pipeline\n",
      "Model metadata:\n",
      "  model_type: ChurnPredictionPipeline\n",
      "  algorithm: LogisticRegression\n",
      "  training_records: 8046\n",
      "  test_auc: 0.49916861786962585\n",
      "  best_regParam: 0.01\n",
      "  best_elasticNetParam: 1.0\n",
      "  best_maxIter: 20\n",
      "  cv_folds: 3\n",
      "  features: ['age', 'monthly_charges', 'total_charges', 'contract_length', 'support_calls', 'payment_delay', 'region', 'subscription_type']\n",
      "\n",
      "Loading saved model...\n",
      "Pipeline model saved to: /var/folders/kv/9k05f6hn2lx0dq4ylk1spdqc0000gn/T/tmp7i16z5li/churn_pipeline\n",
      "Model metadata:\n",
      "  model_type: ChurnPredictionPipeline\n",
      "  algorithm: LogisticRegression\n",
      "  training_records: 8046\n",
      "  test_auc: 0.49916861786962585\n",
      "  best_regParam: 0.01\n",
      "  best_elasticNetParam: 1.0\n",
      "  best_maxIter: 20\n",
      "  cv_folds: 3\n",
      "  features: ['age', 'monthly_charges', 'total_charges', 'contract_length', 'support_calls', 'payment_delay', 'region', 'subscription_type']\n",
      "\n",
      "Loading saved model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-08-25 23:09:23.707\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `region` cannot be resolved. Did you mean one of the following? [`age`, `churn`, `prediction`, `features`, `rawPrediction`]. SQLSTATE: 42703\", \"context\": {\"file\": \"jdk.internal.reflect.GeneratedMethodAccessor49.invoke(Unknown Source)\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o47507.select.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `region` cannot be resolved. Did you mean one of the following? [`age`, `churn`, `prediction`, `features`, `rawPrediction`]. SQLSTATE: 42703;\\n'Project [customer_id#26330L, age#26331, 'region, 'churn_label, prediction#279316, probability#279309]\\n+- Project [customer_id#26330L, age#26331, tenure_months#26332, monthly_charges#26333, total_charges#26334, num_services#26335, contract_type#26336, payment_method#26337, internet_service#26338, tech_support#26339, online_backup#26340, churn#26341, features#279302, rawPrediction#279305, probability#279309, UDF(rawPrediction#279305) AS prediction#279316]\\n   +- Project [customer_id#26330L, age#26331, tenure_months#26332, monthly_charges#26333, total_charges#26334, num_services#26335, contract_type#26336, payment_method#26337, internet_service#26338, tech_support#26339, online_backup#26340, churn#26341, features#279302, rawPrediction#279305, UDF(rawPrediction#279305) AS probability#279309]\\n      +- Project [customer_id#26330L, age#26331, tenure_months#26332, monthly_charges#26333, total_charges#26334, num_services#26335, contract_type#26336, payment_method#26337, internet_service#26338, tech_support#26339, online_backup#26340, churn#26341, features#279302, UDF(features#279302) AS rawPrediction#279305]\\n         +- Project [customer_id#26330L, age#26331, tenure_months#26332, monthly_charges#26333, total_charges#26334, num_services#26335, contract_type#26336, payment_method#26337, internet_service#26338, tech_support#26339, online_backup#26340, churn#26341, UDF(struct(age_double_VectorAssembler_2b46e796cbb9, cast(age#26331 as double), tenure_months_double_VectorAssembler_2b46e796cbb9, cast(tenure_months#26332 as double), monthly_charges_double_VectorAssembler_2b46e796cbb9, cast(monthly_charges#26333 as double), total_charges_double_VectorAssembler_2b46e796cbb9, cast(total_charges#26334 as double), num_services_double_VectorAssembler_2b46e796cbb9, cast(num_services#26335 as double))) AS features#279302]\\n            +- GlobalLimit 10\\n               +- LocalLimit 10\\n                  +- Sample 0.8, 1.0, false, 42\\n                     +- Sort [customer_id#26330L ASC NULLS FIRST, age#26331 ASC NULLS FIRST, tenure_months#26332 ASC NULLS FIRST, monthly_charges#26333 ASC NULLS FIRST, total_charges#26334 ASC NULLS FIRST, num_services#26335 ASC NULLS FIRST, contract_type#26336 ASC NULLS FIRST, payment_method#26337 ASC NULLS FIRST, internet_service#26338 ASC NULLS FIRST, tech_support#26339 ASC NULLS FIRST, online_backup#26340 ASC NULLS FIRST, churn#26341 ASC NULLS FIRST], false\\n                        +- Project [customer_id#26330L, age#26331, tenure_months#26332, monthly_charges#26333, total_charges#26334, num_services#26335, contract_type#26336, payment_method#26337, internet_service#26338, tech_support#26339, online_backup#26340, CASE WHEN ((((contract_type#26336 = Month-to-month) AND (monthly_charges#26333 > cast(cast(70 as decimal(2,0)) as decimal(8,2)))) AND (tenure_months#26332 < 12)) AND (tech_support#26339 = No)) THEN 1 WHEN (((age#26331 < 25) AND (monthly_charges#26333 > cast(cast(60 as decimal(2,0)) as decimal(8,2)))) AND (payment_method#26337 = Electronic check)) THEN 1 WHEN (rand(55) < 0.15) THEN 1 ELSE 0 END AS churn#26341]\\n                           +- Project [customer_id#26330L, age#26331, tenure_months#26332, monthly_charges#26333, total_charges#26334, num_services#26335, contract_type#26336, payment_method#26337, internet_service#26338, tech_support#26339, CASE WHEN (rand(54) < 0.5) THEN Yes ELSE No END AS online_backup#26340]\\n                              +- Project [customer_id#26330L, age#26331, tenure_months#26332, monthly_charges#26333, total_charges#26334, num_services#26335, contract_type#26336, payment_method#26337, internet_service#26338, CASE WHEN (rand(53) < 0.6) THEN Yes ELSE No END AS tech_support#26339]\\n                                 +- Project [customer_id#26330L, age#26331, tenure_months#26332, monthly_charges#26333, total_charges#26334, num_services#26335, contract_type#26336, payment_method#26337, CASE WHEN (rand(51) < 0.4) THEN DSL WHEN (rand(52) < 0.7) THEN Fiber optic ELSE No END AS internet_service#26338]\\n                                    +- Project [customer_id#26330L, age#26331, tenure_months#26332, monthly_charges#26333, total_charges#26334, num_services#26335, contract_type#26336, CASE WHEN (rand(48) < 0.25) THEN Electronic check WHEN (rand(49) < 0.5) THEN Mailed check WHEN (rand(50) < 0.75) THEN Bank transfer ELSE Credit card END AS payment_method#26337]\\n                                       +- Project [customer_id#26330L, age#26331, tenure_months#26332, monthly_charges#26333, total_charges#26334, num_services#26335, CASE WHEN (rand(46) < 0.3) THEN Month-to-month WHEN (rand(47) < 0.6) THEN One year ELSE Two year END AS contract_type#26336]\\n                                          +- Project [customer_id#26330L, age#26331, tenure_months#26332, monthly_charges#26333, total_charges#26334, cast(((rand(45) * cast(8 as double)) + cast(1 as double)) as int) AS num_services#26335]\\n                                             +- Project [customer_id#26330L, age#26331, tenure_months#26332, monthly_charges#26333, (monthly_charges#26333 * cast(tenure_months#26332 as decimal(10,0))) AS total_charges#26334]\\n                                                +- Project [customer_id#26330L, age#26331, tenure_months#26332, cast(((rand(44) * cast(80 as double)) + cast(20 as double)) as decimal(8,2)) AS monthly_charges#26333]\\n                                                   +- Project [customer_id#26330L, age#26331, cast(((rand(43) * cast(60 as double)) + cast(1 as double)) as int) AS tenure_months#26332]\\n                                                      +- Project [customer_id#26330L, cast(((rand(42) * cast(50 as double)) + cast(18 as double)) as int) AS age#26331]\\n                                                         +- Project [id#26329L AS customer_id#26330L]\\n                                                            +- Range (1, 10001, step=1, splits=Some(8))\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:249)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:249)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2263)\\n\\tat org.apache.spark.sql.classic.Dataset.select(Dataset.scala:894)\\n\\tat jdk.internal.reflect.GeneratedMethodAccessor163.invoke(Unknown Source)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:249)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:249)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 19 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/opt/anaconda3/envs/pyspark_env/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/opt/anaconda3/envs/pyspark_env/lib/python3.9/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "Testing loaded model...\n",
      "Loaded model predictions:\n",
      "Error loading model: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `region` cannot be resolved. Did you mean one of the following? [`age`, `churn`, `prediction`, `features`, `rawPrediction`]. SQLSTATE: 42703;\n",
      "'Project [customer_id#26330L, age#26331, 'region, 'churn_label, prediction#279316, probability#279309]\n",
      "+- Project [customer_id#26330L, age#26331, tenure_months#26332, monthly_charges#26333, total_charges#26334, num_services#26335, contract_type#26336, payment_method#26337, internet_service#26338, tech_support#26339, online_backup#26340, churn#26341, features#279302, rawPrediction#279305, probability#279309, UDF(rawPrediction#279305) AS prediction#279316]\n",
      "   +- Project [customer_id#26330L, age#26331, tenure_months#26332, monthly_charges#26333, total_charges#26334, num_services#26335, contract_type#26336, payment_method#26337, internet_service#26338, tech_support#26339, online_backup#26340, churn#26341, features#279302, rawPrediction#279305, UDF(rawPrediction#279305) AS probability#279309]\n",
      "      +- Project [customer_id#26330L, age#26331, tenure_months#26332, monthly_charges#26333, total_charges#26334, num_services#26335, contract_type#26336, payment_method#26337, internet_service#26338, tech_support#26339, online_backup#26340, churn#26341, features#279302, UDF(features#279302) AS rawPrediction#279305]\n",
      "         +- Project [customer_id#26330L, age#26331, tenure_months#26332, monthly_charges#26333, total_charges#26334, num_services#26335, contract_type#26336, payment_method#26337, internet_service#26338, tech_support#26339, online_backup#26340, churn#26341, UDF(struct(age_double_VectorAssembler_2b46e796cbb9, cast(age#26331 as double), tenure_months_double_VectorAssembler_2b46e796cbb9, cast(tenure_months#26332 as double), monthly_charges_double_VectorAssembler_2b46e796cbb9, cast(monthly_charges#26333 as double), total_charges_double_VectorAssembler_2b46e796cbb9, cast(total_charges#26334 as double), num_services_double_VectorAssembler_2b46e796cbb9, cast(num_services#26335 as double))) AS features#279302]\n",
      "            +- GlobalLimit 10\n",
      "               +- LocalLimit 10\n",
      "                  +- Sample 0.8, 1.0, false, 42\n",
      "                     +- Sort [customer_id#26330L ASC NULLS FIRST, age#26331 ASC NULLS FIRST, tenure_months#26332 ASC NULLS FIRST, monthly_charges#26333 ASC NULLS FIRST, total_charges#26334 ASC NULLS FIRST, num_services#26335 ASC NULLS FIRST, contract_type#26336 ASC NULLS FIRST, payment_method#26337 ASC NULLS FIRST, internet_service#26338 ASC NULLS FIRST, tech_support#26339 ASC NULLS FIRST, online_backup#26340 ASC NULLS FIRST, churn#26341 ASC NULLS FIRST], false\n",
      "                        +- Project [customer_id#26330L, age#26331, tenure_months#26332, monthly_charges#26333, total_charges#26334, num_services#26335, contract_type#26336, payment_method#26337, internet_service#26338, tech_support#26339, online_backup#26340, CASE WHEN ((((contract_type#26336 = Month-to-month) AND (monthly_charges#26333 > cast(cast(70 as decimal(2,0)) as decimal(8,2)))) AND (tenure_months#26332 < 12)) AND (tech_support#26339 = No)) THEN 1 WHEN (((age#26331 < 25) AND (monthly_charges#26333 > cast(cast(60 as decimal(2,0)) as decimal(8,2)))) AND (payment_method#26337 = Electronic check)) THEN 1 WHEN (rand(55) < 0.15) THEN 1 ELSE 0 END AS churn#26341]\n",
      "                           +- Project [customer_id#26330L, age#26331, tenure_months#26332, monthly_charges#26333, total_charges#26334, num_services#26335, contract_type#26336, payment_method#26337, internet_service#26338, tech_support#26339, CASE WHEN (rand(54) < 0.5) THEN Yes ELSE No END AS online_backup#26340]\n",
      "                              +- Project [customer_id#26330L, age#26331, tenure_months#26332, monthly_charges#26333, total_charges#26334, num_services#26335, contract_type#26336, payment_method#26337, internet_service#26338, CASE WHEN (rand(53) < 0.6) THEN Yes ELSE No END AS tech_support#26339]\n",
      "                                 +- Project [customer_id#26330L, age#26331, tenure_months#26332, monthly_charges#26333, total_charges#26334, num_services#26335, contract_type#26336, payment_method#26337, CASE WHEN (rand(51) < 0.4) THEN DSL WHEN (rand(52) < 0.7) THEN Fiber optic ELSE No END AS internet_service#26338]\n",
      "                                    +- Project [customer_id#26330L, age#26331, tenure_months#26332, monthly_charges#26333, total_charges#26334, num_services#26335, contract_type#26336, CASE WHEN (rand(48) < 0.25) THEN Electronic check WHEN (rand(49) < 0.5) THEN Mailed check WHEN (rand(50) < 0.75) THEN Bank transfer ELSE Credit card END AS payment_method#26337]\n",
      "                                       +- Project [customer_id#26330L, age#26331, tenure_months#26332, monthly_charges#26333, total_charges#26334, num_services#26335, CASE WHEN (rand(46) < 0.3) THEN Month-to-month WHEN (rand(47) < 0.6) THEN One year ELSE Two year END AS contract_type#26336]\n",
      "                                          +- Project [customer_id#26330L, age#26331, tenure_months#26332, monthly_charges#26333, total_charges#26334, cast(((rand(45) * cast(8 as double)) + cast(1 as double)) as int) AS num_services#26335]\n",
      "                                             +- Project [customer_id#26330L, age#26331, tenure_months#26332, monthly_charges#26333, (monthly_charges#26333 * cast(tenure_months#26332 as decimal(10,0))) AS total_charges#26334]\n",
      "                                                +- Project [customer_id#26330L, age#26331, tenure_months#26332, cast(((rand(44) * cast(80 as double)) + cast(20 as double)) as decimal(8,2)) AS monthly_charges#26333]\n",
      "                                                   +- Project [customer_id#26330L, age#26331, cast(((rand(43) * cast(60 as double)) + cast(1 as double)) as int) AS tenure_months#26332]\n",
      "                                                      +- Project [customer_id#26330L, cast(((rand(42) * cast(50 as double)) + cast(18 as double)) as int) AS age#26331]\n",
      "                                                         +- Project [id#26329L AS customer_id#26330L]\n",
      "                                                            +- Range (1, 10001, step=1, splits=Some(8))\n",
      "\n",
      "\n",
      "Model Deployment Simulation:\n",
      "----------------------------------------\n",
      "Generating new customer data for scoring...\n",
      "New customer data:\n",
      "+-----------+---+-------------+---------------+-------------+------------+-----+\n",
      "|customer_id|age|tenure_months|monthly_charges|total_charges|num_services|churn|\n",
      "+-----------+---+-------------+---------------+-------------+------------+-----+\n",
      "|      90001| 28|           12|           85.0|       1020.0|           6|    0|\n",
      "|      90002| 45|           24|           45.0|        540.0|           3|    0|\n",
      "|      90003| 35|           18|           65.0|       1560.0|           8|    0|\n",
      "|      90004| 52|           30|           95.0|       1140.0|           7|    0|\n",
      "|      90005| 29|            6|           40.0|        480.0|           4|    0|\n",
      "+-----------+---+-------------+---------------+-------------+------------+-----+\n",
      "\n",
      "Scoring new customers...\n",
      "New customer churn predictions:\n",
      "+-----------+---+---------------+----------+\n",
      "|customer_id|age|monthly_charges|prediction|\n",
      "+-----------+---+---------------+----------+\n",
      "|      90001| 28|           85.0|       0.0|\n",
      "|      90002| 45|           45.0|       0.0|\n",
      "|      90003| 35|           65.0|       0.0|\n",
      "|      90004| 52|           95.0|       0.0|\n",
      "|      90005| 29|           40.0|       0.0|\n",
      "+-----------+---+---------------+----------+\n",
      "\n",
      "Customer risk categorization:\n",
      "+-----------+----------+----------+\n",
      "|customer_id|prediction|risk_level|\n",
      "+-----------+----------+----------+\n",
      "|      90001|       0.0|       Low|\n",
      "|      90002|       0.0|       Low|\n",
      "|      90003|       0.0|       Low|\n",
      "|      90004|       0.0|       Low|\n",
      "|      90005|       0.0|       Low|\n",
      "+-----------+----------+----------+\n",
      "\n",
      "Risk level distribution:\n",
      "+-----------+---+---------------+----------+\n",
      "|customer_id|age|monthly_charges|prediction|\n",
      "+-----------+---+---------------+----------+\n",
      "|      90001| 28|           85.0|       0.0|\n",
      "|      90002| 45|           45.0|       0.0|\n",
      "|      90003| 35|           65.0|       0.0|\n",
      "|      90004| 52|           95.0|       0.0|\n",
      "|      90005| 29|           40.0|       0.0|\n",
      "+-----------+---+---------------+----------+\n",
      "\n",
      "Customer risk categorization:\n",
      "+-----------+----------+----------+\n",
      "|customer_id|prediction|risk_level|\n",
      "+-----------+----------+----------+\n",
      "|      90001|       0.0|       Low|\n",
      "|      90002|       0.0|       Low|\n",
      "|      90003|       0.0|       Low|\n",
      "|      90004|       0.0|       Low|\n",
      "|      90005|       0.0|       Low|\n",
      "+-----------+----------+----------+\n",
      "\n",
      "Risk level distribution:\n",
      "+----------+-----+\n",
      "|risk_level|count|\n",
      "+----------+-----+\n",
      "|       Low|    5|\n",
      "+----------+-----+\n",
      "\n",
      "\n",
      "============================================================\n",
      "PRODUCTION DEPLOYMENT RECOMMENDATIONS\n",
      "============================================================\n",
      "\n",
      "Model Deployment Best Practices:\n",
      "\n",
      "1. MODEL VERSIONING:\n",
      "   - Use semantic versioning (e.g., 1.0.0, 1.1.0)\n",
      "   - Track model lineage and training data\n",
      "   - Maintain model registry with metadata\n",
      "\n",
      "2. MODEL MONITORING:\n",
      "   - Monitor prediction drift\n",
      "   - Track model performance metrics\n",
      "   - Set up alerts for degraded performance\n",
      "   - Log prediction requests and responses\n",
      "\n",
      "3. BATCH SCORING:\n",
      "   - Schedule regular batch predictions\n",
      "   - Use data partitioning for large datasets\n",
      "   - Implement checkpointing for fault tolerance\n",
      "\n",
      "4. REAL-TIME SCORING:\n",
      "   - Consider Spark Structured Streaming\n",
      "   - Implement low-latency prediction endpoints\n",
      "   - Use model caching for performance\n",
      "\n",
      "5. MODEL UPDATES:\n",
      "   - Implement A/B testing for new models\n",
      "   - Use canary deployments\n",
      "   - Maintain rollback capabilities\n",
      "   - Automate retraining pipelines\n",
      "\n",
      "6. SECURITY:\n",
      "   - Encrypt model files\n",
      "   - Implement access controls\n",
      "   - Audit model usage\n",
      "   - Protect sensitive features\n",
      "\n",
      "Temporary directory cleaned up: /var/folders/kv/9k05f6hn2lx0dq4ylk1spdqc0000gn/T/tmp7i16z5li\n",
      "\n",
      "Model persistence and deployment complete!\n",
      "+----------+-----+\n",
      "|risk_level|count|\n",
      "+----------+-----+\n",
      "|       Low|    5|\n",
      "+----------+-----+\n",
      "\n",
      "\n",
      "============================================================\n",
      "PRODUCTION DEPLOYMENT RECOMMENDATIONS\n",
      "============================================================\n",
      "\n",
      "Model Deployment Best Practices:\n",
      "\n",
      "1. MODEL VERSIONING:\n",
      "   - Use semantic versioning (e.g., 1.0.0, 1.1.0)\n",
      "   - Track model lineage and training data\n",
      "   - Maintain model registry with metadata\n",
      "\n",
      "2. MODEL MONITORING:\n",
      "   - Monitor prediction drift\n",
      "   - Track model performance metrics\n",
      "   - Set up alerts for degraded performance\n",
      "   - Log prediction requests and responses\n",
      "\n",
      "3. BATCH SCORING:\n",
      "   - Schedule regular batch predictions\n",
      "   - Use data partitioning for large datasets\n",
      "   - Implement checkpointing for fault tolerance\n",
      "\n",
      "4. REAL-TIME SCORING:\n",
      "   - Consider Spark Structured Streaming\n",
      "   - Implement low-latency prediction endpoints\n",
      "   - Use model caching for performance\n",
      "\n",
      "5. MODEL UPDATES:\n",
      "   - Implement A/B testing for new models\n",
      "   - Use canary deployments\n",
      "   - Maintain rollback capabilities\n",
      "   - Automate retraining pipelines\n",
      "\n",
      "6. SECURITY:\n",
      "   - Encrypt model files\n",
      "   - Implement access controls\n",
      "   - Audit model usage\n",
      "   - Protect sensitive features\n",
      "\n",
      "Temporary directory cleaned up: /var/folders/kv/9k05f6hn2lx0dq4ylk1spdqc0000gn/T/tmp7i16z5li\n",
      "\n",
      "Model persistence and deployment complete!\n"
     ]
    }
   ],
   "source": [
    "# Section 4.3: Model Persistence and Deployment\n",
    "\n",
    "import os\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "import tempfile\n",
    "\n",
    "print(\"Model Persistence and Deployment\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create temporary directory for model storage\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "model_path = os.path.join(temp_dir, \"best_churn_model\")\n",
    "pipeline_path = os.path.join(temp_dir, \"churn_pipeline\")\n",
    "\n",
    "print(f\"Model storage directory: {temp_dir}\")\n",
    "\n",
    "# Save the best cross-validated model\n",
    "print(\"\\nSaving trained model...\")\n",
    "try:\n",
    "    # Save the entire pipeline model\n",
    "    cv_model.bestModel.write().overwrite().save(pipeline_path)\n",
    "    print(f\"Pipeline model saved to: {pipeline_path}\")\n",
    "    \n",
    "    # Save model metadata\n",
    "    model_metadata = {\n",
    "        \"model_type\": \"ChurnPredictionPipeline\",\n",
    "        \"algorithm\": \"LogisticRegression\",\n",
    "        \"training_records\": pipeline_train.count(),\n",
    "        \"test_auc\": cv_auc,\n",
    "        \"best_regParam\": best_lr_stage.getRegParam(),\n",
    "        \"best_elasticNetParam\": best_lr_stage.getElasticNetParam(),\n",
    "        \"best_maxIter\": best_lr_stage.getMaxIter(),\n",
    "        \"cv_folds\": cv.getNumFolds(),\n",
    "        \"features\": [\"age\", \"monthly_charges\", \"total_charges\", \"contract_length\", \n",
    "                    \"support_calls\", \"payment_delay\", \"region\", \"subscription_type\"]\n",
    "    }\n",
    "    \n",
    "    print(\"Model metadata:\")\n",
    "    for key, value in model_metadata.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {e}\")\n",
    "\n",
    "# Load the saved model\n",
    "print(f\"\\nLoading saved model...\")\n",
    "try:\n",
    "    # Load the pipeline model\n",
    "    loaded_model = PipelineModel.load(pipeline_path)\n",
    "    print(\"Model loaded successfully\")\n",
    "    \n",
    "    # Verify loaded model works\n",
    "    print(\"Testing loaded model...\")\n",
    "    loaded_predictions = loaded_model.transform(pipeline_test.limit(10))\n",
    "    \n",
    "    print(\"Loaded model predictions:\")\n",
    "    loaded_predictions.select(\n",
    "        \"customer_id\", \"age\", \"region\", \"churn_label\", \"prediction\", \"probability\"\n",
    "    ).show(5)\n",
    "    \n",
    "    # Verify predictions match original model\n",
    "    original_sample = cv_predictions.limit(10).collect()\n",
    "    loaded_sample = loaded_predictions.collect()\n",
    "    \n",
    "    predictions_match = all(\n",
    "        orig.prediction == loaded.prediction \n",
    "        for orig, loaded in zip(original_sample, loaded_sample)\n",
    "    )\n",
    "    \n",
    "    print(f\"Predictions match original model: {predictions_match}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "\n",
    "# Model Deployment Simulation\n",
    "print(f\"\\nModel Deployment Simulation:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Simulate new customer data for scoring\n",
    "print(\"Generating new customer data for scoring...\")\n",
    "\n",
    "new_customers = spark.createDataFrame([\n",
    "    (90001, 28, 12, 85.0, 1020.0, 6),\n",
    "    (90002, 45, 24, 45.0, 540.0, 3),\n",
    "    (90003, 35, 18, 65.0, 1560.0, 8),\n",
    "    (90004, 52, 30, 95.0, 1140.0, 7),\n",
    "    (90005, 29, 6, 40.0, 480.0, 4)\n",
    "], [\"customer_id\", \"age\", \"tenure_months\", \"monthly_charges\", \"total_charges\", \"num_services\"])\n",
    "\n",
    "# Add churn column (unknown for new customers, set to 0 for pipeline compatibility)\n",
    "new_customers = new_customers.withColumn(\"churn\", F.lit(0))\n",
    "\n",
    "print(\"New customer data:\")\n",
    "new_customers.show()\n",
    "\n",
    "# Score new customers\n",
    "print(\"Scoring new customers...\")\n",
    "new_predictions = loaded_model.transform(new_customers)\n",
    "\n",
    "print(\"New customer churn predictions:\")\n",
    "new_predictions.select(\n",
    "    \"customer_id\", \"age\", \"monthly_charges\", \n",
    "    \"prediction\"\n",
    ").show()\n",
    "\n",
    "# Categorize risk levels based on prediction\n",
    "risk_categorized = new_predictions.withColumn(\n",
    "    \"risk_level\",\n",
    "    F.when(F.col(\"prediction\") == 1, \"High\").otherwise(\"Low\")\n",
    ")\n",
    "\n",
    "print(\"Customer risk categorization:\")\n",
    "risk_categorized.select(\n",
    "    \"customer_id\", \"prediction\", \"risk_level\"\n",
    ").show()\n",
    "\n",
    "# Risk summary\n",
    "risk_summary = risk_categorized.groupBy(\"risk_level\").count()\n",
    "print(\"Risk level distribution:\")\n",
    "risk_summary.show()\n",
    "\n",
    "# Production deployment recommendations\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"PRODUCTION DEPLOYMENT RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "Model Deployment Best Practices:\n",
    "\n",
    "1. MODEL VERSIONING:\n",
    "   - Use semantic versioning (e.g., 1.0.0, 1.1.0)\n",
    "   - Track model lineage and training data\n",
    "   - Maintain model registry with metadata\n",
    "\n",
    "2. MODEL MONITORING:\n",
    "   - Monitor prediction drift\n",
    "   - Track model performance metrics\n",
    "   - Set up alerts for degraded performance\n",
    "   - Log prediction requests and responses\n",
    "\n",
    "3. BATCH SCORING:\n",
    "   - Schedule regular batch predictions\n",
    "   - Use data partitioning for large datasets\n",
    "   - Implement checkpointing for fault tolerance\n",
    "\n",
    "4. REAL-TIME SCORING:\n",
    "   - Consider Spark Structured Streaming\n",
    "   - Implement low-latency prediction endpoints\n",
    "   - Use model caching for performance\n",
    "\n",
    "5. MODEL UPDATES:\n",
    "   - Implement A/B testing for new models\n",
    "   - Use canary deployments\n",
    "   - Maintain rollback capabilities\n",
    "   - Automate retraining pipelines\n",
    "\n",
    "6. SECURITY:\n",
    "   - Encrypt model files\n",
    "   - Implement access controls\n",
    "   - Audit model usage\n",
    "   - Protect sensitive features\n",
    "\"\"\")\n",
    "\n",
    "# Cleanup temporary directory\n",
    "try:\n",
    "    import shutil\n",
    "    shutil.rmtree(temp_dir)\n",
    "    print(f\"Temporary directory cleaned up: {temp_dir}\")\n",
    "except:\n",
    "    print(f\"Manual cleanup required: {temp_dir}\")\n",
    "\n",
    "print(\"\\nModel persistence and deployment complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6f922c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 6 Summary: Machine Learning with MLlib\n",
    "\n",
    "## What We Accomplished\n",
    "\n",
    "### 🎯 **Learning Objectives Achieved**\n",
    "- ✅ **Feature Engineering**: Categorical encoding, numerical scaling, feature assembly\n",
    "- ✅ **Supervised Learning**: Classification and regression algorithms comparison\n",
    "- ✅ **Unsupervised Learning**: Clustering and dimensionality reduction techniques\n",
    "- ✅ **ML Pipelines**: End-to-end workflow automation\n",
    "- ✅ **Model Selection**: Cross-validation and hyperparameter tuning\n",
    "- ✅ **Model Deployment**: Persistence, loading, and production scoring\n",
    "\n",
    "### 📊 **Key Results**\n",
    "- **Classification Models**: Compared 4 algorithms with performance metrics\n",
    "- **Regression Models**: Evaluated RMSE, MAE, and R² across multiple algorithms\n",
    "- **Clustering Analysis**: Applied K-Means, Gaussian Mixture, and Bisecting K-Means\n",
    "- **Dimensionality Reduction**: PCA analysis and feature selection techniques\n",
    "- **Pipeline Automation**: Complete ML workflow from raw data to predictions\n",
    "- **Production Ready**: Model persistence and deployment strategies\n",
    "\n",
    "### 🔧 **Technical Skills Developed**\n",
    "- MLlib algorithm implementation and evaluation\n",
    "- Feature engineering pipeline construction\n",
    "- Cross-validation and hyperparameter optimization\n",
    "- Model persistence and deployment workflows\n",
    "- Production ML best practices\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### 🚀 **Module 7: Real-Time Streaming**\n",
    "- Structured Streaming fundamentals\n",
    "- Real-time data processing\n",
    "- Stream-batch integration\n",
    "- Stateful operations\n",
    "\n",
    "### 📈 **Module 8: Production Deployment**\n",
    "- Cluster management\n",
    "- Performance optimization\n",
    "- Monitoring and logging\n",
    "- Production best practices\n",
    "\n",
    "### 🔄 **Advanced Topics**\n",
    "- Graph processing with GraphFrames\n",
    "- Advanced SQL patterns\n",
    "- Custom ML algorithms\n",
    "- Integration with external systems\n",
    "\n",
    "---\n",
    "\n",
    "**🎉 Congratulations!** You've completed the comprehensive PySpark MLlib module. You now have the skills to build, evaluate, and deploy machine learning models at scale using Apache Spark.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
