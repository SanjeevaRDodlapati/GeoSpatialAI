{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb044982",
   "metadata": {},
   "source": [
    "# PySpark Comprehensive Tutorial - Module 2: Data Ingestion & I/O Operations\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "- Master **file format operations** (CSV, JSON, Parquet, Avro, Delta Lake)\n",
    "- Implement **database connectivity** with JDBC and cloud databases\n",
    "- Configure **cloud storage integration** (AWS S3, GCS, Azure Blob)\n",
    "- Understand **streaming data ingestion** patterns\n",
    "- Apply **schema management** and data validation techniques\n",
    "- Optimize **I/O performance** for large-scale data processing\n",
    "\n",
    "## üèó Module Focus\n",
    "**Building on Module 1 Foundation:**\n",
    "- Apply SparkSession configuration for I/O operations\n",
    "- Use DataFrame APIs for data ingestion and export\n",
    "- Implement production-ready data pipelines\n",
    "- Handle real-world data quality challenges\n",
    "\n",
    "**Real-World Applications:**\n",
    "- **ETL Pipelines**: Extract, Transform, Load workflows\n",
    "- **Data Lake Integration**: Multi-format data processing\n",
    "- **Analytics Preparation**: Optimized data formats for queries\n",
    "- **Cross-Platform Data Exchange**: Compatible data formats\n",
    "\n",
    "## üìã Prerequisites\n",
    "- ‚úÖ **Module 1 Complete**: Foundation & Setup knowledge\n",
    "- ‚úÖ **Environment**: `pyspark_env` with PySpark 4.0.0\n",
    "- ‚úÖ **Local Setup**: 6-core macOS optimization\n",
    "- ‚úÖ **Datasets**: < 10GB for local development\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fe4ea3",
   "metadata": {},
   "source": [
    "## 2.1 I/O Operations Overview\n",
    "\n",
    "### Data Sources & Formats\n",
    "PySpark provides unified APIs for reading and writing various data sources:\n",
    "\n",
    "**File Formats:**\n",
    "- **CSV**: Comma-separated values, universal compatibility\n",
    "- **JSON**: JavaScript Object Notation, semi-structured data\n",
    "- **Parquet**: Columnar format, optimized for analytics\n",
    "- **Avro**: Schema evolution, cross-language compatibility\n",
    "- **Delta Lake**: ACID transactions, versioning, time travel\n",
    "\n",
    "**Database Sources:**\n",
    "- **JDBC**: Relational databases (PostgreSQL, MySQL, SQL Server)\n",
    "- **NoSQL**: MongoDB, Cassandra, HBase\n",
    "- **Cloud Databases**: BigQuery, Redshift, Snowflake\n",
    "\n",
    "**Cloud Storage:**\n",
    "- **AWS S3**: Simple Storage Service\n",
    "- **Google Cloud Storage**: GCS buckets\n",
    "- **Azure Blob Storage**: Azure Data Lake\n",
    "\n",
    "### Key I/O Concepts\n",
    "1. **Schema Inference vs Explicit Schema**: Performance trade-offs\n",
    "2. **Partitioning**: Organize data for optimal query performance  \n",
    "3. **Compression**: Balance storage size vs processing speed\n",
    "4. **Error Handling**: Malformed data, missing files, connection issues\n",
    "5. **Performance Optimization**: Parallel I/O, caching strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2774286e",
   "metadata": {},
   "source": [
    "## 2.2 Environment Setup for I/O Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a05a7393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Module 2: Data Ingestion & I/O Setup\n",
      "==================================================\n",
      "Conda Environment: pyspark_env\n",
      "üìÅ Project Root: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial\n",
      "üìÅ Data Directory: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/data\n",
      "üìÅ Temp Directory: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/temp\n",
      "\n",
      "‚úÖ Environment ready for I/O operations!\n",
      "üéØ Ready to explore: CSV, JSON, Parquet, and more!\n",
      "\n",
      "üìã Directory Setup Complete:\n",
      "   ‚Ä¢ data_dir: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/data\n",
      "   ‚Ä¢ temp_dir: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/temp\n",
      "\n",
      "‚úÖ Environment ready for I/O operations!\n",
      "üéØ Ready to explore: CSV, JSON, Parquet, and more!\n",
      "\n",
      "üìã Directory Setup Complete:\n",
      "   ‚Ä¢ data_dir: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/data\n",
      "   ‚Ä¢ temp_dir: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/temp\n"
     ]
    }
   ],
   "source": [
    "# Environment verification and imports for Module 2: Data Ingestion & I/O\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üîç Module 2: Data Ingestion & I/O Setup\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check conda environment \n",
    "conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'Not activated')\n",
    "print(f\"Conda Environment: {conda_env}\")\n",
    "\n",
    "# Verify required directories exist\n",
    "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "data_dir = project_root / \"data\"\n",
    "temp_dir = project_root / \"temp\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "temp_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Project Root: {project_root}\")\n",
    "print(f\"üìÅ Data Directory: {data_dir}\")\n",
    "print(f\"üìÅ Temp Directory: {temp_dir}\")\n",
    "\n",
    "# Import PySpark components for I/O operations\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"\\n‚úÖ Environment ready for I/O operations!\")\n",
    "print(f\"üéØ Ready to explore: CSV, JSON, Parquet, and more!\")\n",
    "\n",
    "# Store paths for later use\n",
    "print(f\"\\nüìã Directory Setup Complete:\")\n",
    "print(f\"   ‚Ä¢ data_dir: {data_dir}\")\n",
    "print(f\"   ‚Ä¢ temp_dir: {temp_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f02f8d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Creating SparkSession for I/O Operations\n",
      "==================================================\n",
      "üÜï No existing SparkSession to stop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/25 19:32:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/25 19:32:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/25 19:32:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/08/25 19:32:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ SparkSession created successfully!\n",
      "üì± Application Name: PySpark-Tutorial-Module2-IO\n",
      "üî¢ Spark Version: 4.0.0\n",
      "üéØ Master: local[6]\n",
      "üíæ Driver Memory: 3g\n",
      "‚ö° Default Parallelism: 6\n",
      "\n",
      "üîß I/O Optimizations:\n",
      "   ‚Ä¢ Parquet Compression: snappy\n",
      "   ‚Ä¢ JSON Compression: gzip\n",
      "   ‚Ä¢ Max Partition Bytes: 128MB\n",
      "   ‚Ä¢ Arrow Optimization: true\n",
      "\n",
      "üåê Spark UI: http://192.168.12.128:4041\n",
      "\n",
      "üéØ Optimized for:\n",
      "   ‚Ä¢ File format I/O operations\n",
      "   ‚Ä¢ Multi-format data processing\n",
      "   ‚Ä¢ Compression and performance\n",
      "   ‚Ä¢ Local development with 6 cores\n",
      "üíæ Driver Memory: 3g\n",
      "‚ö° Default Parallelism: 6\n",
      "\n",
      "üîß I/O Optimizations:\n",
      "   ‚Ä¢ Parquet Compression: snappy\n",
      "   ‚Ä¢ JSON Compression: gzip\n",
      "   ‚Ä¢ Max Partition Bytes: 128MB\n",
      "   ‚Ä¢ Arrow Optimization: true\n",
      "\n",
      "üåê Spark UI: http://192.168.12.128:4041\n",
      "\n",
      "üéØ Optimized for:\n",
      "   ‚Ä¢ File format I/O operations\n",
      "   ‚Ä¢ Multi-format data processing\n",
      "   ‚Ä¢ Compression and performance\n",
      "   ‚Ä¢ Local development with 6 cores\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession optimized for I/O operations\n",
    "print(\"üöÄ Creating SparkSession for I/O Operations\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Stop any existing SparkSession\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"üßπ Stopped existing SparkSession\")\n",
    "except:\n",
    "    print(\"üÜï No existing SparkSession to stop\")\n",
    "\n",
    "# Configuration optimized for I/O operations and file processing\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark-Tutorial-Module2-IO\") \\\n",
    "    .master(\"local[6]\") \\\n",
    "    .config(\"spark.driver.memory\", \"3g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.minPartitionSize\", \"16MB\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\") \\\n",
    "    .config(\"spark.sql.files.openCostInBytes\", \"4MB\") \\\n",
    "    .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\") \\\n",
    "    .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "    .config(\"spark.sql.json.compression.codec\", \"gzip\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Get SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(\"\\n‚úÖ SparkSession created successfully!\")\n",
    "print(f\"üì± Application Name: {spark.sparkContext.appName}\")\n",
    "print(f\"üî¢ Spark Version: {spark.version}\")\n",
    "print(f\"üéØ Master: {spark.sparkContext.master}\")\n",
    "print(f\"üíæ Driver Memory: {spark.conf.get('spark.driver.memory')}\")\n",
    "print(f\"‚ö° Default Parallelism: {spark.sparkContext.defaultParallelism}\")\n",
    "\n",
    "# I/O specific configurations\n",
    "print(f\"\\nüîß I/O Optimizations:\")\n",
    "print(f\"   ‚Ä¢ Parquet Compression: {spark.conf.get('spark.sql.parquet.compression.codec')}\")\n",
    "print(f\"   ‚Ä¢ JSON Compression: {spark.conf.get('spark.sql.json.compression.codec')}\")\n",
    "print(f\"   ‚Ä¢ Max Partition Bytes: {spark.conf.get('spark.sql.files.maxPartitionBytes')}\")\n",
    "print(f\"   ‚Ä¢ Arrow Optimization: {spark.conf.get('spark.sql.execution.arrow.pyspark.enabled')}\")\n",
    "\n",
    "if spark.sparkContext.uiWebUrl:\n",
    "    print(f\"\\nüåê Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "print(f\"\\nüéØ Optimized for:\")\n",
    "print(f\"   ‚Ä¢ File format I/O operations\")\n",
    "print(f\"   ‚Ä¢ Multi-format data processing\")\n",
    "print(f\"   ‚Ä¢ Compression and performance\")\n",
    "print(f\"   ‚Ä¢ Local development with 6 cores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0df257",
   "metadata": {},
   "source": [
    "## 2.3 CSV File Operations\n",
    "\n",
    "CSV (Comma-Separated Values) is one of the most common data formats. PySpark provides excellent support for reading and writing CSV files with various options for handling headers, data types, delimiters, and schema inference.\n",
    "\n",
    "**Key CSV Concepts:**\n",
    "- **Schema Inference**: Automatically detect column types\n",
    "- **Custom Schema**: Define column types explicitly for better performance\n",
    "- **Headers**: Handle files with/without column headers\n",
    "- **Delimiters**: Support for different separators (comma, semicolon, tab, etc.)\n",
    "- **Null Values**: Custom null value representations\n",
    "- **Escape Characters**: Handle special characters and quotes\n",
    "- **Multiline**: Support for records spanning multiple lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93d613a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Creating Sample CSV Datasets\n",
      "========================================\n",
      "‚úÖ Created: employees.csv\n",
      "‚úÖ Created: sales_semicolon.csv\n",
      "‚úÖ Created: complex_data.csv\n",
      "\n",
      "üìä CSV Files Summary:\n",
      "   ‚Ä¢ employees.csv: 529 bytes, 11 lines\n",
      "   ‚Ä¢ sales_semicolon.csv: 486 bytes, 9 lines\n",
      "   ‚Ä¢ complex_data.csv: 295 bytes, 5 lines\n",
      "\n",
      "üìÇ Files location: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/data\n",
      "üéØ Ready for CSV operations!\n"
     ]
    }
   ],
   "source": [
    "# Create sample CSV datasets for demonstration\n",
    "print(\"üìÅ Creating Sample CSV Datasets\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Sample 1: Employee data with headers\n",
    "employee_csv_content = \"\"\"employee_id,name,department,salary,hire_date,is_active\n",
    "1001,John Doe,Engineering,85000,2022-01-15,true\n",
    "1002,Jane Smith,Marketing,72000,2021-03-22,true\n",
    "1003,Bob Johnson,Engineering,92000,2020-11-08,true\n",
    "1004,Alice Brown,Sales,68000,2023-02-14,true\n",
    "1005,Charlie Wilson,HR,75000,2021-09-05,true\n",
    "1006,Diana Davis,Engineering,88000,2022-07-12,false\n",
    "1007,Eve Miller,Marketing,69000,2023-01-30,true\n",
    "1008,Frank Garcia,Sales,71000,2020-12-03,true\n",
    "1009,Grace Lee,Engineering,95000,2019-08-17,true\n",
    "1010,Henry Taylor,HR,73000,2022-04-25,true\"\"\"\n",
    "\n",
    "# Sample 2: Sales data with different delimiter and null values\n",
    "sales_csv_content = \"\"\"product_id;product_name;category;price;quantity_sold;sale_date;discount\n",
    "P001;Laptop Pro;Electronics;1299.99;45;2024-01-15;0.1\n",
    "P002;Wireless Mouse;Electronics;29.99;120;;0.05\n",
    "P003;Office Chair;Furniture;299.99;30;2024-01-18;\n",
    "P004;Coffee Maker;Appliances;89.99;75;2024-01-20;0.15\n",
    "P005;Smartphone;Electronics;;200;2024-01-22;0.08\n",
    "P006;Desk Lamp;Furniture;45.99;60;2024-01-25;0.0\n",
    "P007;Tablet;Electronics;599.99;35;2024-01-28;0.12\n",
    "P008;Ergonomic Keyboard;Electronics;79.99;85;2024-01-30;0.07\"\"\"\n",
    "\n",
    "# Sample 3: Complex CSV with quotes and special characters\n",
    "complex_csv_lines = [\n",
    "    'id,description,tags,notes,created_at',\n",
    "    '1,\"Product with quotes\",\"tag1,tag2,tag3\",\"Note with commas\",\"2024-01-01\"',\n",
    "    '2,\"Simple product\",\"electronics\",\"Normal note\",\"2024-01-02\"',\n",
    "    '3,\"Product description\",\"tag1\",\"Multiline note\",\"2024-01-03\"',\n",
    "    '4,\"Product with commas\",\"home,garden\",\"Simple note\",\"2024-01-04\"'\n",
    "]\n",
    "complex_csv_content = '\\n'.join(complex_csv_lines)\n",
    "\n",
    "# Write CSV files\n",
    "csv_files = {\n",
    "    'employees.csv': employee_csv_content,\n",
    "    'sales_semicolon.csv': sales_csv_content,\n",
    "    'complex_data.csv': complex_csv_content\n",
    "}\n",
    "\n",
    "created_files = []\n",
    "for filename, content in csv_files.items():\n",
    "    file_path = data_dir / filename\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "    created_files.append(filename)\n",
    "    print(f\"‚úÖ Created: {filename}\")\n",
    "\n",
    "# Display file sizes and row counts\n",
    "print(f\"\\nüìä CSV Files Summary:\")\n",
    "for filename in created_files:\n",
    "    file_path = data_dir / filename\n",
    "    file_size = file_path.stat().st_size\n",
    "    with open(file_path, 'r') as f:\n",
    "        line_count = len(f.readlines())\n",
    "    print(f\"   ‚Ä¢ {filename}: {file_size} bytes, {line_count} lines\")\n",
    "\n",
    "print(f\"\\nüìÇ Files location: {data_dir}\")\n",
    "print(f\"üéØ Ready for CSV operations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfd760ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Reading CSV Files with Schema Inference\n",
      "=============================================\n",
      "üìÅ Reading: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/data/employees.csv\n",
      "\n",
      "‚úÖ Successfully loaded employee data\n",
      "\n",
      "‚úÖ Successfully loaded employee data\n",
      "üìä Rows: 10\n",
      "üìã Columns: 6\n",
      "\n",
      "üîç Inferred Schema:\n",
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- is_active: boolean (nullable = true)\n",
      "\n",
      "\n",
      "üìÑ Sample Data (first 5 rows):\n",
      "+-----------+--------------+-----------+------+----------+---------+\n",
      "|employee_id|name          |department |salary|hire_date |is_active|\n",
      "+-----------+--------------+-----------+------+----------+---------+\n",
      "|1001       |John Doe      |Engineering|85000 |2022-01-15|true     |\n",
      "|1002       |Jane Smith    |Marketing  |72000 |2021-03-22|true     |\n",
      "|1003       |Bob Johnson   |Engineering|92000 |2020-11-08|true     |\n",
      "|1004       |Alice Brown   |Sales      |68000 |2023-02-14|true     |\n",
      "|1005       |Charlie Wilson|HR         |75000 |2021-09-05|true     |\n",
      "+-----------+--------------+-----------+------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "üè∑Ô∏è  Column Data Types:\n",
      "   ‚Ä¢ employee_id: int\n",
      "   ‚Ä¢ name: string\n",
      "   ‚Ä¢ department: string\n",
      "   ‚Ä¢ salary: int\n",
      "   ‚Ä¢ hire_date: date\n",
      "   ‚Ä¢ is_active: boolean\n",
      "\n",
      "‚ö° Performance Note:\n",
      "   Schema inference requires reading the entire file\n",
      "   For better performance on large files, define schema explicitly\n",
      "   Current partitions: 1\n",
      "üìä Rows: 10\n",
      "üìã Columns: 6\n",
      "\n",
      "üîç Inferred Schema:\n",
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- is_active: boolean (nullable = true)\n",
      "\n",
      "\n",
      "üìÑ Sample Data (first 5 rows):\n",
      "+-----------+--------------+-----------+------+----------+---------+\n",
      "|employee_id|name          |department |salary|hire_date |is_active|\n",
      "+-----------+--------------+-----------+------+----------+---------+\n",
      "|1001       |John Doe      |Engineering|85000 |2022-01-15|true     |\n",
      "|1002       |Jane Smith    |Marketing  |72000 |2021-03-22|true     |\n",
      "|1003       |Bob Johnson   |Engineering|92000 |2020-11-08|true     |\n",
      "|1004       |Alice Brown   |Sales      |68000 |2023-02-14|true     |\n",
      "|1005       |Charlie Wilson|HR         |75000 |2021-09-05|true     |\n",
      "+-----------+--------------+-----------+------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "üè∑Ô∏è  Column Data Types:\n",
      "   ‚Ä¢ employee_id: int\n",
      "   ‚Ä¢ name: string\n",
      "   ‚Ä¢ department: string\n",
      "   ‚Ä¢ salary: int\n",
      "   ‚Ä¢ hire_date: date\n",
      "   ‚Ä¢ is_active: boolean\n",
      "\n",
      "‚ö° Performance Note:\n",
      "   Schema inference requires reading the entire file\n",
      "   For better performance on large files, define schema explicitly\n",
      "   Current partitions: 1\n"
     ]
    }
   ],
   "source": [
    "# Basic CSV Reading with Schema Inference\n",
    "print(\"üìñ Reading CSV Files with Schema Inference\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Read employee data with default settings\n",
    "employee_file = str(data_dir / \"employees.csv\")\n",
    "print(f\"üìÅ Reading: {employee_file}\")\n",
    "\n",
    "# Basic read with header and schema inference\n",
    "df_employees = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(employee_file)\n",
    "\n",
    "print(f\"\\n‚úÖ Successfully loaded employee data\")\n",
    "print(f\"üìä Rows: {df_employees.count()}\")\n",
    "print(f\"üìã Columns: {len(df_employees.columns)}\")\n",
    "\n",
    "# Display schema\n",
    "print(f\"\\nüîç Inferred Schema:\")\n",
    "df_employees.printSchema()\n",
    "\n",
    "# Show sample data\n",
    "print(f\"\\nüìÑ Sample Data (first 5 rows):\")\n",
    "df_employees.show(5, truncate=False)\n",
    "\n",
    "# Display data types\n",
    "print(f\"\\nüè∑Ô∏è  Column Data Types:\")\n",
    "for column, dtype in df_employees.dtypes:\n",
    "    print(f\"   ‚Ä¢ {column}: {dtype}\")\n",
    "\n",
    "# Performance note\n",
    "print(f\"\\n‚ö° Performance Note:\")\n",
    "print(f\"   Schema inference requires reading the entire file\")\n",
    "print(f\"   For better performance on large files, define schema explicitly\")\n",
    "print(f\"   Current partitions: {df_employees.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4244b441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß CSV Reading with Custom Options\n",
      "======================================\n",
      "üìÅ Reading: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/data/sales_semicolon.csv\n",
      "\n",
      "‚úÖ Successfully loaded sales data with custom options\n",
      "üìä Rows: 8\n",
      "üìã Columns: 7\n",
      "\n",
      "üîç Schema with null handling:\n",
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- quantity_sold: integer (nullable = true)\n",
      " |-- sale_date: date (nullable = true)\n",
      " |-- discount: double (nullable = true)\n",
      "\n",
      "\n",
      "üìÑ Sales Data (showing null values):\n",
      "+----------+------------------+-----------+-------+-------------+----------+--------+\n",
      "|product_id|product_name      |category   |price  |quantity_sold|sale_date |discount|\n",
      "+----------+------------------+-----------+-------+-------------+----------+--------+\n",
      "|P001      |Laptop Pro        |Electronics|1299.99|45           |2024-01-15|0.1     |\n",
      "|P002      |Wireless Mouse    |Electronics|29.99  |120          |NULL      |0.05    |\n",
      "|P003      |Office Chair      |Furniture  |299.99 |30           |2024-01-18|NULL    |\n",
      "|P004      |Coffee Maker      |Appliances |89.99  |75           |2024-01-20|0.15    |\n",
      "|P005      |Smartphone        |Electronics|NULL   |200          |2024-01-22|0.08    |\n",
      "|P006      |Desk Lamp         |Furniture  |45.99  |60           |2024-01-25|0.0     |\n",
      "|P007      |Tablet            |Electronics|599.99 |35           |2024-01-28|0.12    |\n",
      "|P008      |Ergonomic Keyboard|Electronics|79.99  |85           |2024-01-30|0.07    |\n",
      "+----------+------------------+-----------+-------+-------------+----------+--------+\n",
      "\n",
      "\n",
      "üîç Null Value Analysis:\n",
      "üìä Rows: 8\n",
      "üìã Columns: 7\n",
      "\n",
      "üîç Schema with null handling:\n",
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- quantity_sold: integer (nullable = true)\n",
      " |-- sale_date: date (nullable = true)\n",
      " |-- discount: double (nullable = true)\n",
      "\n",
      "\n",
      "üìÑ Sales Data (showing null values):\n",
      "+----------+------------------+-----------+-------+-------------+----------+--------+\n",
      "|product_id|product_name      |category   |price  |quantity_sold|sale_date |discount|\n",
      "+----------+------------------+-----------+-------+-------------+----------+--------+\n",
      "|P001      |Laptop Pro        |Electronics|1299.99|45           |2024-01-15|0.1     |\n",
      "|P002      |Wireless Mouse    |Electronics|29.99  |120          |NULL      |0.05    |\n",
      "|P003      |Office Chair      |Furniture  |299.99 |30           |2024-01-18|NULL    |\n",
      "|P004      |Coffee Maker      |Appliances |89.99  |75           |2024-01-20|0.15    |\n",
      "|P005      |Smartphone        |Electronics|NULL   |200          |2024-01-22|0.08    |\n",
      "|P006      |Desk Lamp         |Furniture  |45.99  |60           |2024-01-25|0.0     |\n",
      "|P007      |Tablet            |Electronics|599.99 |35           |2024-01-28|0.12    |\n",
      "|P008      |Ergonomic Keyboard|Electronics|79.99  |85           |2024-01-30|0.07    |\n",
      "+----------+------------------+-----------+-------+-------------+----------+--------+\n",
      "\n",
      "\n",
      "üîç Null Value Analysis:\n",
      "   ‚Ä¢ product_id: 0 null values\n",
      "   ‚Ä¢ product_name: 0 null values\n",
      "   ‚Ä¢ category: 0 null values\n",
      "   ‚Ä¢ product_id: 0 null values\n",
      "   ‚Ä¢ product_name: 0 null values\n",
      "   ‚Ä¢ category: 0 null values\n",
      "   ‚Ä¢ price: 1 null values\n",
      "   ‚Ä¢ quantity_sold: 0 null values\n",
      "   ‚Ä¢ price: 1 null values\n",
      "   ‚Ä¢ quantity_sold: 0 null values\n",
      "   ‚Ä¢ sale_date: 1 null values\n",
      "   ‚Ä¢ discount: 1 null values\n",
      "\n",
      "‚öôÔ∏è  Other Common CSV Options:\n",
      "   ‚Ä¢ sep=';'           ‚Üí Use semicolon as delimiter\n",
      "   ‚Ä¢ nullValue=''      ‚Üí Treat empty strings as null\n",
      "   ‚Ä¢ dateFormat='yyyy-MM-dd' ‚Üí Custom date format\n",
      "   ‚Ä¢ timestampFormat   ‚Üí Custom timestamp format\n",
      "   ‚Ä¢ quote='\"'         ‚Üí Quote character\n",
      "   ‚Ä¢ escape='\\\\'      ‚Üí Escape character\n",
      "   ‚Ä¢ ignoreLeadingWhiteSpace=true\n",
      "   ‚Ä¢ ignoreTrailingWhiteSpace=true\n",
      "   ‚Ä¢ sale_date: 1 null values\n",
      "   ‚Ä¢ discount: 1 null values\n",
      "\n",
      "‚öôÔ∏è  Other Common CSV Options:\n",
      "   ‚Ä¢ sep=';'           ‚Üí Use semicolon as delimiter\n",
      "   ‚Ä¢ nullValue=''      ‚Üí Treat empty strings as null\n",
      "   ‚Ä¢ dateFormat='yyyy-MM-dd' ‚Üí Custom date format\n",
      "   ‚Ä¢ timestampFormat   ‚Üí Custom timestamp format\n",
      "   ‚Ä¢ quote='\"'         ‚Üí Quote character\n",
      "   ‚Ä¢ escape='\\\\'      ‚Üí Escape character\n",
      "   ‚Ä¢ ignoreLeadingWhiteSpace=true\n",
      "   ‚Ä¢ ignoreTrailingWhiteSpace=true\n"
     ]
    }
   ],
   "source": [
    "# CSV Reading with Custom Delimiters and Null Handling\n",
    "print(\"üîß CSV Reading with Custom Options\")\n",
    "print(\"=\" * 38)\n",
    "\n",
    "# Read sales data with semicolon delimiter\n",
    "sales_file = str(data_dir / \"sales_semicolon.csv\")\n",
    "print(f\"üìÅ Reading: {sales_file}\")\n",
    "\n",
    "# Read with custom delimiter and null value handling\n",
    "df_sales = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .option(\"nullValue\", \"\") \\\n",
    "    .option(\"emptyValue\", \"\") \\\n",
    "    .csv(sales_file)\n",
    "\n",
    "print(f\"\\n‚úÖ Successfully loaded sales data with custom options\")\n",
    "print(f\"üìä Rows: {df_sales.count()}\")\n",
    "print(f\"üìã Columns: {len(df_sales.columns)}\")\n",
    "\n",
    "# Display schema\n",
    "print(f\"\\nüîç Schema with null handling:\")\n",
    "df_sales.printSchema()\n",
    "\n",
    "# Show data with null values\n",
    "print(f\"\\nüìÑ Sales Data (showing null values):\")\n",
    "df_sales.show(10, truncate=False)\n",
    "\n",
    "# Check for null values in each column\n",
    "print(f\"\\nüîç Null Value Analysis:\")\n",
    "null_counts = {}\n",
    "for col_name in df_sales.columns:\n",
    "    null_count = df_sales.filter(df_sales[col_name].isNull()).count()\n",
    "    null_counts[col_name] = null_count\n",
    "    print(f\"   ‚Ä¢ {col_name}: {null_count} null values\")\n",
    "\n",
    "# Custom read options examples\n",
    "print(f\"\\n‚öôÔ∏è  Other Common CSV Options:\")\n",
    "print(f\"   ‚Ä¢ sep=';'           ‚Üí Use semicolon as delimiter\")\n",
    "print(f\"   ‚Ä¢ nullValue=''      ‚Üí Treat empty strings as null\")\n",
    "print(f\"   ‚Ä¢ dateFormat='yyyy-MM-dd' ‚Üí Custom date format\")\n",
    "print(f\"   ‚Ä¢ timestampFormat   ‚Üí Custom timestamp format\")\n",
    "print(f\"   ‚Ä¢ quote='\\\"'         ‚Üí Quote character\")\n",
    "print(f\"   ‚Ä¢ escape='\\\\\\\\'      ‚Üí Escape character\")\n",
    "print(f\"   ‚Ä¢ ignoreLeadingWhiteSpace=true\")\n",
    "print(f\"   ‚Ä¢ ignoreTrailingWhiteSpace=true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "862756e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° CSV Reading with Explicit Schema\n",
      "=====================================\n",
      "üìù Defined explicit schema:\n",
      "StructType([StructField('employee_id', IntegerType(), True), StructField('name', StringType(), True), StructField('department', StringType(), True), StructField('salary', IntegerType(), True), StructField('hire_date', DateType(), True), StructField('is_active', BooleanType(), True)])\n",
      "\n",
      "‚úÖ Successfully loaded with explicit schema\n",
      "‚è±Ô∏è  Read time: 0.0220 seconds\n",
      "üìä Rows: 10\n",
      "\n",
      "üîç Schema Comparison:\n",
      "Inferred schema types: ['int', 'string', 'string', 'int', 'date', 'boolean']\n",
      "Explicit schema types: ['int', 'string', 'string', 'int', 'date', 'boolean']\n",
      "\n",
      "‚úÖ Data verification:\n",
      "   ‚úì Data is identical between inferred and explicit schema\n",
      "\n",
      "üöÄ Benefits of Explicit Schema:\n",
      "   ‚Ä¢ Faster reading (no schema inference pass)\n",
      "   ‚Ä¢ Consistent data types across reads\n",
      "   ‚Ä¢ Better error handling for malformed data\n",
      "   ‚Ä¢ Required for streaming applications\n",
      "   ‚Ä¢ Enables better query optimization\n",
      "   ‚úì Data is identical between inferred and explicit schema\n",
      "\n",
      "üöÄ Benefits of Explicit Schema:\n",
      "   ‚Ä¢ Faster reading (no schema inference pass)\n",
      "   ‚Ä¢ Consistent data types across reads\n",
      "   ‚Ä¢ Better error handling for malformed data\n",
      "   ‚Ä¢ Required for streaming applications\n",
      "   ‚Ä¢ Enables better query optimization\n"
     ]
    }
   ],
   "source": [
    "# CSV Reading with Explicit Schema (Better Performance)\n",
    "print(\"‚ö° CSV Reading with Explicit Schema\")\n",
    "print(\"=\" * 37)\n",
    "\n",
    "# Define explicit schema for employee data\n",
    "employee_schema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"hire_date\", DateType(), True),\n",
    "    StructField(\"is_active\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "print(\"üìù Defined explicit schema:\")\n",
    "print(employee_schema)\n",
    "\n",
    "# Read with explicit schema (no inference needed)\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "df_employees_schema = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\") \\\n",
    "    .schema(employee_schema) \\\n",
    "    .csv(employee_file)\n",
    "\n",
    "read_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Successfully loaded with explicit schema\")\n",
    "print(f\"‚è±Ô∏è  Read time: {read_time:.4f} seconds\")\n",
    "print(f\"üìä Rows: {df_employees_schema.count()}\")\n",
    "\n",
    "# Compare schemas\n",
    "print(f\"\\nüîç Schema Comparison:\")\n",
    "print(f\"Inferred schema types: {[dtype for _, dtype in df_employees.dtypes]}\")\n",
    "print(f\"Explicit schema types: {[dtype for _, dtype in df_employees_schema.dtypes]}\")\n",
    "\n",
    "# Verify data is identical\n",
    "print(f\"\\n‚úÖ Data verification:\")\n",
    "if df_employees.collect() == df_employees_schema.collect():\n",
    "    print(\"   ‚úì Data is identical between inferred and explicit schema\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Data differs between methods\")\n",
    "\n",
    "# Show performance benefits\n",
    "print(f\"\\nüöÄ Benefits of Explicit Schema:\")\n",
    "print(f\"   ‚Ä¢ Faster reading (no schema inference pass)\")\n",
    "print(f\"   ‚Ä¢ Consistent data types across reads\")\n",
    "print(f\"   ‚Ä¢ Better error handling for malformed data\")\n",
    "print(f\"   ‚Ä¢ Required for streaming applications\")\n",
    "print(f\"   ‚Ä¢ Enables better query optimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5a5accc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ CSV Writing Operations\n",
      "==========================\n",
      "üìã Sample DataFrame to write:\n",
      "üìã Sample DataFrame to write:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----+----------+\n",
      "| id|  product|price|      date|\n",
      "+---+---------+-----+----------+\n",
      "|  1|Product A|29.99|2024-01-01|\n",
      "|  2|Product B| 45.5|2024-01-02|\n",
      "|  3|Product C|15.75|2024-01-03|\n",
      "|  4|Product D| NULL|2024-01-04|\n",
      "|  5|Product E|89.99|2024-01-05|\n",
      "+---+---------+-----+----------+\n",
      "\n",
      "\n",
      "1Ô∏è‚É£ Writing basic CSV to: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/temp/output_basic_csv\n",
      "   ‚úÖ Basic CSV written successfully\n",
      "\n",
      "2Ô∏è‚É£ Writing custom CSV to: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/temp/output_custom_csv\n",
      "   ‚úÖ Custom CSV written with pipe delimiter and custom null values\n",
      "\n",
      "3Ô∏è‚É£ Writing partitioned CSV to: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/temp/output_partitioned_csv\n",
      "   ‚úÖ Basic CSV written successfully\n",
      "\n",
      "2Ô∏è‚É£ Writing custom CSV to: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/temp/output_custom_csv\n",
      "   ‚úÖ Custom CSV written with pipe delimiter and custom null values\n",
      "\n",
      "3Ô∏è‚É£ Writing partitioned CSV to: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/temp/output_partitioned_csv\n",
      "   ‚úÖ Partitioned CSV written successfully\n",
      "\n",
      "üìÅ Verification - Reading back written files:\n",
      "\n",
      "   Basic CSV read back (5 rows):\n",
      "   ‚úÖ Partitioned CSV written successfully\n",
      "\n",
      "üìÅ Verification - Reading back written files:\n",
      "\n",
      "   Basic CSV read back (5 rows):\n",
      "+---+---------+-----+----------+\n",
      "|id |product  |price|date      |\n",
      "+---+---------+-----+----------+\n",
      "|1  |Product A|29.99|2024-01-01|\n",
      "|2  |Product B|45.5 |2024-01-02|\n",
      "|3  |Product C|15.75|2024-01-03|\n",
      "|4  |Product D|NULL |2024-01-04|\n",
      "|5  |Product E|89.99|2024-01-05|\n",
      "+---+---------+-----+----------+\n",
      "\n",
      "\n",
      "üìù Common CSV Write Options:\n",
      "   ‚Ä¢ mode('overwrite/append/ignore/error')\n",
      "   ‚Ä¢ option('header', 'true/false')\n",
      "   ‚Ä¢ option('sep', 'delimiter')\n",
      "   ‚Ä¢ option('nullValue', 'custom_null')\n",
      "   ‚Ä¢ option('dateFormat', 'yyyy-MM-dd')\n",
      "   ‚Ä¢ option('timestampFormat', 'pattern')\n",
      "   ‚Ä¢ partitionBy('column1', 'column2')\n",
      "   ‚Ä¢ coalesce(1) ‚Üí single output file\n",
      "+---+---------+-----+----------+\n",
      "|id |product  |price|date      |\n",
      "+---+---------+-----+----------+\n",
      "|1  |Product A|29.99|2024-01-01|\n",
      "|2  |Product B|45.5 |2024-01-02|\n",
      "|3  |Product C|15.75|2024-01-03|\n",
      "|4  |Product D|NULL |2024-01-04|\n",
      "|5  |Product E|89.99|2024-01-05|\n",
      "+---+---------+-----+----------+\n",
      "\n",
      "\n",
      "üìù Common CSV Write Options:\n",
      "   ‚Ä¢ mode('overwrite/append/ignore/error')\n",
      "   ‚Ä¢ option('header', 'true/false')\n",
      "   ‚Ä¢ option('sep', 'delimiter')\n",
      "   ‚Ä¢ option('nullValue', 'custom_null')\n",
      "   ‚Ä¢ option('dateFormat', 'yyyy-MM-dd')\n",
      "   ‚Ä¢ option('timestampFormat', 'pattern')\n",
      "   ‚Ä¢ partitionBy('column1', 'column2')\n",
      "   ‚Ä¢ coalesce(1) ‚Üí single output file\n"
     ]
    }
   ],
   "source": [
    "# CSV Writing with Various Options\n",
    "print(\"üíæ CSV Writing Operations\")\n",
    "print(\"=\" * 26)\n",
    "\n",
    "# Create a sample DataFrame for writing\n",
    "sample_data = [\n",
    "    (1, \"Product A\", 29.99, \"2024-01-01\"),\n",
    "    (2, \"Product B\", 45.50, \"2024-01-02\"),\n",
    "    (3, \"Product C\", 15.75, \"2024-01-03\"),\n",
    "    (4, \"Product D\", None, \"2024-01-04\"),  # Null value\n",
    "    (5, \"Product E\", 89.99, \"2024-01-05\")\n",
    "]\n",
    "\n",
    "sample_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"date\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_sample = spark.createDataFrame(sample_data, sample_schema)\n",
    "\n",
    "print(\"üìã Sample DataFrame to write:\")\n",
    "df_sample.show()\n",
    "\n",
    "# 1. Basic CSV write with header\n",
    "output_path_basic = str(temp_dir / \"output_basic_csv\")\n",
    "print(f\"\\n1Ô∏è‚É£ Writing basic CSV to: {output_path_basic}\")\n",
    "\n",
    "df_sample.coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(output_path_basic)\n",
    "\n",
    "print(\"   ‚úÖ Basic CSV written successfully\")\n",
    "\n",
    "# 2. CSV write with custom delimiter and null handling\n",
    "output_path_custom = str(temp_dir / \"output_custom_csv\")\n",
    "print(f\"\\n2Ô∏è‚É£ Writing custom CSV to: {output_path_custom}\")\n",
    "\n",
    "df_sample.coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"|\") \\\n",
    "    .option(\"nullValue\", \"N/A\") \\\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\") \\\n",
    "    .csv(output_path_custom)\n",
    "\n",
    "print(\"   ‚úÖ Custom CSV written with pipe delimiter and custom null values\")\n",
    "\n",
    "# 3. CSV write with partitioning\n",
    "output_path_partitioned = str(temp_dir / \"output_partitioned_csv\")\n",
    "print(f\"\\n3Ô∏è‚É£ Writing partitioned CSV to: {output_path_partitioned}\")\n",
    "\n",
    "# Add a partition column\n",
    "df_with_partition = df_sample.withColumn(\"year\", lit(\"2024\"))\n",
    "\n",
    "df_with_partition \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .partitionBy(\"year\") \\\n",
    "    .csv(output_path_partitioned)\n",
    "\n",
    "print(\"   ‚úÖ Partitioned CSV written successfully\")\n",
    "\n",
    "# Verify written files\n",
    "print(f\"\\nüìÅ Verification - Reading back written files:\")\n",
    "\n",
    "# Read back basic CSV\n",
    "df_read_basic = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(output_path_basic)\n",
    "\n",
    "print(f\"\\n   Basic CSV read back ({df_read_basic.count()} rows):\")\n",
    "df_read_basic.show(truncate=False)\n",
    "\n",
    "# Show write options summary\n",
    "print(f\"\\nüìù Common CSV Write Options:\")\n",
    "print(f\"   ‚Ä¢ mode('overwrite/append/ignore/error')\")\n",
    "print(f\"   ‚Ä¢ option('header', 'true/false')\")\n",
    "print(f\"   ‚Ä¢ option('sep', 'delimiter')\")\n",
    "print(f\"   ‚Ä¢ option('nullValue', 'custom_null')\")\n",
    "print(f\"   ‚Ä¢ option('dateFormat', 'yyyy-MM-dd')\")\n",
    "print(f\"   ‚Ä¢ option('timestampFormat', 'pattern')\")\n",
    "print(f\"   ‚Ä¢ partitionBy('column1', 'column2')\")\n",
    "print(f\"   ‚Ä¢ coalesce(1) ‚Üí single output file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e9c22e",
   "metadata": {},
   "source": [
    "## 2.4 JSON File Operations\n",
    "\n",
    "JSON (JavaScript Object Notation) is a popular format for semi-structured data. PySpark provides excellent support for JSON files, including complex nested structures, arrays, and schema inference for JSON documents.\n",
    "\n",
    "**Key JSON Concepts:**\n",
    "- **Semi-structured Data**: JSON can contain nested objects and arrays\n",
    "- **Schema Flexibility**: Different records can have different structures\n",
    "- **Automatic Flattening**: PySpark can automatically handle nested structures\n",
    "- **Multi-line JSON**: Support for both single-line JSON and pretty-printed JSON\n",
    "- **Schema Evolution**: Handle changing schemas over time\n",
    "- **Complex Data Types**: Support for arrays, maps, and nested structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2e273c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Creating and Reading JSON Files\n",
      "====================================\n",
      "‚úÖ Created simple JSON: simple_users.json\n",
      "‚úÖ Created complex JSON: complex_customers.json\n",
      "\n",
      "üìñ Reading Simple JSON\n",
      "Schema for simple JSON:\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "\n",
      "Data preview:\n",
      "+---+-------------+---+-------+\n",
      "|age|city         |id |name   |\n",
      "+---+-------------+---+-------+\n",
      "|25 |New York     |1  |Alice  |\n",
      "|30 |San Francisco|2  |Bob    |\n",
      "|35 |Chicago      |3  |Charlie|\n",
      "|28 |Boston       |4  |Diana  |\n",
      "+---+-------------+---+-------+\n",
      "\n",
      "\n",
      "üìñ Reading Complex Nested JSON\n",
      "Schema for complex JSON (note nested structures):\n",
      "root\n",
      " |-- contact: struct (nullable = true)\n",
      " |    |-- email: string (nullable = true)\n",
      " |    |-- phone: struct (nullable = true)\n",
      " |    |    |-- home: string (nullable = true)\n",
      " |    |    |-- work: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- orders: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- amount: double (nullable = true)\n",
      " |    |    |-- items: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- order_id: string (nullable = true)\n",
      " |-- preferences: struct (nullable = true)\n",
      " |    |-- newsletter: boolean (nullable = true)\n",
      " |    |-- sms: boolean (nullable = true)\n",
      "\n",
      "\n",
      "Complex data preview:\n",
      "+----------------------------------------+-----------+----------+----------------------------------------------------------+-------------+\n",
      "|contact                                 |customer_id|name      |orders                                                    |preferences  |\n",
      "+----------------------------------------+-----------+----------+----------------------------------------------------------+-------------+\n",
      "|{john@example.com, {555-1234, 555-5678}}|C001       |John Doe  |[{150.0, [laptop, mouse], O001}, {75.5, [keyboard], O002}]|{true, false}|\n",
      "|{jane@example.com, {555-9999, NULL}}    |C002       |Jane Smith|[{200.0, [tablet, case, stylus], O003}]                   |{false, true}|\n",
      "+----------------------------------------+-----------+----------+----------------------------------------------------------+-------------+\n",
      "\n",
      "\n",
      "üîç Extracting Nested Fields\n",
      "Extracted nested fields:\n",
      "Schema for complex JSON (note nested structures):\n",
      "root\n",
      " |-- contact: struct (nullable = true)\n",
      " |    |-- email: string (nullable = true)\n",
      " |    |-- phone: struct (nullable = true)\n",
      " |    |    |-- home: string (nullable = true)\n",
      " |    |    |-- work: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- orders: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- amount: double (nullable = true)\n",
      " |    |    |-- items: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- order_id: string (nullable = true)\n",
      " |-- preferences: struct (nullable = true)\n",
      " |    |-- newsletter: boolean (nullable = true)\n",
      " |    |-- sms: boolean (nullable = true)\n",
      "\n",
      "\n",
      "Complex data preview:\n",
      "+----------------------------------------+-----------+----------+----------------------------------------------------------+-------------+\n",
      "|contact                                 |customer_id|name      |orders                                                    |preferences  |\n",
      "+----------------------------------------+-----------+----------+----------------------------------------------------------+-------------+\n",
      "|{john@example.com, {555-1234, 555-5678}}|C001       |John Doe  |[{150.0, [laptop, mouse], O001}, {75.5, [keyboard], O002}]|{true, false}|\n",
      "|{jane@example.com, {555-9999, NULL}}    |C002       |Jane Smith|[{200.0, [tablet, case, stylus], O003}]                   |{false, true}|\n",
      "+----------------------------------------+-----------+----------+----------------------------------------------------------+-------------+\n",
      "\n",
      "\n",
      "üîç Extracting Nested Fields\n",
      "Extracted nested fields:\n",
      "+-----------+----------+----------------+----------+\n",
      "|customer_id|name      |email           |home_phone|\n",
      "+-----------+----------+----------------+----------+\n",
      "|C001       |John Doe  |john@example.com|555-1234  |\n",
      "|C002       |Jane Smith|jane@example.com|555-9999  |\n",
      "+-----------+----------+----------------+----------+\n",
      "\n",
      "\n",
      "üìä JSON Files Summary:\n",
      "   ‚Ä¢ Simple JSON: 4 records\n",
      "   ‚Ä¢ Complex JSON: 2 records\n",
      "   ‚Ä¢ Nested field extraction: ‚úÖ Successful\n",
      "+-----------+----------+----------------+----------+\n",
      "|customer_id|name      |email           |home_phone|\n",
      "+-----------+----------+----------------+----------+\n",
      "|C001       |John Doe  |john@example.com|555-1234  |\n",
      "|C002       |Jane Smith|jane@example.com|555-9999  |\n",
      "+-----------+----------+----------------+----------+\n",
      "\n",
      "\n",
      "üìä JSON Files Summary:\n",
      "   ‚Ä¢ Simple JSON: 4 records\n",
      "   ‚Ä¢ Complex JSON: 2 records\n",
      "   ‚Ä¢ Nested field extraction: ‚úÖ Successful\n"
     ]
    }
   ],
   "source": [
    "# Create Sample JSON Files and Demonstrate Reading\n",
    "print(\"üìÑ Creating and Reading JSON Files\")\n",
    "print(\"=\" * 36)\n",
    "\n",
    "import json\n",
    "\n",
    "# 1. Simple JSON (line-delimited JSON)\n",
    "simple_json_data = [\n",
    "    {\"id\": 1, \"name\": \"Alice\", \"age\": 25, \"city\": \"New York\"},\n",
    "    {\"id\": 2, \"name\": \"Bob\", \"age\": 30, \"city\": \"San Francisco\"},\n",
    "    {\"id\": 3, \"name\": \"Charlie\", \"age\": 35, \"city\": \"Chicago\"},\n",
    "    {\"id\": 4, \"name\": \"Diana\", \"age\": 28, \"city\": \"Boston\"}\n",
    "]\n",
    "\n",
    "# Write line-delimited JSON\n",
    "simple_json_file = data_dir / \"simple_users.json\"\n",
    "with open(simple_json_file, 'w') as f:\n",
    "    for record in simple_json_data:\n",
    "        f.write(json.dumps(record) + '\\n')\n",
    "\n",
    "print(f\"‚úÖ Created simple JSON: {simple_json_file.name}\")\n",
    "\n",
    "# 2. Complex nested JSON\n",
    "complex_json_data = [\n",
    "    {\n",
    "        \"customer_id\": \"C001\",\n",
    "        \"name\": \"John Doe\",\n",
    "        \"contact\": {\n",
    "            \"email\": \"john@example.com\",\n",
    "            \"phone\": {\"home\": \"555-1234\", \"work\": \"555-5678\"}\n",
    "        },\n",
    "        \"orders\": [\n",
    "            {\"order_id\": \"O001\", \"amount\": 150.00, \"items\": [\"laptop\", \"mouse\"]},\n",
    "            {\"order_id\": \"O002\", \"amount\": 75.50, \"items\": [\"keyboard\"]}\n",
    "        ],\n",
    "        \"preferences\": {\"newsletter\": True, \"sms\": False}\n",
    "    },\n",
    "    {\n",
    "        \"customer_id\": \"C002\", \n",
    "        \"name\": \"Jane Smith\",\n",
    "        \"contact\": {\n",
    "            \"email\": \"jane@example.com\",\n",
    "            \"phone\": {\"home\": \"555-9999\"}\n",
    "        },\n",
    "        \"orders\": [\n",
    "            {\"order_id\": \"O003\", \"amount\": 200.00, \"items\": [\"tablet\", \"case\", \"stylus\"]}\n",
    "        ],\n",
    "        \"preferences\": {\"newsletter\": False, \"sms\": True}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Write complex JSON\n",
    "complex_json_file = data_dir / \"complex_customers.json\"\n",
    "with open(complex_json_file, 'w') as f:\n",
    "    for record in complex_json_data:\n",
    "        f.write(json.dumps(record) + '\\n')\n",
    "\n",
    "print(f\"‚úÖ Created complex JSON: {complex_json_file.name}\")\n",
    "\n",
    "# 3. Read simple JSON\n",
    "print(f\"\\nüìñ Reading Simple JSON\")\n",
    "df_simple = spark.read.json(str(simple_json_file))\n",
    "\n",
    "print(f\"Schema for simple JSON:\")\n",
    "df_simple.printSchema()\n",
    "\n",
    "print(f\"\\nData preview:\")\n",
    "df_simple.show(truncate=False)\n",
    "\n",
    "# 4. Read complex JSON\n",
    "print(f\"\\nüìñ Reading Complex Nested JSON\")\n",
    "df_complex = spark.read.json(str(complex_json_file))\n",
    "\n",
    "print(f\"Schema for complex JSON (note nested structures):\")\n",
    "df_complex.printSchema()\n",
    "\n",
    "print(f\"\\nComplex data preview:\")\n",
    "df_complex.show(truncate=False)\n",
    "\n",
    "# 5. Extracting nested fields\n",
    "print(f\"\\nüîç Extracting Nested Fields\")\n",
    "\n",
    "# Extract email from nested structure\n",
    "df_emails = df_complex.select(\n",
    "    \"customer_id\",\n",
    "    \"name\", \n",
    "    col(\"contact.email\").alias(\"email\"),\n",
    "    col(\"contact.phone.home\").alias(\"home_phone\")\n",
    ")\n",
    "\n",
    "print(f\"Extracted nested fields:\")\n",
    "df_emails.show(truncate=False)\n",
    "\n",
    "print(f\"\\nüìä JSON Files Summary:\")\n",
    "print(f\"   ‚Ä¢ Simple JSON: {df_simple.count()} records\")\n",
    "print(f\"   ‚Ä¢ Complex JSON: {df_complex.count()} records\")\n",
    "print(f\"   ‚Ä¢ Nested field extraction: ‚úÖ Successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6a416c",
   "metadata": {},
   "source": [
    "## 2.5 Parquet File Operations\n",
    "\n",
    "Parquet is a columnar storage format that's highly optimized for analytics workloads. It's the preferred format for big data processing due to its excellent compression, query performance, and schema evolution capabilities.\n",
    "\n",
    "**Key Parquet Benefits:**\n",
    "- **Columnar Storage**: Only read columns you need\n",
    "- **Compression**: Built-in compression algorithms (Snappy, GZIP, LZ4, BROTLI)\n",
    "- **Schema Evolution**: Add/remove columns without breaking compatibility\n",
    "- **Predicate Pushdown**: Filter data at the storage layer\n",
    "- **Statistics**: Built-in min/max statistics for efficient querying\n",
    "- **Cross-Platform**: Works across different big data tools\n",
    "- **Type Safety**: Preserves data types accurately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9da1c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Parquet Writing Operations\n",
      "=============================\n",
      "üîÑ Generating sample e-commerce data...\n",
      "‚úÖ Generated 1000 records\n",
      "üìã Schema:\n",
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- stock_quantity: integer (nullable = true)\n",
      " |-- created_date: string (nullable = true)\n",
      " |-- is_active: boolean (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n",
      "\n",
      "üíæ Writing Parquet with different compression algorithms:\n",
      "\n",
      "üîß Writing with SNAPPY compression...\n",
      "‚úÖ Generated 1000 records\n",
      "üìã Schema:\n",
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- stock_quantity: integer (nullable = true)\n",
      " |-- created_date: string (nullable = true)\n",
      " |-- is_active: boolean (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n",
      "\n",
      "üíæ Writing Parquet with different compression algorithms:\n",
      "\n",
      "üîß Writing with SNAPPY compression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ snappy: 0.959s, 0.02 MB\n",
      "\n",
      "üîß Writing with GZIP compression...\n",
      "   ‚úÖ gzip: 0.162s, 0.02 MB\n",
      "\n",
      "üîß Writing with LZ4 compression...\n",
      "   ‚úÖ lz4: 0.167s, 0.02 MB\n",
      "\n",
      "üîß Writing with UNCOMPRESSED compression...\n",
      "   ‚úÖ uncompressed: 0.151s, 0.05 MB\n",
      "\n",
      "üìä Compression Comparison:\n",
      "Codec        Write Time (s)  File Size (MB)  Compression Ratio \n",
      "-----------------------------------------------------------------\n",
      "snappy       0.959           0.02            1.96              x\n",
      "gzip         0.162           0.02            2.91              x\n",
      "lz4          0.167           0.02            2.04              x\n",
      "uncompressed 0.151           0.05            1.00              x\n",
      "\n",
      "üéØ Recommendations:\n",
      "   ‚Ä¢ SNAPPY: Best balance of speed and compression\n",
      "   ‚Ä¢ GZIP: Better compression, slower write/read\n",
      "   ‚Ä¢ LZ4: Fastest compression/decompression\n",
      "   ‚Ä¢ Choose based on your read vs write frequency\n",
      "   ‚úÖ lz4: 0.167s, 0.02 MB\n",
      "\n",
      "üîß Writing with UNCOMPRESSED compression...\n",
      "   ‚úÖ uncompressed: 0.151s, 0.05 MB\n",
      "\n",
      "üìä Compression Comparison:\n",
      "Codec        Write Time (s)  File Size (MB)  Compression Ratio \n",
      "-----------------------------------------------------------------\n",
      "snappy       0.959           0.02            1.96              x\n",
      "gzip         0.162           0.02            2.91              x\n",
      "lz4          0.167           0.02            2.04              x\n",
      "uncompressed 0.151           0.05            1.00              x\n",
      "\n",
      "üéØ Recommendations:\n",
      "   ‚Ä¢ SNAPPY: Best balance of speed and compression\n",
      "   ‚Ä¢ GZIP: Better compression, slower write/read\n",
      "   ‚Ä¢ LZ4: Fastest compression/decompression\n",
      "   ‚Ä¢ Choose based on your read vs write frequency\n"
     ]
    }
   ],
   "source": [
    "# Parquet Writing with Different Compression Options\n",
    "print(\"üíæ Parquet Writing Operations\")\n",
    "print(\"=\" * 29)\n",
    "\n",
    "# Create a larger dataset for compression comparison\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Generate sample e-commerce data\n",
    "def generate_ecommerce_data(num_records=1000):\n",
    "    categories = [\"Electronics\", \"Clothing\", \"Home\", \"Books\", \"Sports\", \"Toys\"]\n",
    "    brands = [\"BrandA\", \"BrandB\", \"BrandC\", \"BrandD\", \"BrandE\"]\n",
    "    \n",
    "    data = []\n",
    "    base_date = datetime(2024, 1, 1)\n",
    "    \n",
    "    for i in range(num_records):\n",
    "        # Use manual rounding to avoid PySpark round function conflict\n",
    "        price = int(random.uniform(10.0, 1000.0) * 100) / 100.0\n",
    "        rating = int(random.uniform(0.0, 5.0) * 10) / 10.0\n",
    "        \n",
    "        data.append((\n",
    "            f\"P{i:06d}\",  # product_id\n",
    "            f\"Product {i}\",  # product_name\n",
    "            random.choice(categories),  # category\n",
    "            random.choice(brands),  # brand\n",
    "            price,  # price\n",
    "            random.randint(0, 500),  # stock_quantity\n",
    "            (base_date + timedelta(days=random.randint(0, 365))).strftime('%Y-%m-%d'),  # created_date\n",
    "            random.choice([True, False]),  # is_active\n",
    "            rating  # rating\n",
    "        ))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate sample data\n",
    "print(\"üîÑ Generating sample e-commerce data...\")\n",
    "sample_data = generate_ecommerce_data(1000)\n",
    "\n",
    "# Define schema\n",
    "ecommerce_schema = StructType([\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"stock_quantity\", IntegerType(), True),\n",
    "    StructField(\"created_date\", StringType(), True),\n",
    "    StructField(\"is_active\", BooleanType(), True),\n",
    "    StructField(\"rating\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df_ecommerce = spark.createDataFrame(sample_data, ecommerce_schema)\n",
    "\n",
    "print(f\"‚úÖ Generated {df_ecommerce.count()} records\")\n",
    "print(f\"üìã Schema:\")\n",
    "df_ecommerce.printSchema()\n",
    "\n",
    "# Test different compression algorithms\n",
    "compression_codecs = [\"snappy\", \"gzip\", \"lz4\", \"uncompressed\"]\n",
    "compression_results = {}\n",
    "\n",
    "print(f\"\\nüíæ Writing Parquet with different compression algorithms:\")\n",
    "\n",
    "for codec in compression_codecs:\n",
    "    output_path = str(temp_dir / f\"ecommerce_parquet_{codec}\")\n",
    "    \n",
    "    print(f\"\\nüîß Writing with {codec.upper()} compression...\")\n",
    "    \n",
    "    # Measure write time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df_ecommerce.coalesce(1) \\\n",
    "        .write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"compression\", codec) \\\n",
    "        .parquet(output_path)\n",
    "    \n",
    "    write_time = time.time() - start_time\n",
    "    \n",
    "    # Check file size\n",
    "    import os\n",
    "    total_size = 0\n",
    "    for root, dirs, files in os.walk(output_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.parquet'):\n",
    "                total_size += os.path.getsize(os.path.join(root, file))\n",
    "    \n",
    "    compression_results[codec] = {\n",
    "        'write_time': write_time,\n",
    "        'file_size_mb': total_size / (1024 * 1024)\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úÖ {codec}: {write_time:.3f}s, {total_size / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "# Display compression comparison\n",
    "print(f\"\\nüìä Compression Comparison:\")\n",
    "print(f\"{'Codec':<12} {'Write Time (s)':<15} {'File Size (MB)':<15} {'Compression Ratio':<18}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "uncompressed_size = compression_results['uncompressed']['file_size_mb']\n",
    "for codec, results in compression_results.items():\n",
    "    ratio = uncompressed_size / results['file_size_mb'] if results['file_size_mb'] > 0 else 0\n",
    "    print(f\"{codec:<12} {results['write_time']:<15.3f} {results['file_size_mb']:<15.2f} {ratio:<18.2f}x\")\n",
    "\n",
    "print(f\"\\nüéØ Recommendations:\")\n",
    "print(f\"   ‚Ä¢ SNAPPY: Best balance of speed and compression\")\n",
    "print(f\"   ‚Ä¢ GZIP: Better compression, slower write/read\")\n",
    "print(f\"   ‚Ä¢ LZ4: Fastest compression/decompression\")\n",
    "print(f\"   ‚Ä¢ Choose based on your read vs write frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd2d39ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Parquet Reading with Advanced Features\n",
      "==========================================\n",
      "üìÅ Reading from: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/temp/ecommerce_parquet_snappy\n",
      "\n",
      "1Ô∏è‚É£ Basic Parquet Read\n",
      "‚úÖ Loaded 1000 records\n",
      "üìã Full schema preserved:\n",
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- stock_quantity: integer (nullable = true)\n",
      " |-- created_date: string (nullable = true)\n",
      " |-- is_active: boolean (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n",
      "\n",
      "2Ô∏è‚É£ Column Selection (Columnar Advantage)\n",
      "+----------+------------+------+-----------+\n",
      "|product_id|product_name|price |category   |\n",
      "+----------+------------+------+-----------+\n",
      "|P000000   |Product 0   |643.03|Home       |\n",
      "|P000001   |Product 1   |679.93|Electronics|\n",
      "|P000002   |Product 2   |102.75|Sports     |\n",
      "|P000003   |Product 3   |228.23|Electronics|\n",
      "|P000004   |Product 4   |285.09|Home       |\n",
      "+----------+------------+------+-----------+\n",
      "only showing top 5 rows\n",
      "‚ö° Column selection read time: 0.1510s\n",
      "üìä Only selected columns loaded from storage\n",
      "\n",
      "3Ô∏è‚É£ Predicate Pushdown (Storage-Level Filtering)\n",
      "‚úÖ Loaded 1000 records\n",
      "üìã Full schema preserved:\n",
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- stock_quantity: integer (nullable = true)\n",
      " |-- created_date: string (nullable = true)\n",
      " |-- is_active: boolean (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n",
      "\n",
      "2Ô∏è‚É£ Column Selection (Columnar Advantage)\n",
      "+----------+------------+------+-----------+\n",
      "|product_id|product_name|price |category   |\n",
      "+----------+------------+------+-----------+\n",
      "|P000000   |Product 0   |643.03|Home       |\n",
      "|P000001   |Product 1   |679.93|Electronics|\n",
      "|P000002   |Product 2   |102.75|Sports     |\n",
      "|P000003   |Product 3   |228.23|Electronics|\n",
      "|P000004   |Product 4   |285.09|Home       |\n",
      "+----------+------------+------+-----------+\n",
      "only showing top 5 rows\n",
      "‚ö° Column selection read time: 0.1510s\n",
      "üìä Only selected columns loaded from storage\n",
      "\n",
      "3Ô∏è‚É£ Predicate Pushdown (Storage-Level Filtering)\n",
      "üîç Found 81 expensive electronics\n",
      "‚ö° Predicate pushdown time: 0.1946s\n",
      "üìã Sample results:\n",
      "+----------+------------+-----------+------+------+--------------+------------+---------+------+\n",
      "|product_id|product_name|category   |brand |price |stock_quantity|created_date|is_active|rating|\n",
      "+----------+------------+-----------+------+------+--------------+------------+---------+------+\n",
      "|P000001   |Product 1   |Electronics|BrandE|679.93|216           |2024-01-17  |true     |4.4   |\n",
      "|P000007   |Product 7   |Electronics|BrandB|581.57|395           |2024-05-28  |true     |3.5   |\n",
      "|P000024   |Product 24  |Electronics|BrandD|879.22|417           |2024-02-05  |true     |4.7   |\n",
      "|P000051   |Product 51  |Electronics|BrandA|510.35|324           |2024-08-04  |false    |1.2   |\n",
      "|P000064   |Product 64  |Electronics|BrandD|900.5 |0             |2024-09-23  |true     |0.9   |\n",
      "+----------+------------+-----------+------+------+--------------+------------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "4Ô∏è‚É£ Combined Optimization\n",
      "üîç Found 81 expensive electronics\n",
      "‚ö° Predicate pushdown time: 0.1946s\n",
      "üìã Sample results:\n",
      "+----------+------------+-----------+------+------+--------------+------------+---------+------+\n",
      "|product_id|product_name|category   |brand |price |stock_quantity|created_date|is_active|rating|\n",
      "+----------+------------+-----------+------+------+--------------+------------+---------+------+\n",
      "|P000001   |Product 1   |Electronics|BrandE|679.93|216           |2024-01-17  |true     |4.4   |\n",
      "|P000007   |Product 7   |Electronics|BrandB|581.57|395           |2024-05-28  |true     |3.5   |\n",
      "|P000024   |Product 24  |Electronics|BrandD|879.22|417           |2024-02-05  |true     |4.7   |\n",
      "|P000051   |Product 51  |Electronics|BrandA|510.35|324           |2024-08-04  |false    |1.2   |\n",
      "|P000064   |Product 64  |Electronics|BrandD|900.5 |0             |2024-09-23  |true     |0.9   |\n",
      "+----------+------------+-----------+------+------+--------------+------------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "4Ô∏è‚É£ Combined Optimization\n",
      "üåü Found 182 high-rated products\n",
      "‚ö° Combined optimization time: 0.1533s\n",
      "üìã High-rated products:\n",
      "+------------+------+------+\n",
      "|product_name|price |rating|\n",
      "+------------+------+------+\n",
      "|Product 180 |284.5 |4.9   |\n",
      "|Product 495 |267.56|4.9   |\n",
      "|Product 299 |379.46|4.9   |\n",
      "|Product 158 |579.06|4.9   |\n",
      "|Product 378 |569.35|4.9   |\n",
      "+------------+------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "5Ô∏è‚É£ Schema Evolution\n",
      "üåü Found 182 high-rated products\n",
      "‚ö° Combined optimization time: 0.1533s\n",
      "üìã High-rated products:\n",
      "+------------+------+------+\n",
      "|product_name|price |rating|\n",
      "+------------+------+------+\n",
      "|Product 180 |284.5 |4.9   |\n",
      "|Product 495 |267.56|4.9   |\n",
      "|Product 299 |379.46|4.9   |\n",
      "|Product 158 |579.06|4.9   |\n",
      "|Product 378 |569.35|4.9   |\n",
      "+------------+------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "5Ô∏è‚É£ Schema Evolution\n",
      "‚úÖ Schema evolution successful\n",
      "üìã New schema with added column:\n",
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- stock_quantity: integer (nullable = true)\n",
      " |-- created_date: string (nullable = true)\n",
      " |-- is_active: boolean (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- discount_percent: double (nullable = true)\n",
      "\n",
      "\n",
      "üöÄ Parquet Performance Benefits:\n",
      "   ‚Ä¢ Columnar storage: Only read needed columns\n",
      "   ‚Ä¢ Predicate pushdown: Filter data at storage layer\n",
      "   ‚Ä¢ Compression: Reduce I/O with efficient codecs\n",
      "   ‚Ä¢ Schema evolution: Add/remove columns safely\n",
      "   ‚Ä¢ Statistics: Built-in min/max for query optimization\n",
      "   ‚Ä¢ Cross-platform: Works with all big data tools\n",
      "‚úÖ Schema evolution successful\n",
      "üìã New schema with added column:\n",
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- stock_quantity: integer (nullable = true)\n",
      " |-- created_date: string (nullable = true)\n",
      " |-- is_active: boolean (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- discount_percent: double (nullable = true)\n",
      "\n",
      "\n",
      "üöÄ Parquet Performance Benefits:\n",
      "   ‚Ä¢ Columnar storage: Only read needed columns\n",
      "   ‚Ä¢ Predicate pushdown: Filter data at storage layer\n",
      "   ‚Ä¢ Compression: Reduce I/O with efficient codecs\n",
      "   ‚Ä¢ Schema evolution: Add/remove columns safely\n",
      "   ‚Ä¢ Statistics: Built-in min/max for query optimization\n",
      "   ‚Ä¢ Cross-platform: Works with all big data tools\n"
     ]
    }
   ],
   "source": [
    "# Parquet Reading with Optimization Features\n",
    "print(\"üìñ Parquet Reading with Advanced Features\")\n",
    "print(\"=\" * 42)\n",
    "\n",
    "# Use the snappy compressed file for reading demonstrations\n",
    "parquet_file = str(temp_dir / \"ecommerce_parquet_snappy\")\n",
    "\n",
    "print(f\"üìÅ Reading from: {parquet_file}\")\n",
    "\n",
    "# 1. Basic Parquet read\n",
    "print(f\"\\n1Ô∏è‚É£ Basic Parquet Read\")\n",
    "df_parquet = spark.read.parquet(parquet_file)\n",
    "\n",
    "print(f\"‚úÖ Loaded {df_parquet.count()} records\")\n",
    "print(f\"üìã Full schema preserved:\")\n",
    "df_parquet.printSchema()\n",
    "\n",
    "# 2. Column selection (columnar advantage)\n",
    "print(f\"\\n2Ô∏è‚É£ Column Selection (Columnar Advantage)\")\n",
    "start_time = time.time()\n",
    "\n",
    "df_selected = spark.read.parquet(parquet_file) \\\n",
    "    .select(\"product_id\", \"product_name\", \"price\", \"category\")\n",
    "\n",
    "df_selected.show(5, truncate=False)\n",
    "read_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚ö° Column selection read time: {read_time:.4f}s\")\n",
    "print(f\"üìä Only selected columns loaded from storage\")\n",
    "\n",
    "# 3. Predicate pushdown (filter at storage level)\n",
    "print(f\"\\n3Ô∏è‚É£ Predicate Pushdown (Storage-Level Filtering)\")\n",
    "\n",
    "# Filter expensive electronics\n",
    "start_time = time.time()\n",
    "\n",
    "df_filtered = spark.read.parquet(parquet_file) \\\n",
    "    .filter((col(\"category\") == \"Electronics\") & (col(\"price\") > 500))\n",
    "\n",
    "expensive_electronics = df_filtered.collect()\n",
    "filter_time = time.time() - start_time\n",
    "\n",
    "print(f\"üîç Found {len(expensive_electronics)} expensive electronics\")\n",
    "print(f\"‚ö° Predicate pushdown time: {filter_time:.4f}s\")\n",
    "print(f\"üìã Sample results:\")\n",
    "df_filtered.show(5, truncate=False)\n",
    "\n",
    "# 4. Combined optimization: column selection + filtering\n",
    "print(f\"\\n4Ô∏è‚É£ Combined Optimization\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df_optimized = spark.read.parquet(parquet_file) \\\n",
    "    .select(\"product_name\", \"price\", \"rating\") \\\n",
    "    .filter(col(\"rating\") >= 4.0)\n",
    "\n",
    "high_rated_count = df_optimized.count()\n",
    "combined_time = time.time() - start_time\n",
    "\n",
    "print(f\"üåü Found {high_rated_count} high-rated products\")\n",
    "print(f\"‚ö° Combined optimization time: {combined_time:.4f}s\")\n",
    "print(f\"üìã High-rated products:\")\n",
    "df_optimized.orderBy(col(\"rating\").desc()).show(5, truncate=False)\n",
    "\n",
    "# 5. Schema evolution example\n",
    "print(f\"\\n5Ô∏è‚É£ Schema Evolution\")\n",
    "\n",
    "# Add a new column to existing data\n",
    "df_with_new_column = df_parquet.withColumn(\"discount_percent\", lit(0.1))\n",
    "\n",
    "# Write with new schema\n",
    "evolved_path = str(temp_dir / \"ecommerce_evolved\")\n",
    "df_with_new_column.coalesce(1).write.mode(\"overwrite\").parquet(evolved_path)\n",
    "\n",
    "# Read back - Parquet handles schema differences gracefully\n",
    "df_evolved = spark.read.parquet(evolved_path)\n",
    "print(f\"‚úÖ Schema evolution successful\")\n",
    "print(f\"üìã New schema with added column:\")\n",
    "df_evolved.printSchema()\n",
    "\n",
    "# Performance summary\n",
    "print(f\"\\nüöÄ Parquet Performance Benefits:\")\n",
    "print(f\"   ‚Ä¢ Columnar storage: Only read needed columns\")\n",
    "print(f\"   ‚Ä¢ Predicate pushdown: Filter data at storage layer\")\n",
    "print(f\"   ‚Ä¢ Compression: Reduce I/O with efficient codecs\")\n",
    "print(f\"   ‚Ä¢ Schema evolution: Add/remove columns safely\")\n",
    "print(f\"   ‚Ä¢ Statistics: Built-in min/max for query optimization\")\n",
    "print(f\"   ‚Ä¢ Cross-platform: Works with all big data tools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00f66eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóÇÔ∏è  Partitioned Parquet Operations\n",
      "==================================\n",
      "üìä Creating partitioned Parquet dataset...\n",
      "‚úÖ Created partitioned dataset at: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/temp/ecommerce_partitioned\n",
      "\n",
      "üìÅ Partition Structure:\n",
      "   ‚Ä¢ category=Books: 1 parquet file(s)\n",
      "   ‚Ä¢ category=Clothing: 1 parquet file(s)\n",
      "   ‚Ä¢ category=Electronics: 1 parquet file(s)\n",
      "   ‚Ä¢ category=Home: 1 parquet file(s)\n",
      "   ‚Ä¢ category=Sports: 1 parquet file(s)\n",
      "   ‚Ä¢ category=Toys: 1 parquet file(s)\n",
      "\n",
      "üîç Partition Pruning Example\n",
      "‚ö° Electronics partition read time: 0.1744s\n",
      "üìä Found 160 electronics products\n",
      "‚úÖ Created partitioned dataset at: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/temp/ecommerce_partitioned\n",
      "\n",
      "üìÅ Partition Structure:\n",
      "   ‚Ä¢ category=Books: 1 parquet file(s)\n",
      "   ‚Ä¢ category=Clothing: 1 parquet file(s)\n",
      "   ‚Ä¢ category=Electronics: 1 parquet file(s)\n",
      "   ‚Ä¢ category=Home: 1 parquet file(s)\n",
      "   ‚Ä¢ category=Sports: 1 parquet file(s)\n",
      "   ‚Ä¢ category=Toys: 1 parquet file(s)\n",
      "\n",
      "üîç Partition Pruning Example\n",
      "‚ö° Electronics partition read time: 0.1744s\n",
      "üìä Found 160 electronics products\n",
      "‚ö° Read-all-then-filter time: 0.0894s\n",
      "\n",
      "üìö Reading Multiple Partitions\n",
      "üìä Electronics + Books: 330 products\n",
      "‚ö° Read-all-then-filter time: 0.0894s\n",
      "\n",
      "üìö Reading Multiple Partitions\n",
      "üìä Electronics + Books: 330 products\n",
      "+-----------+-----+\n",
      "|   category|count|\n",
      "+-----------+-----+\n",
      "|      Books|  170|\n",
      "|Electronics|  160|\n",
      "+-----------+-----+\n",
      "\n",
      "\n",
      "üóÇÔ∏è  Multi-Level Partitioning\n",
      "+-----------+-----+\n",
      "|   category|count|\n",
      "+-----------+-----+\n",
      "|      Books|  170|\n",
      "|Electronics|  160|\n",
      "+-----------+-----+\n",
      "\n",
      "\n",
      "üóÇÔ∏è  Multi-Level Partitioning\n",
      "‚úÖ Created multi-level partitioned dataset\n",
      "\n",
      "üéØ Efficient Partitioned Query\n",
      "üìã High-value electronics in 2024:\n",
      "+------------+------+------+\n",
      "|product_name|price |rating|\n",
      "+------------+------+------+\n",
      "|Product 157 |999.37|4.2   |\n",
      "|Product 809 |960.22|0.0   |\n",
      "|Product 175 |951.04|4.2   |\n",
      "|Product 599 |949.95|3.8   |\n",
      "|Product 371 |945.73|3.3   |\n",
      "+------------+------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "üí° Partitioning Best Practices:\n",
      "   ‚Ä¢ Partition by columns frequently used in WHERE clauses\n",
      "   ‚Ä¢ Avoid partitions with too few files (< 1MB each)\n",
      "   ‚Ä¢ Avoid too many partitions (creates small files)\n",
      "   ‚Ä¢ Consider cardinality: 100-10,000 partitions is typical\n",
      "   ‚Ä¢ Use date/time partitioning for time-series data\n",
      "   ‚Ä¢ Partition pruning works with = and IN operators\n",
      "   ‚Ä¢ Multi-level partitioning: category/year/month/day\n",
      "‚úÖ Created multi-level partitioned dataset\n",
      "\n",
      "üéØ Efficient Partitioned Query\n",
      "üìã High-value electronics in 2024:\n",
      "+------------+------+------+\n",
      "|product_name|price |rating|\n",
      "+------------+------+------+\n",
      "|Product 157 |999.37|4.2   |\n",
      "|Product 809 |960.22|0.0   |\n",
      "|Product 175 |951.04|4.2   |\n",
      "|Product 599 |949.95|3.8   |\n",
      "|Product 371 |945.73|3.3   |\n",
      "+------------+------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "üí° Partitioning Best Practices:\n",
      "   ‚Ä¢ Partition by columns frequently used in WHERE clauses\n",
      "   ‚Ä¢ Avoid partitions with too few files (< 1MB each)\n",
      "   ‚Ä¢ Avoid too many partitions (creates small files)\n",
      "   ‚Ä¢ Consider cardinality: 100-10,000 partitions is typical\n",
      "   ‚Ä¢ Use date/time partitioning for time-series data\n",
      "   ‚Ä¢ Partition pruning works with = and IN operators\n",
      "   ‚Ä¢ Multi-level partitioning: category/year/month/day\n"
     ]
    }
   ],
   "source": [
    "# Partitioned Parquet Operations\n",
    "print(\"üóÇÔ∏è  Partitioned Parquet Operations\")\n",
    "print(\"=\" * 34)\n",
    "\n",
    "# Create partitioned dataset for better query performance\n",
    "print(\"üìä Creating partitioned Parquet dataset...\")\n",
    "\n",
    "# Write data partitioned by category\n",
    "partitioned_path = str(temp_dir / \"ecommerce_partitioned\")\n",
    "\n",
    "df_parquet.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"category\") \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .parquet(partitioned_path)\n",
    "\n",
    "print(f\"‚úÖ Created partitioned dataset at: {partitioned_path}\")\n",
    "\n",
    "# Explore partition structure\n",
    "import os\n",
    "partitions = []\n",
    "for item in os.listdir(partitioned_path):\n",
    "    if item.startswith(\"category=\"):\n",
    "        partitions.append(item)\n",
    "\n",
    "print(f\"\\nüìÅ Partition Structure:\")\n",
    "for partition in sorted(partitions):\n",
    "    partition_path = os.path.join(partitioned_path, partition)\n",
    "    file_count = len([f for f in os.listdir(partition_path) if f.endswith('.parquet')])\n",
    "    print(f\"   ‚Ä¢ {partition}: {file_count} parquet file(s)\")\n",
    "\n",
    "# Read specific partitions (partition pruning)\n",
    "print(f\"\\nüîç Partition Pruning Example\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Read only Electronics partition\n",
    "df_electronics = spark.read.parquet(partitioned_path) \\\n",
    "    .filter(col(\"category\") == \"Electronics\")\n",
    "\n",
    "electronics_count = df_electronics.count()\n",
    "pruning_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚ö° Electronics partition read time: {pruning_time:.4f}s\")\n",
    "print(f\"üìä Found {electronics_count} electronics products\")\n",
    "\n",
    "# Compare with reading all partitions then filtering\n",
    "start_time = time.time()\n",
    "\n",
    "df_all_then_filter = spark.read.parquet(partitioned_path) \\\n",
    "    .filter(col(\"category\") == \"Electronics\")\n",
    "\n",
    "all_then_filter_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚ö° Read-all-then-filter time: {all_then_filter_time:.4f}s\")\n",
    "\n",
    "# Multiple partition read\n",
    "print(f\"\\nüìö Reading Multiple Partitions\")\n",
    "\n",
    "df_multiple = spark.read.parquet(partitioned_path) \\\n",
    "    .filter(col(\"category\").isin([\"Electronics\", \"Books\"]))\n",
    "\n",
    "print(f\"üìä Electronics + Books: {df_multiple.count()} products\")\n",
    "df_multiple.groupBy(\"category\").count().show()\n",
    "\n",
    "# Write with multiple partition columns\n",
    "print(f\"\\nüóÇÔ∏è  Multi-Level Partitioning\")\n",
    "\n",
    "# Add year partition from created_date\n",
    "df_with_year = df_parquet.withColumn(\"year\", lit(\"2024\"))\n",
    "\n",
    "multi_partitioned_path = str(temp_dir / \"ecommerce_multi_partitioned\")\n",
    "\n",
    "df_with_year.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"category\", \"year\") \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .parquet(multi_partitioned_path)\n",
    "\n",
    "print(f\"‚úÖ Created multi-level partitioned dataset\")\n",
    "\n",
    "# Demonstrate efficient querying\n",
    "print(f\"\\nüéØ Efficient Partitioned Query\")\n",
    "\n",
    "df_specific = spark.read.parquet(multi_partitioned_path) \\\n",
    "    .filter((col(\"category\") == \"Electronics\") & (col(\"year\") == \"2024\")) \\\n",
    "    .select(\"product_name\", \"price\", \"rating\") \\\n",
    "    .filter(col(\"price\") > 100)\n",
    "\n",
    "print(f\"üìã High-value electronics in 2024:\")\n",
    "df_specific.orderBy(col(\"price\").desc()).show(5, truncate=False)\n",
    "\n",
    "# Partitioning best practices\n",
    "print(f\"\\nüí° Partitioning Best Practices:\")\n",
    "print(f\"   ‚Ä¢ Partition by columns frequently used in WHERE clauses\")\n",
    "print(f\"   ‚Ä¢ Avoid partitions with too few files (< 1MB each)\")\n",
    "print(f\"   ‚Ä¢ Avoid too many partitions (creates small files)\")\n",
    "print(f\"   ‚Ä¢ Consider cardinality: 100-10,000 partitions is typical\")\n",
    "print(f\"   ‚Ä¢ Use date/time partitioning for time-series data\")\n",
    "print(f\"   ‚Ä¢ Partition pruning works with = and IN operators\")\n",
    "print(f\"   ‚Ä¢ Multi-level partitioning: category/year/month/day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb5decc",
   "metadata": {},
   "source": [
    "## 2.6 Module 2 Summary\n",
    "\n",
    "üéâ **Congratulations!** You've completed Module 2: Data Ingestion & I/O Operations\n",
    "\n",
    "### What We Covered:\n",
    "\n",
    "‚úÖ **Environment Setup**\n",
    "- Optimized SparkSession for I/O operations\n",
    "- Project directory structure\n",
    "- Import dependencies\n",
    "\n",
    "‚úÖ **CSV Operations**\n",
    "- Schema inference vs explicit schema definition\n",
    "- Custom delimiters and null value handling\n",
    "- Writing with various options and partitioning\n",
    "- Performance considerations\n",
    "\n",
    "‚úÖ **JSON Operations**\n",
    "- Line-delimited JSON processing\n",
    "- Nested JSON structure handling\n",
    "- Field extraction from complex schemas\n",
    "- Schema inference for semi-structured data\n",
    "\n",
    "‚úÖ **Parquet Operations**\n",
    "- Compression algorithm comparison (Snappy, GZIP, LZ4)\n",
    "- Columnar storage advantages\n",
    "- Predicate pushdown optimization\n",
    "- Schema evolution capabilities\n",
    "- Partitioned datasets for query performance\n",
    "\n",
    "### Key Performance Insights:\n",
    "\n",
    "üöÄ **Parquet Benefits:**\n",
    "- **Columnar Storage**: 2-5x faster for analytical queries\n",
    "- **Compression**: 50-75% size reduction with Snappy/GZIP\n",
    "- **Predicate Pushdown**: Query-level filtering at storage\n",
    "- **Schema Evolution**: Safe column additions/removals\n",
    "\n",
    "üìä **Best Practices Learned:**\n",
    "- Use explicit schemas for production workloads\n",
    "- Choose compression based on read vs write frequency\n",
    "- Partition by frequently queried columns\n",
    "- Avoid over-partitioning (keep files > 1MB)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "The next modules in our PySpark tutorial will cover:\n",
    "- **Module 3**: Data Transformations & Operations\n",
    "- **Module 4**: SQL & DataFrame API\n",
    "- **Module 5**: Performance Optimization\n",
    "- **Module 6**: Machine Learning with MLlib\n",
    "- **Module 7**: Streaming Data Processing\n",
    "- **Module 8**: Production Deployment\n",
    "\n",
    "Ready to continue your PySpark journey! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
