{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34007d30",
   "metadata": {},
   "source": [
    "# ðŸš€ Google Colab Setup for PySpark Tutorial\n",
    "\n",
    "**âš ï¸ IMPORTANT**: Run all cells in this notebook FIRST before starting any PySpark tutorial module in Google Colab.\n",
    "\n",
    "This notebook will:\n",
    "- âœ… Install Java (required for PySpark)\n",
    "- âœ… Install PySpark and all dependencies\n",
    "- âœ… Configure environment variables\n",
    "- âœ… Test the installation\n",
    "- âœ… Provide troubleshooting tips\n",
    "\n",
    "**Estimated time**: 2-3 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2707c0",
   "metadata": {},
   "source": [
    "## ðŸ” Environment Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c569910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import platform\n",
    "\n",
    "print(\"ðŸ” Environment Detection:\")\n",
    "print(f\"   Python Version: {sys.version.split()[0]}\")\n",
    "print(f\"   Operating System: {platform.system()}\")\n",
    "print(f\"   Platform: {platform.platform()}\")\n",
    "\n",
    "# Check if running in Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"   Google Colab: {'âœ… Yes' if IN_COLAB else 'âŒ No (Local environment)'}\")\n",
    "\n",
    "if not IN_COLAB:\n",
    "    print(\"\\nðŸ’» You're running locally - PySpark should already be installed!\")\n",
    "    print(\"   Skip to the 'Test Installation' section at the bottom.\")\n",
    "else:\n",
    "    print(\"\\nðŸš€ Google Colab detected - proceeding with setup...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bdc697",
   "metadata": {},
   "source": [
    "## â˜• Java Installation (Required for PySpark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f61959",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    print(\"â˜• Installing Java 8 (required for PySpark)...\")\n",
    "    \n",
    "    # Update package list and install Java\n",
    "    !apt-get update -qq > /dev/null 2>&1\n",
    "    !apt-get install openjdk-8-jdk-headless -qq > /dev/null 2>&1\n",
    "    \n",
    "    # Set JAVA_HOME environment variable\n",
    "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "    \n",
    "    print(\"âœ… Java installation complete!\")\n",
    "    \n",
    "    # Verify Java installation\n",
    "    print(\"\\nðŸ” Verifying Java installation:\")\n",
    "    !java -version\n",
    "    \n",
    "else:\n",
    "    print(\"ðŸ’» Local environment - checking existing Java installation...\")\n",
    "    try:\n",
    "        !java -version\n",
    "        print(\"âœ… Java is already installed!\")\n",
    "    except:\n",
    "        print(\"âŒ Java not found. Please install Java 8 or 11 manually.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0502d65f",
   "metadata": {},
   "source": [
    "## ðŸ PySpark and Dependencies Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2c72dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    print(\"ðŸ“¦ Installing PySpark and dependencies...\")\n",
    "    \n",
    "    # Install core packages\n",
    "    !pip install -q pyspark==3.5.0\n",
    "    print(\"   âœ… PySpark 3.5.0 installed\")\n",
    "    \n",
    "    # Install data manipulation libraries\n",
    "    !pip install -q pandas numpy\n",
    "    print(\"   âœ… Data libraries (pandas, numpy) installed\")\n",
    "    \n",
    "    # Install visualization libraries\n",
    "    !pip install -q matplotlib seaborn plotly\n",
    "    print(\"   âœ… Visualization libraries installed\")\n",
    "    \n",
    "    # Install ML and utility libraries\n",
    "    !pip install -q scikit-learn faker\n",
    "    print(\"   âœ… ML and utility libraries installed\")\n",
    "    \n",
    "    print(\"\\nðŸŽ‰ All packages installed successfully!\")\n",
    "    \n",
    "else:\n",
    "    print(\"ðŸ’» Local environment - checking existing installations...\")\n",
    "    \n",
    "    packages = ['pyspark', 'pandas', 'numpy', 'matplotlib', 'seaborn', 'plotly', 'sklearn', 'faker']\n",
    "    for package in packages:\n",
    "        try:\n",
    "            if package == 'sklearn':\n",
    "                import sklearn\n",
    "            else:\n",
    "                __import__(package)\n",
    "            print(f\"   âœ… {package} is installed\")\n",
    "        except ImportError:\n",
    "            print(f\"   âŒ {package} is missing - install with: pip install {package}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538b1f3c",
   "metadata": {},
   "source": [
    "## âš™ï¸ Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9756412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    print(\"âš™ï¸ Configuring PySpark environment variables...\")\n",
    "    \n",
    "    # Set PySpark environment variables\n",
    "    os.environ[\"SPARK_HOME\"] = \"/usr/local/lib/python3.10/dist-packages/pyspark\"\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "    os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python3\"\n",
    "    \n",
    "    # Add PySpark to Python path\n",
    "    import sys\n",
    "    sys.path.append('/usr/local/lib/python3.10/dist-packages')\n",
    "    \n",
    "    print(\"âœ… Environment configuration complete!\")\n",
    "    \n",
    "    # Display environment variables\n",
    "    print(\"\\nðŸ” Environment Variables:\")\n",
    "    print(f\"   JAVA_HOME: {os.environ.get('JAVA_HOME', 'Not set')}\")\n",
    "    print(f\"   SPARK_HOME: {os.environ.get('SPARK_HOME', 'Not set')}\")\n",
    "    print(f\"   PYSPARK_PYTHON: {os.environ.get('PYSPARK_PYTHON', 'Not set')}\")\n",
    "    \n",
    "else:\n",
    "    print(\"ðŸ’» Local environment - environment should be pre-configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc11c7e",
   "metadata": {},
   "source": [
    "## ðŸ§ª Test PySpark Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d678c3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ§ª Testing PySpark installation...\")\n",
    "\n",
    "try:\n",
    "    # Import PySpark\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col, count, sum as spark_sum\n",
    "    print(\"   âœ… PySpark imports successful\")\n",
    "    \n",
    "    # Create Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"ColabSetupTest\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(f\"   âœ… Spark session created successfully\")\n",
    "    print(f\"   ðŸ“Š Spark version: {spark.version}\")\n",
    "    print(f\"   ðŸ“ Application name: {spark.sparkContext.appName}\")\n",
    "    \n",
    "    # Create test DataFrame\n",
    "    test_data = [(1, \"Alice\", 25), (2, \"Bob\", 30), (3, \"Charlie\", 35)]\n",
    "    columns = [\"id\", \"name\", \"age\"]\n",
    "    df = spark.createDataFrame(test_data, columns)\n",
    "    \n",
    "    print(\"   âœ… Test DataFrame created\")\n",
    "    \n",
    "    # Test basic operations\n",
    "    row_count = df.count()\n",
    "    avg_age = df.agg(spark_sum(\"age\")).collect()[0][0] / row_count\n",
    "    \n",
    "    print(f\"   âœ… Basic operations work - {row_count} rows, avg age: {avg_age}\")\n",
    "    \n",
    "    # Display sample data\n",
    "    print(\"\\nðŸ“Š Sample DataFrame:\")\n",
    "    df.show()\n",
    "    \n",
    "    print(\"\\nðŸŽ‰ PySpark is working perfectly!\")\n",
    "    print(\"ðŸš€ You're ready to start the PySpark tutorial!\")\n",
    "    \n",
    "    # Clean up\n",
    "    spark.stop()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error testing PySpark: {str(e)}\")\n",
    "    print(\"\\nðŸ”§ Troubleshooting suggestions:\")\n",
    "    print(\"   1. Restart the runtime: Runtime > Restart runtime\")\n",
    "    print(\"   2. Re-run all cells in this notebook\")\n",
    "    print(\"   3. Check the troubleshooting section below\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afc1119",
   "metadata": {},
   "source": [
    "## ðŸ”§ Troubleshooting Guide\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "#### 1. Java Not Found Error\n",
    "```\n",
    "Error: JAVA_HOME is not set\n",
    "```\n",
    "**Solution**: Re-run the Java installation cell above\n",
    "\n",
    "#### 2. PySpark Import Error\n",
    "```\n",
    "ModuleNotFoundError: No module named 'pyspark'\n",
    "```\n",
    "**Solution**: \n",
    "- Restart runtime: `Runtime > Restart runtime`\n",
    "- Re-run installation cells\n",
    "\n",
    "#### 3. Memory Issues\n",
    "```\n",
    "OutOfMemoryError or session crashes\n",
    "```\n",
    "**Solution**: Use smaller datasets or reduce Spark memory settings\n",
    "\n",
    "#### 4. Session Timeout\n",
    "```\n",
    "Session disconnected after inactivity\n",
    "```\n",
    "**Solution**: Re-run this setup notebook and continue from where you left off\n",
    "\n",
    "### ðŸ“ž Getting Help\n",
    "- Check the main tutorial README for more troubleshooting tips\n",
    "- Open an issue on GitHub if problems persist\n",
    "- Use the Colab \"Help\" menu for Colab-specific issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2274bf5f",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Next Steps\n",
    "\n",
    "### âœ… Setup Complete!\n",
    "\n",
    "You can now proceed to any PySpark tutorial module:\n",
    "\n",
    "1. **[Module 1: Foundation & Setup](01_pyspark_foundation_setup.ipynb)** - Start here for basics\n",
    "2. **[Module 2: DataFrame Operations](02_dataframe_operations.ipynb)** - Core data operations\n",
    "3. **[Module 6: Machine Learning](06_machine_learning_mllib.ipynb)** - ML with PySpark\n",
    "4. **[Module 10: End-to-End Project](10_end_to_end_project.ipynb)** - Complete project\n",
    "\n",
    "### ðŸ’¡ Pro Tips for Colab:\n",
    "\n",
    "1. **Save frequently**: Colab sessions can timeout\n",
    "2. **Use smaller datasets**: Colab has memory limitations\n",
    "3. **Monitor resources**: Check RAM/Disk usage in the sidebar\n",
    "4. **Keep this setup handy**: Bookmark this notebook for future sessions\n",
    "\n",
    "### ðŸš€ Happy Learning!\n",
    "\n",
    "You're all set to master PySpark with this comprehensive tutorial series!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
