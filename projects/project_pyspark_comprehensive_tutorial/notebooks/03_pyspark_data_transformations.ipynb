{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86f549ec",
   "metadata": {},
   "source": [
    "# Module 3: Data Transformations & Operations\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this module, you will master:\n",
    "\n",
    "### ðŸ”„ **Core Transformations**\n",
    "- DataFrame column operations and manipulations\n",
    "- Row filtering and conditional logic\n",
    "- Data type conversions and casting\n",
    "- String manipulations and text processing\n",
    "\n",
    "### ðŸ“Š **Advanced Operations**\n",
    "- Aggregations and grouping operations\n",
    "- Window functions for analytics\n",
    "- Join operations and data combining\n",
    "- Set operations (union, intersect, except)\n",
    "\n",
    "### ðŸ§¹ **Data Cleaning & Preprocessing**\n",
    "- Handling null values and missing data\n",
    "- Data deduplication and uniqueness\n",
    "- Data validation and quality checks\n",
    "- Outlier detection and handling\n",
    "\n",
    "### âš¡ **Performance & Optimization**\n",
    "- Caching and persistence strategies\n",
    "- Partitioning for transformations\n",
    "- Broadcast variables and accumulators\n",
    "- Catalyst optimizer understanding\n",
    "\n",
    "### ðŸ› ï¸ **Real-World Applications**\n",
    "- ETL pipeline patterns\n",
    "- Data quality frameworks\n",
    "- Complex business logic implementation\n",
    "- Performance monitoring and tuning\n",
    "\n",
    "**Prerequisites:** Module 1 (Foundation) and Module 2 (Data I/O)  \n",
    "**Estimated Time:** 90-120 minutes  \n",
    "**Difficulty:** Intermediate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde4b713",
   "metadata": {},
   "source": [
    "## 3.1 Overview: Data Transformations in PySpark\n",
    "\n",
    "Data transformations are the heart of any data processing pipeline. In PySpark, transformations are **lazy operations** that define what you want to do with your data, but don't execute until an **action** is called.\n",
    "\n",
    "### ðŸ”„ **Transformation Types**\n",
    "\n",
    "**1. Narrow Transformations**\n",
    "- Operations where each input partition contributes to only one output partition\n",
    "- No data shuffle required across the cluster\n",
    "- Examples: `filter()`, `map()`, `select()`, `withColumn()`\n",
    "- **Performance**: Fast, highly parallelizable\n",
    "\n",
    "**2. Wide Transformations**\n",
    "- Operations that require data from multiple partitions\n",
    "- Trigger shuffle operations across the cluster\n",
    "- Examples: `groupBy()`, `join()`, `orderBy()`, `distinct()`\n",
    "- **Performance**: More expensive, but often necessary\n",
    "\n",
    "### ðŸ“Š **DataFrame API vs SQL**\n",
    "\n",
    "PySpark provides two equivalent ways to transform data:\n",
    "- **DataFrame API**: Programmatic, type-safe, chainable operations\n",
    "- **SQL Interface**: Familiar SQL syntax for complex queries\n",
    "- **Interchangeable**: Can mix both approaches seamlessly\n",
    "\n",
    "### âš¡ **Lazy Evaluation Benefits**\n",
    "\n",
    "1. **Query Optimization**: Catalyst optimizer can analyze the entire pipeline\n",
    "2. **Efficient Execution**: Combines multiple operations into optimized stages\n",
    "3. **Memory Management**: Only computes what's needed when needed\n",
    "4. **Fault Tolerance**: Can replay transformations if nodes fail\n",
    "\n",
    "### ðŸŽ¯ **Key Concepts We'll Explore**\n",
    "\n",
    "- **Immutability**: DataFrames are immutable; transformations create new DataFrames\n",
    "- **Lineage**: Spark tracks the transformation graph for fault tolerance\n",
    "- **Partitioning**: How data distribution affects transformation performance\n",
    "- **Caching**: When and how to persist intermediate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c684493",
   "metadata": {},
   "source": [
    "## 3.2 Environment Setup for Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e7b758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Module 3: Data Transformations & Operations\n",
      "==================================================\n",
      "ðŸ“¦ Conda Environment: pyspark_env\n",
      "ðŸ“ Directory ready: data/\n",
      "ðŸ“ Directory ready: temp/\n",
      "ðŸ“ Directory ready: outputs/\n",
      "âœ… PySpark imports successful\n",
      "âœ… Additional libraries imported (pandas, numpy)\n",
      "\n",
      "ðŸŽ¯ Environment Ready for Module 3!\n",
      "   â€¢ Project root: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/notebooks\n",
      "   â€¢ Data directory: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/notebooks/data\n",
      "   â€¢ Temp directory: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/notebooks/temp\n",
      "   â€¢ Random seeds set for reproducibility\n",
      "   â€¢ Ready for transformation operations!\n",
      "âœ… Additional libraries imported (pandas, numpy)\n",
      "\n",
      "ðŸŽ¯ Environment Ready for Module 3!\n",
      "   â€¢ Project root: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/notebooks\n",
      "   â€¢ Data directory: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/notebooks/data\n",
      "   â€¢ Temp directory: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/notebooks/temp\n",
      "   â€¢ Random seeds set for reproducibility\n",
      "   â€¢ Ready for transformation operations!\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup and Verification for Module 3: Data Transformations\n",
    "print(\"ðŸš€ Module 3: Data Transformations & Operations\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Environment verification\n",
    "conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'Not in conda environment')\n",
    "print(f\"ðŸ“¦ Conda Environment: {conda_env}\")\n",
    "\n",
    "# Set up project paths\n",
    "project_root = Path.cwd()\n",
    "data_dir = project_root / \"data\"\n",
    "temp_dir = project_root / \"temp\"\n",
    "outputs_dir = project_root / \"outputs\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [data_dir, temp_dir, outputs_dir]:\n",
    "    directory.mkdir(exist_ok=True)\n",
    "    print(f\"ðŸ“ Directory ready: {directory.name}/\")\n",
    "\n",
    "# PySpark imports for transformations\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import *\n",
    "    from pyspark.sql.types import *\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark import SparkConf, SparkContext\n",
    "    \n",
    "    print(f\"âœ… PySpark imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ PySpark import error: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Additional imports for transformations\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    print(f\"âœ… Additional libraries imported (pandas, numpy)\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸  Optional libraries not available: {e}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "if 'np' in locals():\n",
    "    np.random.seed(42)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Environment Ready for Module 3!\")\n",
    "print(f\"   â€¢ Project root: {project_root}\")\n",
    "print(f\"   â€¢ Data directory: {data_dir}\")\n",
    "print(f\"   â€¢ Temp directory: {temp_dir}\")\n",
    "print(f\"   â€¢ Random seeds set for reproducibility\")\n",
    "print(f\"   â€¢ Ready for transformation operations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de5815f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ Creating SparkSession for Data Transformations\n",
      "==================================================\n",
      "ðŸ†• No existing SparkSession to stop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/25 20:28:02 WARN Utils: Your hostname, Sanjeevas-iMac.local, resolves to a loopback address: 127.0.0.1; using 192.168.12.128 instead (on interface en1)\n",
      "25/08/25 20:28:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/25 20:28:02 WARN Utils: Your hostname, Sanjeevas-iMac.local, resolves to a loopback address: 127.0.0.1; using 192.168.12.128 instead (on interface en1)\n",
      "25/08/25 20:28:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/25 20:28:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/25 20:28:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/25 20:28:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/08/25 20:28:03 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/08/25 20:28:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/08/25 20:28:03 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… SparkSession created successfully!\n",
      "ðŸ“± Application Name: PySpark-Tutorial-Module3-Transformations\n",
      "ðŸ”¢ Spark Version: 4.0.0\n",
      "ðŸŽ¯ Master: local[6]\n",
      "ðŸ’¾ Driver Memory: 4g\n",
      "âš¡ Default Parallelism: 6\n",
      "ðŸ”€ Shuffle Partitions: 12\n",
      "\n",
      "ðŸ”§ Transformation Optimizations:\n",
      "   â€¢ Adaptive Query Execution: true\n",
      "   â€¢ Whole-Stage Code Generation: true\n",
      "   â€¢ Skew Join Optimization: true\n",
      "   â€¢ Arrow Optimization: true\n",
      "\n",
      "ðŸŒ Spark UI: http://192.168.12.128:4042\n",
      "\n",
      "ðŸŽ¯ Optimized for:\n",
      "   â€¢ Complex data transformations\n",
      "   â€¢ Join operations and aggregations\n",
      "   â€¢ Window functions and analytics\n",
      "   â€¢ Local development with 6 cores\n",
      "ðŸ’¾ Driver Memory: 4g\n",
      "âš¡ Default Parallelism: 6\n",
      "ðŸ”€ Shuffle Partitions: 12\n",
      "\n",
      "ðŸ”§ Transformation Optimizations:\n",
      "   â€¢ Adaptive Query Execution: true\n",
      "   â€¢ Whole-Stage Code Generation: true\n",
      "   â€¢ Skew Join Optimization: true\n",
      "   â€¢ Arrow Optimization: true\n",
      "\n",
      "ðŸŒ Spark UI: http://192.168.12.128:4042\n",
      "\n",
      "ðŸŽ¯ Optimized for:\n",
      "   â€¢ Complex data transformations\n",
      "   â€¢ Join operations and aggregations\n",
      "   â€¢ Window functions and analytics\n",
      "   â€¢ Local development with 6 cores\n",
      "+--------------------+\n",
      "|              status|\n",
      "+--------------------+\n",
      "|SQL interface ready!|\n",
      "+--------------------+\n",
      "\n",
      "âœ… SQL interface verified and ready!\n",
      "+--------------------+\n",
      "|              status|\n",
      "+--------------------+\n",
      "|SQL interface ready!|\n",
      "+--------------------+\n",
      "\n",
      "âœ… SQL interface verified and ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create SparkSession optimized for Data Transformations\n",
    "print(\"âš¡ Creating SparkSession for Data Transformations\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Stop any existing SparkSession\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"ðŸ§¹ Stopped existing SparkSession\")\n",
    "except:\n",
    "    print(\"ðŸ†• No existing SparkSession to stop\")\n",
    "\n",
    "# Configuration optimized for transformations and analytics\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark-Tutorial-Module3-Transformations\") \\\n",
    "    .master(\"local[6]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.minPartitionSize\", \"16MB\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"12\") \\\n",
    "    .config(\"spark.sql.codegen.wholeStage\", \"true\") \\\n",
    "    .config(\"spark.sql.codegen.maxFields\", \"200\") \\\n",
    "    .config(\"spark.sql.join.preferSortMergeJoin\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"64MB\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Get SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Set log level to reduce noise\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"\\nâœ… SparkSession created successfully!\")\n",
    "print(f\"ðŸ“± Application Name: {spark.sparkContext.appName}\")\n",
    "print(f\"ðŸ”¢ Spark Version: {spark.version}\")\n",
    "print(f\"ðŸŽ¯ Master: {spark.sparkContext.master}\")\n",
    "print(f\"ðŸ’¾ Driver Memory: {spark.conf.get('spark.driver.memory')}\")\n",
    "print(f\"âš¡ Default Parallelism: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"ðŸ”€ Shuffle Partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "\n",
    "# Transformation-specific optimizations\n",
    "print(f\"\\nðŸ”§ Transformation Optimizations:\")\n",
    "print(f\"   â€¢ Adaptive Query Execution: {spark.conf.get('spark.sql.adaptive.enabled')}\")\n",
    "print(f\"   â€¢ Whole-Stage Code Generation: {spark.conf.get('spark.sql.codegen.wholeStage')}\")\n",
    "print(f\"   â€¢ Skew Join Optimization: {spark.conf.get('spark.sql.adaptive.skewJoin.enabled')}\")\n",
    "print(f\"   â€¢ Arrow Optimization: {spark.conf.get('spark.sql.execution.arrow.pyspark.enabled')}\")\n",
    "\n",
    "if spark.sparkContext.uiWebUrl:\n",
    "    print(f\"\\nðŸŒ Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Optimized for:\")\n",
    "print(f\"   â€¢ Complex data transformations\")\n",
    "print(f\"   â€¢ Join operations and aggregations\")\n",
    "print(f\"   â€¢ Window functions and analytics\")\n",
    "print(f\"   â€¢ Local development with 6 cores\")\n",
    "\n",
    "# Enable SQL interface\n",
    "spark.sql(\"SELECT 'SQL interface ready!' as status\").show()\n",
    "print(\"âœ… SQL interface verified and ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "325c23dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Creating Sample Datasets for Transformations\n",
      "================================================\n",
      "1ï¸âƒ£ Creating Sales Transaction Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created sales dataset: 1000 records\n",
      "\n",
      "2ï¸âƒ£ Creating Customer Dataset...\n",
      "âœ… Created customer dataset: 10 records\n",
      "\n",
      "3ï¸âƒ£ Creating Product Information Dataset...\n",
      "âœ… Created product dataset: 10 records\n",
      "\n",
      "ðŸ“„ Sample Data Preview:\n",
      "\n",
      "ðŸ›’ Sales Data (first 3 rows):\n",
      "+--------------+----------+----------+--------------+-----------+----------+--------+------------+-------+-----------+---------+------+\n",
      "|transaction_id|sale_date |product_id|product_name  |category   |unit_price|quantity|total_amount|region |sales_rep  |is_online|rating|\n",
      "+--------------+----------+----------+--------------+-----------+----------+--------+------------+-------+-----------+---------+------+\n",
      "|T000001       |2024-01-13|P002      |Wireless Mouse|Electronics|29.99     |5       |119.96      |South  |Bob Smith  |true     |3.3   |\n",
      "|T000002       |2024-01-16|P001      |Laptop Pro    |Electronics|1299.99   |2       |5199.96     |South  |Eve Brown  |true     |3.8   |\n",
      "|T000003       |2024-08-02|P009      |Standing Desk |Furniture  |449.99    |4       |3599.92     |Central|Carol Davis|true     |4.0   |\n",
      "+--------------+----------+----------+--------------+-----------+----------+--------+------------+-------+-----------+---------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "ðŸ‘¥ Customer Data (first 3 rows):\n",
      "âœ… Created product dataset: 10 records\n",
      "\n",
      "ðŸ“„ Sample Data Preview:\n",
      "\n",
      "ðŸ›’ Sales Data (first 3 rows):\n",
      "+--------------+----------+----------+--------------+-----------+----------+--------+------------+-------+-----------+---------+------+\n",
      "|transaction_id|sale_date |product_id|product_name  |category   |unit_price|quantity|total_amount|region |sales_rep  |is_online|rating|\n",
      "+--------------+----------+----------+--------------+-----------+----------+--------+------------+-------+-----------+---------+------+\n",
      "|T000001       |2024-01-13|P002      |Wireless Mouse|Electronics|29.99     |5       |119.96      |South  |Bob Smith  |true     |3.3   |\n",
      "|T000002       |2024-01-16|P001      |Laptop Pro    |Electronics|1299.99   |2       |5199.96     |South  |Eve Brown  |true     |3.8   |\n",
      "|T000003       |2024-08-02|P009      |Standing Desk |Furniture  |449.99    |4       |3599.92     |Central|Carol Davis|true     |4.0   |\n",
      "+--------------+----------+----------+--------------+-----------+----------+--------+------------+-------+-----------+---------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "ðŸ‘¥ Customer Data (first 3 rows):\n",
      "+-----------+----------+---------+---------------------+--------+-----------+----------+------+\n",
      "|customer_id|first_name|last_name|email                |tier    |total_spent|join_date |region|\n",
      "+-----------+----------+---------+---------------------+--------+-----------+----------+------+\n",
      "|C001       |John      |Doe      |john.doe@email.com   |Premium |25000.5    |2022-01-15|North |\n",
      "|C002       |Jane      |Smith    |jane.smith@email.com |Standard|12000.75   |2022-03-22|South |\n",
      "|C003       |Bob       |Johnson  |bob.johnson@email.com|Premium |35000.0    |2021-11-08|East  |\n",
      "+-----------+----------+---------+---------------------+--------+-----------+----------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "ðŸ“¦ Product Data (first 3 rows):\n",
      "+----------+--------------+-----------+-------+--------------+-----------+---------+-------------------------------+\n",
      "|product_id|product_name  |category   |price  |stock_quantity|launch_date|is_active|tags                           |\n",
      "+----------+--------------+-----------+-------+--------------+-----------+---------+-------------------------------+\n",
      "|P001      |Laptop Pro    |Electronics|1299.99|50            |2024-01-01 |true     |[laptop, computer, electronics]|\n",
      "|P002      |Wireless Mouse|Electronics|29.99  |200           |2024-01-01 |true     |[mouse, wireless, accessories] |\n",
      "|P003      |Office Chair  |Furniture  |299.99 |30            |2024-01-01 |true     |[chair, office, furniture]     |\n",
      "+----------+--------------+-----------+-------+--------------+-----------+---------+-------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "ðŸ“Š Datasets Summary:\n",
      "+-----------+----------+---------+---------------------+--------+-----------+----------+------+\n",
      "|customer_id|first_name|last_name|email                |tier    |total_spent|join_date |region|\n",
      "+-----------+----------+---------+---------------------+--------+-----------+----------+------+\n",
      "|C001       |John      |Doe      |john.doe@email.com   |Premium |25000.5    |2022-01-15|North |\n",
      "|C002       |Jane      |Smith    |jane.smith@email.com |Standard|12000.75   |2022-03-22|South |\n",
      "|C003       |Bob       |Johnson  |bob.johnson@email.com|Premium |35000.0    |2021-11-08|East  |\n",
      "+-----------+----------+---------+---------------------+--------+-----------+----------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "ðŸ“¦ Product Data (first 3 rows):\n",
      "+----------+--------------+-----------+-------+--------------+-----------+---------+-------------------------------+\n",
      "|product_id|product_name  |category   |price  |stock_quantity|launch_date|is_active|tags                           |\n",
      "+----------+--------------+-----------+-------+--------------+-----------+---------+-------------------------------+\n",
      "|P001      |Laptop Pro    |Electronics|1299.99|50            |2024-01-01 |true     |[laptop, computer, electronics]|\n",
      "|P002      |Wireless Mouse|Electronics|29.99  |200           |2024-01-01 |true     |[mouse, wireless, accessories] |\n",
      "|P003      |Office Chair  |Furniture  |299.99 |30            |2024-01-01 |true     |[chair, office, furniture]     |\n",
      "+----------+--------------+-----------+-------+--------------+-----------+---------+-------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "ðŸ“Š Datasets Summary:\n",
      "   â€¢ Sales transactions: 1000 records\n",
      "   â€¢ Customers: 10 records\n",
      "   â€¢ Products: 10 records\n",
      "   â€¢ Ready for transformation demonstrations!\n",
      "   â€¢ Sales transactions: 1000 records\n",
      "   â€¢ Customers: 10 records\n",
      "   â€¢ Products: 10 records\n",
      "   â€¢ Ready for transformation demonstrations!\n"
     ]
    }
   ],
   "source": [
    "# Create Sample Datasets for Transformation Demonstrations\n",
    "print(\"ðŸ“Š Creating Sample Datasets for Transformations\")\n",
    "print(\"=\" * 48)\n",
    "\n",
    "# 1. Sales Transaction Dataset\n",
    "print(\"1ï¸âƒ£ Creating Sales Transaction Dataset...\")\n",
    "\n",
    "def generate_sales_data(num_records=1000):\n",
    "    \"\"\"Generate realistic sales transaction data\"\"\"\n",
    "    products = [\n",
    "        (\"P001\", \"Laptop Pro\", \"Electronics\", 1299.99),\n",
    "        (\"P002\", \"Wireless Mouse\", \"Electronics\", 29.99),\n",
    "        (\"P003\", \"Office Chair\", \"Furniture\", 299.99),\n",
    "        (\"P004\", \"Coffee Maker\", \"Appliances\", 89.99),\n",
    "        (\"P005\", \"Smartphone\", \"Electronics\", 799.99),\n",
    "        (\"P006\", \"Desk Lamp\", \"Furniture\", 45.99),\n",
    "        (\"P007\", \"Tablet\", \"Electronics\", 599.99),\n",
    "        (\"P008\", \"Ergonomic Keyboard\", \"Electronics\", 79.99),\n",
    "        (\"P009\", \"Standing Desk\", \"Furniture\", 449.99),\n",
    "        (\"P010\", \"Blender\", \"Appliances\", 129.99)\n",
    "    ]\n",
    "    \n",
    "    regions = [\"North\", \"South\", \"East\", \"West\", \"Central\"]\n",
    "    sales_reps = [\"Alice Johnson\", \"Bob Smith\", \"Carol Davis\", \"David Wilson\", \"Eve Brown\"]\n",
    "    \n",
    "    data = []\n",
    "    base_date = datetime(2024, 1, 1)\n",
    "    \n",
    "    for i in range(num_records):\n",
    "        product = random.choice(products)\n",
    "        sale_date = base_date + timedelta(days=random.randint(0, 365))\n",
    "        \n",
    "        data.append((\n",
    "            f\"T{i+1:06d}\",  # transaction_id\n",
    "            sale_date.strftime('%Y-%m-%d'),  # sale_date\n",
    "            product[0],  # product_id\n",
    "            product[1],  # product_name\n",
    "            product[2],  # category\n",
    "            product[3],  # unit_price\n",
    "            random.randint(1, 10),  # quantity\n",
    "            product[3] * random.randint(1, 10),  # total_amount\n",
    "            random.choice(regions),  # region\n",
    "            random.choice(sales_reps),  # sales_rep\n",
    "            random.choice([True, False]) if random.random() > 0.1 else None,  # is_online (some nulls)\n",
    "            int(random.uniform(1.0, 5.0) * 10) / 10.0  # rating\n",
    "        ))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate sales data\n",
    "sales_data = generate_sales_data(1000)\n",
    "\n",
    "# Create DataFrame\n",
    "sales_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"sale_date\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"sales_rep\", StringType(), True),\n",
    "    StructField(\"is_online\", BooleanType(), True),\n",
    "    StructField(\"rating\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df_sales = spark.createDataFrame(sales_data, sales_schema)\n",
    "\n",
    "print(f\"âœ… Created sales dataset: {df_sales.count()} records\")\n",
    "\n",
    "# 2. Customer Dataset\n",
    "print(\"\\n2ï¸âƒ£ Creating Customer Dataset...\")\n",
    "\n",
    "customer_data = [\n",
    "    (\"C001\", \"John\", \"Doe\", \"john.doe@email.com\", \"Premium\", 25000.50, \"2022-01-15\", \"North\"),\n",
    "    (\"C002\", \"Jane\", \"Smith\", \"jane.smith@email.com\", \"Standard\", 12000.75, \"2022-03-22\", \"South\"),\n",
    "    (\"C003\", \"Bob\", \"Johnson\", \"bob.johnson@email.com\", \"Premium\", 35000.00, \"2021-11-08\", \"East\"),\n",
    "    (\"C004\", \"Alice\", \"Brown\", \"alice.brown@email.com\", \"Basic\", 5000.25, \"2023-02-14\", \"West\"),\n",
    "    (\"C005\", \"Charlie\", \"Wilson\", \"charlie.wilson@email.com\", \"Standard\", 18000.00, \"2022-09-05\", \"Central\"),\n",
    "    (\"C006\", \"Diana\", \"Davis\", \"\", \"Premium\", 42000.50, \"2021-07-12\", \"North\"),  # Missing email\n",
    "    (\"C007\", \"Eve\", \"Miller\", \"eve.miller@email.com\", \"Standard\", 15000.00, \"2023-01-30\", \"South\"),\n",
    "    (\"C008\", \"Frank\", \"Garcia\", \"frank.garcia@email.com\", \"Basic\", 3000.75, \"2023-05-20\", \"East\"),\n",
    "    (\"C009\", \"Grace\", \"Lee\", \"grace.lee@email.com\", \"Premium\", 55000.00, \"2020-08-17\", \"West\"),\n",
    "    (\"C010\", \"Henry\", \"Taylor\", \"henry.taylor@email.com\", \"Standard\", 22000.25, \"2022-04-25\", \"Central\")\n",
    "]\n",
    "\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"tier\", StringType(), True),\n",
    "    StructField(\"total_spent\", DoubleType(), True),\n",
    "    StructField(\"join_date\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_customers = spark.createDataFrame(customer_data, customer_schema)\n",
    "\n",
    "print(f\"âœ… Created customer dataset: {df_customers.count()} records\")\n",
    "\n",
    "# 3. Product Information Dataset\n",
    "print(\"\\n3ï¸âƒ£ Creating Product Information Dataset...\")\n",
    "\n",
    "product_data = [\n",
    "    (\"P001\", \"Laptop Pro\", \"Electronics\", 1299.99, 50, \"2024-01-01\", True, [\"laptop\", \"computer\", \"electronics\"]),\n",
    "    (\"P002\", \"Wireless Mouse\", \"Electronics\", 29.99, 200, \"2024-01-01\", True, [\"mouse\", \"wireless\", \"accessories\"]),\n",
    "    (\"P003\", \"Office Chair\", \"Furniture\", 299.99, 30, \"2024-01-01\", True, [\"chair\", \"office\", \"furniture\"]),\n",
    "    (\"P004\", \"Coffee Maker\", \"Appliances\", 89.99, 75, \"2024-01-01\", True, [\"coffee\", \"appliance\", \"kitchen\"]),\n",
    "    (\"P005\", \"Smartphone\", \"Electronics\", 799.99, 120, \"2024-01-01\", True, [\"phone\", \"smartphone\", \"mobile\"]),\n",
    "    (\"P006\", \"Desk Lamp\", \"Furniture\", 45.99, 80, \"2024-01-01\", False, [\"lamp\", \"desk\", \"lighting\"]),  # Discontinued\n",
    "    (\"P007\", \"Tablet\", \"Electronics\", 599.99, 60, \"2024-01-01\", True, [\"tablet\", \"device\", \"portable\"]),\n",
    "    (\"P008\", \"Ergonomic Keyboard\", \"Electronics\", 79.99, 90, \"2024-01-01\", True, [\"keyboard\", \"ergonomic\", \"input\"]),\n",
    "    (\"P009\", \"Standing Desk\", \"Furniture\", 449.99, 25, \"2024-01-01\", True, [\"desk\", \"standing\", \"office\"]),\n",
    "    (\"P010\", \"Blender\", \"Appliances\", 129.99, 40, \"2024-01-01\", True, [\"blender\", \"kitchen\", \"appliance\"])\n",
    "]\n",
    "\n",
    "product_schema = StructType([\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"stock_quantity\", IntegerType(), True),\n",
    "    StructField(\"launch_date\", StringType(), True),\n",
    "    StructField(\"is_active\", BooleanType(), True),\n",
    "    StructField(\"tags\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "df_products = spark.createDataFrame(product_data, product_schema)\n",
    "\n",
    "print(f\"âœ… Created product dataset: {df_products.count()} records\")\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\nðŸ“„ Sample Data Preview:\")\n",
    "print(f\"\\nðŸ›’ Sales Data (first 3 rows):\")\n",
    "df_sales.show(3, truncate=False)\n",
    "\n",
    "print(f\"\\nðŸ‘¥ Customer Data (first 3 rows):\")\n",
    "df_customers.show(3, truncate=False)\n",
    "\n",
    "print(f\"\\nðŸ“¦ Product Data (first 3 rows):\")\n",
    "df_products.show(3, truncate=False)\n",
    "\n",
    "print(f\"\\nðŸ“Š Datasets Summary:\")\n",
    "print(f\"   â€¢ Sales transactions: {df_sales.count()} records\")\n",
    "print(f\"   â€¢ Customers: {df_customers.count()} records\") \n",
    "print(f\"   â€¢ Products: {df_products.count()} records\")\n",
    "print(f\"   â€¢ Ready for transformation demonstrations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee508e75",
   "metadata": {},
   "source": [
    "## 3.3 Basic Column Operations\n",
    "\n",
    "Column operations are fundamental transformations in PySpark. They allow you to create new columns, modify existing ones, and perform calculations across your dataset.\n",
    "\n",
    "**Key Column Operations:**\n",
    "- **Selection**: Choose specific columns from a DataFrame\n",
    "- **Creation**: Add new columns with computed values\n",
    "- **Modification**: Transform existing column values\n",
    "- **Renaming**: Change column names for clarity\n",
    "- **Casting**: Convert between data types\n",
    "- **Conditional Logic**: Apply if-then-else logic to columns\n",
    "\n",
    "**Performance Notes:**\n",
    "- Column operations are **narrow transformations** (no shuffle required)\n",
    "- Multiple column operations can be chained efficiently\n",
    "- Use vectorized operations whenever possible for best performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d95614ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Basic Column Operations\n",
      "===========================\n",
      "1ï¸âƒ£ Column Selection\n",
      "ðŸ“‹ Selected columns: ['transaction_id', 'product_name', 'total_amount', 'region']\n",
      "+--------------+--------------+------------+-------+\n",
      "|transaction_id|  product_name|total_amount| region|\n",
      "+--------------+--------------+------------+-------+\n",
      "|       T000001|Wireless Mouse|      119.96|  South|\n",
      "|       T000002|    Laptop Pro|     5199.96|  South|\n",
      "|       T000003| Standing Desk|     3599.92|Central|\n",
      "+--------------+--------------+------------+-------+\n",
      "only showing top 3 rows\n",
      "\n",
      "ðŸ“‹ Selected with aliases:\n",
      "+-------+--------------+------------+------------+\n",
      "|     id|  product_name|total_amount|sales_region|\n",
      "+-------+--------------+------------+------------+\n",
      "|T000001|Wireless Mouse|      119.96|       South|\n",
      "|T000002|    Laptop Pro|     5199.96|       South|\n",
      "|T000003| Standing Desk|     3599.92|     Central|\n",
      "+-------+--------------+------------+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "2ï¸âƒ£ Adding New Columns\n",
      "ðŸ“Š Added financial calculations:\n",
      "+--------------+--------------+------------+-------+\n",
      "|transaction_id|  product_name|total_amount| region|\n",
      "+--------------+--------------+------------+-------+\n",
      "|       T000001|Wireless Mouse|      119.96|  South|\n",
      "|       T000002|    Laptop Pro|     5199.96|  South|\n",
      "|       T000003| Standing Desk|     3599.92|Central|\n",
      "+--------------+--------------+------------+-------+\n",
      "only showing top 3 rows\n",
      "\n",
      "ðŸ“‹ Selected with aliases:\n",
      "+-------+--------------+------------+------------+\n",
      "|     id|  product_name|total_amount|sales_region|\n",
      "+-------+--------------+------------+------------+\n",
      "|T000001|Wireless Mouse|      119.96|       South|\n",
      "|T000002|    Laptop Pro|     5199.96|       South|\n",
      "|T000003| Standing Desk|     3599.92|     Central|\n",
      "+-------+--------------+------------+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "2ï¸âƒ£ Adding New Columns\n",
      "ðŸ“Š Added financial calculations:\n",
      "+--------------+------------+---------------+----------------+------------------+------------------+\n",
      "|transaction_id|total_amount|discount_amount|discounted_total|        tax_amount|      final_amount|\n",
      "+--------------+------------+---------------+----------------+------------------+------------------+\n",
      "|       T000001|      119.96|         11.996|         107.964|           8.63712|         116.60112|\n",
      "|       T000002|     5199.96|        519.996|        4679.964|374.39712000000003|        5054.36112|\n",
      "|       T000003|     3599.92|        359.992|        3239.928|         259.19424|3499.1222399999997|\n",
      "+--------------+------------+---------------+----------------+------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "3ï¸âƒ£ String Operations\n",
      "+--------------+------------+---------------+----------------+------------------+------------------+\n",
      "|transaction_id|total_amount|discount_amount|discounted_total|        tax_amount|      final_amount|\n",
      "+--------------+------------+---------------+----------------+------------------+------------------+\n",
      "|       T000001|      119.96|         11.996|         107.964|           8.63712|         116.60112|\n",
      "|       T000002|     5199.96|        519.996|        4679.964|374.39712000000003|        5054.36112|\n",
      "|       T000003|     3599.92|        359.992|        3239.928|         259.19424|3499.1222399999997|\n",
      "+--------------+------------+---------------+----------------+------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "3ï¸âƒ£ String Operations\n",
      "ðŸ“ String transformations:\n",
      "ðŸ“ String transformations:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-----------+------------+----------------+--------+\n",
      "|customer_id|full_name     |name_length|email_domain|first_name_upper|initials|\n",
      "+-----------+--------------+-----------+------------+----------------+--------+\n",
      "|C001       |John Doe      |8          |email.com   |JOHN            |JD      |\n",
      "|C002       |Jane Smith    |10         |email.com   |JANE            |JS      |\n",
      "|C003       |Bob Johnson   |11         |email.com   |BOB             |BJ      |\n",
      "|C004       |Alice Brown   |11         |email.com   |ALICE           |AB      |\n",
      "|C005       |Charlie Wilson|14         |email.com   |CHARLIE         |CW      |\n",
      "+-----------+--------------+-----------+------------+----------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "4ï¸âƒ£ Date Operations\n",
      "ðŸ“… Date transformations:\n",
      "+--------------+----------+----+-----+-------+-----------+---------------+\n",
      "|transaction_id| sale_date|year|month|quarter|day_of_week|days_from_today|\n",
      "+--------------+----------+----+-----+-------+-----------+---------------+\n",
      "|       T000001|2024-01-13|2024|    1|      1|          7|            590|\n",
      "|       T000002|2024-01-16|2024|    1|      1|          3|            587|\n",
      "|       T000003|2024-08-02|2024|    8|      3|          6|            388|\n",
      "|       T000004|2024-12-23|2024|   12|      4|          2|            245|\n",
      "|       T000005|2024-02-19|2024|    2|      1|          2|            553|\n",
      "+--------------+----------+----+-----+-------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "5ï¸âƒ£ Conditional Logic\n",
      "ðŸŽ¯ Conditional transformations:\n",
      "+--------------+------------+---------------+---------------+-----------+---------------+\n",
      "|transaction_id|total_amount|amount_category|is_weekend_sale|   category|commission_rate|\n",
      "+--------------+------------+---------------+---------------+-----------+---------------+\n",
      "|       T000001|      119.96|            Low|           true|Electronics|           0.05|\n",
      "|       T000002|     5199.96|           High|          false|Electronics|           0.05|\n",
      "|       T000003|     3599.92|           High|          false|  Furniture|           0.03|\n",
      "|       T000004|     1799.94|           High|          false|  Furniture|           0.03|\n",
      "|       T000005|     3599.94|           High|          false|Electronics|           0.05|\n",
      "+--------------+------------+---------------+---------------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "6ï¸âƒ£ Type Casting\n",
      "ðŸ”„ Type casting examples:\n",
      "Original types: [('transaction_id', StringType()), ('sale_date', StringType()), ('product_id', StringType())]\n",
      "Casted types: [('quantity_str', StringType()), ('rating_int', IntegerType()), ('unit_price_float', FloatType())]\n",
      "ðŸŽ¯ Conditional transformations:\n",
      "+--------------+------------+---------------+---------------+-----------+---------------+\n",
      "|transaction_id|total_amount|amount_category|is_weekend_sale|   category|commission_rate|\n",
      "+--------------+------------+---------------+---------------+-----------+---------------+\n",
      "|       T000001|      119.96|            Low|           true|Electronics|           0.05|\n",
      "|       T000002|     5199.96|           High|          false|Electronics|           0.05|\n",
      "|       T000003|     3599.92|           High|          false|  Furniture|           0.03|\n",
      "|       T000004|     1799.94|           High|          false|  Furniture|           0.03|\n",
      "|       T000005|     3599.94|           High|          false|Electronics|           0.05|\n",
      "+--------------+------------+---------------+---------------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "6ï¸âƒ£ Type Casting\n",
      "ðŸ”„ Type casting examples:\n",
      "Original types: [('transaction_id', StringType()), ('sale_date', StringType()), ('product_id', StringType())]\n",
      "Casted types: [('quantity_str', StringType()), ('rating_int', IntegerType()), ('unit_price_float', FloatType())]\n",
      "+--------+------------+------+----------+----------+----------------+\n",
      "|quantity|quantity_str|rating|rating_int|unit_price|unit_price_float|\n",
      "+--------+------------+------+----------+----------+----------------+\n",
      "|       5|           5|   3.3|         3|     29.99|           29.99|\n",
      "|       2|           2|   3.8|         3|   1299.99|         1299.99|\n",
      "|       4|           4|   4.0|         4|    449.99|          449.99|\n",
      "+--------+------------+------+----------+----------+----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "ðŸ“Š Column Operations Summary:\n",
      "   â€¢ Selection: Choose specific columns from DataFrames\n",
      "   â€¢ Addition: Create new columns with withColumn()\n",
      "   â€¢ String ops: concat(), upper(), split(), substring()\n",
      "   â€¢ Date ops: year(), month(), datediff(), current_date()\n",
      "   â€¢ Conditional: when().otherwise() for if-then-else logic\n",
      "   â€¢ Casting: Change data types with cast()\n",
      "   â€¢ All operations are lazy and optimized by Catalyst!\n",
      "+--------+------------+------+----------+----------+----------------+\n",
      "|quantity|quantity_str|rating|rating_int|unit_price|unit_price_float|\n",
      "+--------+------------+------+----------+----------+----------------+\n",
      "|       5|           5|   3.3|         3|     29.99|           29.99|\n",
      "|       2|           2|   3.8|         3|   1299.99|         1299.99|\n",
      "|       4|           4|   4.0|         4|    449.99|          449.99|\n",
      "+--------+------------+------+----------+----------+----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "ðŸ“Š Column Operations Summary:\n",
      "   â€¢ Selection: Choose specific columns from DataFrames\n",
      "   â€¢ Addition: Create new columns with withColumn()\n",
      "   â€¢ String ops: concat(), upper(), split(), substring()\n",
      "   â€¢ Date ops: year(), month(), datediff(), current_date()\n",
      "   â€¢ Conditional: when().otherwise() for if-then-else logic\n",
      "   â€¢ Casting: Change data types with cast()\n",
      "   â€¢ All operations are lazy and optimized by Catalyst!\n"
     ]
    }
   ],
   "source": [
    "# Basic Column Operations and Transformations\n",
    "print(\"ðŸ”§ Basic Column Operations\")\n",
    "print(\"=\" * 27)\n",
    "\n",
    "# 1. Column Selection\n",
    "print(\"1ï¸âƒ£ Column Selection\")\n",
    "\n",
    "# Select specific columns\n",
    "df_selected = df_sales.select(\"transaction_id\", \"product_name\", \"total_amount\", \"region\")\n",
    "print(f\"ðŸ“‹ Selected columns: {df_selected.columns}\")\n",
    "df_selected.show(3)\n",
    "\n",
    "# Select with column expressions\n",
    "df_selected_expr = df_sales.select(\n",
    "    col(\"transaction_id\").alias(\"id\"),\n",
    "    col(\"product_name\"),\n",
    "    col(\"total_amount\"),\n",
    "    col(\"region\").alias(\"sales_region\")\n",
    ")\n",
    "print(f\"\\nðŸ“‹ Selected with aliases:\")\n",
    "df_selected_expr.show(3)\n",
    "\n",
    "# 2. Adding New Columns\n",
    "print(f\"\\n2ï¸âƒ£ Adding New Columns\")\n",
    "\n",
    "# Add calculated columns\n",
    "df_with_calculations = df_sales.withColumn(\"discount_amount\", col(\"total_amount\") * 0.1) \\\n",
    "                              .withColumn(\"discounted_total\", col(\"total_amount\") - col(\"discount_amount\")) \\\n",
    "                              .withColumn(\"tax_amount\", col(\"discounted_total\") * 0.08) \\\n",
    "                              .withColumn(\"final_amount\", col(\"discounted_total\") + col(\"tax_amount\"))\n",
    "\n",
    "print(f\"ðŸ“Š Added financial calculations:\")\n",
    "df_with_calculations.select(\"transaction_id\", \"total_amount\", \"discount_amount\", \n",
    "                           \"discounted_total\", \"tax_amount\", \"final_amount\").show(3)\n",
    "\n",
    "# 3. String Operations\n",
    "print(f\"\\n3ï¸âƒ£ String Operations\")\n",
    "\n",
    "# String manipulations\n",
    "df_string_ops = df_customers.withColumn(\"full_name\", concat(col(\"first_name\"), lit(\" \"), col(\"last_name\"))) \\\n",
    "                           .withColumn(\"name_length\", length(col(\"full_name\"))) \\\n",
    "                           .withColumn(\"email_domain\", \n",
    "                                     when(col(\"email\").isNotNull() & (col(\"email\") != \"\"), \n",
    "                                          split(col(\"email\"), \"@\").getItem(1))\n",
    "                                     .otherwise(None)) \\\n",
    "                           .withColumn(\"first_name_upper\", upper(col(\"first_name\"))) \\\n",
    "                           .withColumn(\"initials\", concat(substring(col(\"first_name\"), 1, 1), \n",
    "                                                        substring(col(\"last_name\"), 1, 1)))\n",
    "\n",
    "print(f\"ðŸ“ String transformations:\")\n",
    "df_string_ops.select(\"customer_id\", \"full_name\", \"name_length\", \"email_domain\", \n",
    "                    \"first_name_upper\", \"initials\").show(5, truncate=False)\n",
    "\n",
    "# 4. Date Operations\n",
    "print(f\"\\n4ï¸âƒ£ Date Operations\")\n",
    "\n",
    "# Convert string dates to date type and perform date calculations\n",
    "df_date_ops = df_sales.withColumn(\"sale_date_typed\", to_date(col(\"sale_date\"), \"yyyy-MM-dd\")) \\\n",
    "                     .withColumn(\"year\", year(col(\"sale_date_typed\"))) \\\n",
    "                     .withColumn(\"month\", month(col(\"sale_date_typed\"))) \\\n",
    "                     .withColumn(\"quarter\", quarter(col(\"sale_date_typed\"))) \\\n",
    "                     .withColumn(\"day_of_week\", dayofweek(col(\"sale_date_typed\"))) \\\n",
    "                     .withColumn(\"days_from_today\", datediff(current_date(), col(\"sale_date_typed\")))\n",
    "\n",
    "print(f\"ðŸ“… Date transformations:\")\n",
    "df_date_ops.select(\"transaction_id\", \"sale_date\", \"year\", \"month\", \"quarter\", \n",
    "                  \"day_of_week\", \"days_from_today\").show(5)\n",
    "\n",
    "# 5. Conditional Logic\n",
    "print(f\"\\n5ï¸âƒ£ Conditional Logic\")\n",
    "\n",
    "# Using when() for conditional logic\n",
    "df_conditional = df_sales.withColumn(\"amount_category\", \n",
    "                                   when(col(\"total_amount\") >= 1000, \"High\")\n",
    "                                   .when(col(\"total_amount\") >= 500, \"Medium\")\n",
    "                                   .when(col(\"total_amount\") >= 100, \"Low\")\n",
    "                                   .otherwise(\"Very Low\")) \\\n",
    "                        .withColumn(\"is_weekend_sale\", \n",
    "                                  when(dayofweek(to_date(col(\"sale_date\"), \"yyyy-MM-dd\")).isin([1, 7]), True)\n",
    "                                  .otherwise(False)) \\\n",
    "                        .withColumn(\"commission_rate\",\n",
    "                                  when(col(\"category\") == \"Electronics\", 0.05)\n",
    "                                  .when(col(\"category\") == \"Furniture\", 0.03)\n",
    "                                  .otherwise(0.02))\n",
    "\n",
    "print(f\"ðŸŽ¯ Conditional transformations:\")\n",
    "df_conditional.select(\"transaction_id\", \"total_amount\", \"amount_category\", \n",
    "                     \"is_weekend_sale\", \"category\", \"commission_rate\").show(5)\n",
    "\n",
    "# 6. Type Casting\n",
    "print(f\"\\n6ï¸âƒ£ Type Casting\")\n",
    "\n",
    "# Data type conversions\n",
    "df_cast = df_sales.withColumn(\"quantity_str\", col(\"quantity\").cast(StringType())) \\\n",
    "                 .withColumn(\"rating_int\", col(\"rating\").cast(IntegerType())) \\\n",
    "                 .withColumn(\"unit_price_float\", col(\"unit_price\").cast(FloatType()))\n",
    "\n",
    "print(f\"ðŸ”„ Type casting examples:\")\n",
    "print(f\"Original types: {[(field.name, field.dataType) for field in df_sales.schema.fields[:3]]}\")\n",
    "print(f\"Casted types: {[(field.name, field.dataType) for field in df_cast.select('quantity_str', 'rating_int', 'unit_price_float').schema.fields]}\")\n",
    "\n",
    "df_cast.select(\"quantity\", \"quantity_str\", \"rating\", \"rating_int\", \"unit_price\", \"unit_price_float\").show(3)\n",
    "\n",
    "print(f\"\\nðŸ“Š Column Operations Summary:\")\n",
    "print(f\"   â€¢ Selection: Choose specific columns from DataFrames\")\n",
    "print(f\"   â€¢ Addition: Create new columns with withColumn()\")\n",
    "print(f\"   â€¢ String ops: concat(), upper(), split(), substring()\")\n",
    "print(f\"   â€¢ Date ops: year(), month(), datediff(), current_date()\")\n",
    "print(f\"   â€¢ Conditional: when().otherwise() for if-then-else logic\")\n",
    "print(f\"   â€¢ Casting: Change data types with cast()\")\n",
    "print(f\"   â€¢ All operations are lazy and optimized by Catalyst!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd04408c",
   "metadata": {},
   "source": [
    "## 3.4 Filtering & Conditional Operations\n",
    "\n",
    "Filtering is essential for data analysis, allowing you to work with subsets of data that meet specific criteria. PySpark provides powerful filtering capabilities with optimized predicate pushdown.\n",
    "\n",
    "**Key Filtering Concepts:**\n",
    "- **Row Filtering**: Select rows based on conditions\n",
    "- **Compound Conditions**: Combine multiple filters with AND/OR logic\n",
    "- **Null Handling**: Deal with missing values in filters\n",
    "- **Performance**: Catalyst optimizer pushes filters down to data source\n",
    "- **SQL Equivalence**: Filter operations map directly to SQL WHERE clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30ce9c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Filtering & Conditional Operations\n",
      "====================================\n",
      "1ï¸âƒ£ Basic Filtering\n",
      "ðŸ“Š Filter Results:\n",
      "   â€¢ High value sales (>$500): 642 records\n",
      "   â€¢ Electronics sales: 526 records\n",
      "   â€¢ Recent sales (since June): 591 records\n",
      "+--------------+-------------+------------+-----------+\n",
      "|transaction_id| product_name|total_amount|   category|\n",
      "+--------------+-------------+------------+-----------+\n",
      "|       T000002|   Laptop Pro|     5199.96|Electronics|\n",
      "|       T000003|Standing Desk|     3599.92|  Furniture|\n",
      "|       T000004| Office Chair|     1799.94|  Furniture|\n",
      "+--------------+-------------+------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "2ï¸âƒ£ Compound Conditions (AND/OR Logic)\n",
      "ðŸ“Š Compound Filter Results:\n",
      "   â€¢ Electronics sales: 526 records\n",
      "   â€¢ Recent sales (since June): 591 records\n",
      "+--------------+-------------+------------+-----------+\n",
      "|transaction_id| product_name|total_amount|   category|\n",
      "+--------------+-------------+------------+-----------+\n",
      "|       T000002|   Laptop Pro|     5199.96|Electronics|\n",
      "|       T000003|Standing Desk|     3599.92|  Furniture|\n",
      "|       T000004| Office Chair|     1799.94|  Furniture|\n",
      "+--------------+-------------+------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "2ï¸âƒ£ Compound Conditions (AND/OR Logic)\n",
      "ðŸ“Š Compound Filter Results:\n",
      "   â€¢ High-value Electronics (>$300, ratingâ‰¥4.0): 95 records\n",
      "   â€¢ Furniture OR Appliances: 474 records\n",
      "   â€¢ High-value Electronics (>$300, ratingâ‰¥4.0): 95 records\n",
      "   â€¢ Furniture OR Appliances: 474 records\n",
      "   â€¢ Complex condition: 392 records\n",
      "\n",
      "3ï¸âƒ£ String Filtering\n",
      "ðŸ“ String Filter Results:\n",
      "   â€¢ Products containing 'Pro': 86 records\n",
      "   â€¢ Complex condition: 392 records\n",
      "\n",
      "3ï¸âƒ£ String Filtering\n",
      "ðŸ“ String Filter Results:\n",
      "   â€¢ Products containing 'Pro': 86 records\n",
      "   â€¢ Products starting with 'Wireless': 104 records\n",
      "   â€¢ Products ending with 'Keyboard': 103 records\n",
      "   â€¢ Products matching laptop/phone pattern: 193 records\n",
      "   â€¢ Products starting with 'Wireless': 104 records\n",
      "   â€¢ Products ending with 'Keyboard': 103 records\n",
      "   â€¢ Products matching laptop/phone pattern: 193 records\n",
      "+------------+-----------+------------------+\n",
      "|product_name|category   |total_amount      |\n",
      "+------------+-----------+------------------+\n",
      "|Laptop Pro  |Electronics|2599.98           |\n",
      "|Laptop Pro  |Electronics|1299.99           |\n",
      "|Laptop Pro  |Electronics|3899.9700000000003|\n",
      "|Laptop Pro  |Electronics|9099.93           |\n",
      "|Laptop Pro  |Electronics|7799.9400000000005|\n",
      "|Laptop Pro  |Electronics|12999.9           |\n",
      "|Laptop Pro  |Electronics|11699.91          |\n",
      "|Laptop Pro  |Electronics|6499.95           |\n",
      "|Laptop Pro  |Electronics|5199.96           |\n",
      "|Laptop Pro  |Electronics|10399.92          |\n",
      "+------------+-----------+------------------+\n",
      "\n",
      "\n",
      "4ï¸âƒ£ Null Value Filtering\n",
      "ðŸ” Null Value Results:\n",
      "   â€¢ Sales with null is_online: 104 records\n",
      "+------------+-----------+------------------+\n",
      "|product_name|category   |total_amount      |\n",
      "+------------+-----------+------------------+\n",
      "|Laptop Pro  |Electronics|2599.98           |\n",
      "|Laptop Pro  |Electronics|1299.99           |\n",
      "|Laptop Pro  |Electronics|3899.9700000000003|\n",
      "|Laptop Pro  |Electronics|9099.93           |\n",
      "|Laptop Pro  |Electronics|7799.9400000000005|\n",
      "|Laptop Pro  |Electronics|12999.9           |\n",
      "|Laptop Pro  |Electronics|11699.91          |\n",
      "|Laptop Pro  |Electronics|6499.95           |\n",
      "|Laptop Pro  |Electronics|5199.96           |\n",
      "|Laptop Pro  |Electronics|10399.92          |\n",
      "+------------+-----------+------------------+\n",
      "\n",
      "\n",
      "4ï¸âƒ£ Null Value Filtering\n",
      "ðŸ” Null Value Results:\n",
      "   â€¢ Sales with null is_online: 104 records\n",
      "   â€¢ Sales with non-null is_online: 896 records\n",
      "   â€¢ Customers without email: 1 records\n",
      "   â€¢ Customers with email: 9 records\n",
      "\n",
      "5ï¸âƒ£ Date Range Filtering\n",
      "ðŸ“… Date Filter Results:\n",
      "   â€¢ Sales with non-null is_online: 896 records\n",
      "   â€¢ Customers without email: 1 records\n",
      "   â€¢ Customers with email: 9 records\n",
      "\n",
      "5ï¸âƒ£ Date Range Filtering\n",
      "ðŸ“… Date Filter Results:\n",
      "   â€¢ Q1 2024 sales: 254 records\n",
      "   â€¢ Recent 30 days: 0 records\n",
      "   â€¢ Weekend sales: 269 records\n",
      "\n",
      "6ï¸âƒ£ List-based Filtering (IN/NOT IN)\n",
      "ðŸ“‹ List Filter Results:\n",
      "   â€¢ Q1 2024 sales: 254 records\n",
      "   â€¢ Recent 30 days: 0 records\n",
      "   â€¢ Weekend sales: 269 records\n",
      "\n",
      "6ï¸âƒ£ List-based Filtering (IN/NOT IN)\n",
      "ðŸ“‹ List Filter Results:\n",
      "   â€¢ North/South regions: 408 records\n",
      "   â€¢ Specific products: 319 records\n",
      "   â€¢ Excluding Central region: 784 records\n",
      "\n",
      "7ï¸âƒ£ Filter Performance Comparison\n",
      "   â€¢ North/South regions: 408 records\n",
      "   â€¢ Specific products: 319 records\n",
      "   â€¢ Excluding Central region: 784 records\n",
      "\n",
      "7ï¸âƒ£ Filter Performance Comparison\n",
      "âš¡ Performance Results:\n",
      "   â€¢ filter() method: 0.1701s, 850 records\n",
      "   â€¢ where() method: 0.0657s, 850 records\n",
      "   â€¢ filter() and where() are identical (where is alias)\n",
      "\n",
      "ðŸ“Š Filtering Best Practices:\n",
      "   â€¢ Apply filters early in transformation pipeline\n",
      "   â€¢ Use column references (col()) for better optimization\n",
      "   â€¢ Combine filters with & (and) and | (or) operators\n",
      "   â€¢ Handle null values explicitly in conditions\n",
      "   â€¢ Use isin() for list-based filtering\n",
      "   â€¢ Leverage predicate pushdown for better performance\n",
      "âš¡ Performance Results:\n",
      "   â€¢ filter() method: 0.1701s, 850 records\n",
      "   â€¢ where() method: 0.0657s, 850 records\n",
      "   â€¢ filter() and where() are identical (where is alias)\n",
      "\n",
      "ðŸ“Š Filtering Best Practices:\n",
      "   â€¢ Apply filters early in transformation pipeline\n",
      "   â€¢ Use column references (col()) for better optimization\n",
      "   â€¢ Combine filters with & (and) and | (or) operators\n",
      "   â€¢ Handle null values explicitly in conditions\n",
      "   â€¢ Use isin() for list-based filtering\n",
      "   â€¢ Leverage predicate pushdown for better performance\n"
     ]
    }
   ],
   "source": [
    "# Filtering & Conditional Operations\n",
    "print(\"ðŸ” Filtering & Conditional Operations\")\n",
    "print(\"=\" * 36)\n",
    "\n",
    "# 1. Basic Filtering\n",
    "print(\"1ï¸âƒ£ Basic Filtering\")\n",
    "\n",
    "# Simple conditions\n",
    "high_value_sales = df_sales.filter(col(\"total_amount\") > 500)\n",
    "electronics = df_sales.filter(col(\"category\") == \"Electronics\")\n",
    "recent_sales = df_sales.filter(col(\"sale_date\") >= \"2024-06-01\")\n",
    "\n",
    "print(f\"ðŸ“Š Filter Results:\")\n",
    "print(f\"   â€¢ High value sales (>$500): {high_value_sales.count()} records\")\n",
    "print(f\"   â€¢ Electronics sales: {electronics.count()} records\")\n",
    "print(f\"   â€¢ Recent sales (since June): {recent_sales.count()} records\")\n",
    "\n",
    "high_value_sales.select(\"transaction_id\", \"product_name\", \"total_amount\", \"category\").show(3)\n",
    "\n",
    "# 2. Compound Conditions\n",
    "print(f\"\\n2ï¸âƒ£ Compound Conditions (AND/OR Logic)\")\n",
    "\n",
    "# Multiple AND conditions\n",
    "high_value_electronics = df_sales.filter(\n",
    "    (col(\"category\") == \"Electronics\") & \n",
    "    (col(\"total_amount\") > 300) & \n",
    "    (col(\"rating\") >= 4.0)\n",
    ")\n",
    "\n",
    "# OR conditions\n",
    "furniture_or_appliances = df_sales.filter(\n",
    "    (col(\"category\") == \"Furniture\") | \n",
    "    (col(\"category\") == \"Appliances\")\n",
    ")\n",
    "\n",
    "# Complex conditions with parentheses\n",
    "complex_filter = df_sales.filter(\n",
    "    ((col(\"category\") == \"Electronics\") & (col(\"total_amount\") > 500)) |\n",
    "    ((col(\"category\") == \"Furniture\") & (col(\"rating\") >= 4.5))\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“Š Compound Filter Results:\")\n",
    "print(f\"   â€¢ High-value Electronics (>$300, ratingâ‰¥4.0): {high_value_electronics.count()} records\")\n",
    "print(f\"   â€¢ Furniture OR Appliances: {furniture_or_appliances.count()} records\")\n",
    "print(f\"   â€¢ Complex condition: {complex_filter.count()} records\")\n",
    "\n",
    "# 3. String Filtering\n",
    "print(f\"\\n3ï¸âƒ£ String Filtering\")\n",
    "\n",
    "# String patterns and matching\n",
    "products_with_pro = df_sales.filter(col(\"product_name\").contains(\"Pro\"))\n",
    "products_starting_wireless = df_sales.filter(col(\"product_name\").startswith(\"Wireless\"))\n",
    "products_ending_keyboard = df_sales.filter(col(\"product_name\").endswith(\"Keyboard\"))\n",
    "\n",
    "# Regular expressions\n",
    "products_with_pattern = df_sales.filter(col(\"product_name\").rlike(\".*[Ll]aptop.*|.*[Pp]hone.*\"))\n",
    "\n",
    "print(f\"ðŸ“ String Filter Results:\")\n",
    "print(f\"   â€¢ Products containing 'Pro': {products_with_pro.count()} records\")\n",
    "print(f\"   â€¢ Products starting with 'Wireless': {products_starting_wireless.count()} records\")\n",
    "print(f\"   â€¢ Products ending with 'Keyboard': {products_ending_keyboard.count()} records\")\n",
    "print(f\"   â€¢ Products matching laptop/phone pattern: {products_with_pattern.count()} records\")\n",
    "\n",
    "products_with_pro.select(\"product_name\", \"category\", \"total_amount\").distinct().show(truncate=False)\n",
    "\n",
    "# 4. Null Value Filtering\n",
    "print(f\"\\n4ï¸âƒ£ Null Value Filtering\")\n",
    "\n",
    "# Check for null values\n",
    "sales_with_nulls = df_sales.filter(col(\"is_online\").isNull())\n",
    "sales_without_nulls = df_sales.filter(col(\"is_online\").isNotNull())\n",
    "\n",
    "# Customers with missing emails\n",
    "customers_no_email = df_customers.filter((col(\"email\").isNull()) | (col(\"email\") == \"\"))\n",
    "customers_with_email = df_customers.filter(col(\"email\").isNotNull() & (col(\"email\") != \"\"))\n",
    "\n",
    "print(f\"ðŸ” Null Value Results:\")\n",
    "print(f\"   â€¢ Sales with null is_online: {sales_with_nulls.count()} records\")\n",
    "print(f\"   â€¢ Sales with non-null is_online: {sales_without_nulls.count()} records\")\n",
    "print(f\"   â€¢ Customers without email: {customers_no_email.count()} records\")\n",
    "print(f\"   â€¢ Customers with email: {customers_with_email.count()} records\")\n",
    "\n",
    "# 5. Date Range Filtering\n",
    "print(f\"\\n5ï¸âƒ£ Date Range Filtering\")\n",
    "\n",
    "# Convert to date type for proper comparison\n",
    "df_sales_typed = df_sales.withColumn(\"sale_date_typed\", to_date(col(\"sale_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Date range filters\n",
    "q1_2024 = df_sales_typed.filter(\n",
    "    (col(\"sale_date_typed\") >= \"2024-01-01\") & \n",
    "    (col(\"sale_date_typed\") < \"2024-04-01\")\n",
    ")\n",
    "\n",
    "recent_30_days = df_sales_typed.filter(\n",
    "    col(\"sale_date_typed\") >= date_sub(current_date(), 30)\n",
    ")\n",
    "\n",
    "weekend_sales = df_sales_typed.filter(\n",
    "    dayofweek(col(\"sale_date_typed\")).isin([1, 7])  # Sunday=1, Saturday=7\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“… Date Filter Results:\")\n",
    "print(f\"   â€¢ Q1 2024 sales: {q1_2024.count()} records\")\n",
    "print(f\"   â€¢ Recent 30 days: {recent_30_days.count()} records\")\n",
    "print(f\"   â€¢ Weekend sales: {weekend_sales.count()} records\")\n",
    "\n",
    "# 6. List-based Filtering\n",
    "print(f\"\\n6ï¸âƒ£ List-based Filtering (IN/NOT IN)\")\n",
    "\n",
    "# Filter by lists of values\n",
    "target_regions = [\"North\", \"South\"]\n",
    "target_products = [\"P001\", \"P005\", \"P007\"]\n",
    "\n",
    "region_filter = df_sales.filter(col(\"region\").isin(target_regions))\n",
    "product_filter = df_sales.filter(col(\"product_id\").isin(target_products))\n",
    "exclude_regions = df_sales.filter(~col(\"region\").isin([\"Central\"]))\n",
    "\n",
    "print(f\"ðŸ“‹ List Filter Results:\")\n",
    "print(f\"   â€¢ North/South regions: {region_filter.count()} records\")\n",
    "print(f\"   â€¢ Specific products: {product_filter.count()} records\")\n",
    "print(f\"   â€¢ Excluding Central region: {exclude_regions.count()} records\")\n",
    "\n",
    "# 7. Performance Comparison\n",
    "print(f\"\\n7ï¸âƒ£ Filter Performance Comparison\")\n",
    "\n",
    "# Measure filter performance\n",
    "start_time = time.time()\n",
    "result1 = df_sales.filter(col(\"total_amount\") > 200).count()\n",
    "filter_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "result2 = df_sales.where(col(\"total_amount\") > 200).count()  # where() is alias for filter()\n",
    "where_time = time.time() - start_time\n",
    "\n",
    "print(f\"âš¡ Performance Results:\")\n",
    "print(f\"   â€¢ filter() method: {filter_time:.4f}s, {result1} records\")\n",
    "print(f\"   â€¢ where() method: {where_time:.4f}s, {result2} records\")\n",
    "print(f\"   â€¢ filter() and where() are identical (where is alias)\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Filtering Best Practices:\")\n",
    "print(f\"   â€¢ Apply filters early in transformation pipeline\")\n",
    "print(f\"   â€¢ Use column references (col()) for better optimization\")\n",
    "print(f\"   â€¢ Combine filters with & (and) and | (or) operators\")\n",
    "print(f\"   â€¢ Handle null values explicitly in conditions\")\n",
    "print(f\"   â€¢ Use isin() for list-based filtering\")\n",
    "print(f\"   â€¢ Leverage predicate pushdown for better performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8925132",
   "metadata": {},
   "source": [
    "## 3.5 Aggregations & Grouping Operations\n",
    "\n",
    "Aggregations allow you to summarize and analyze data by computing statistics across groups of records. These are wide transformations that may trigger shuffle operations.\n",
    "\n",
    "**Key Aggregation Concepts:**\n",
    "- **GroupBy Operations**: Group data by one or more columns\n",
    "- **Aggregate Functions**: sum, count, avg, min, max, stddev, etc.\n",
    "- **Multiple Aggregations**: Apply several aggregation functions simultaneously\n",
    "- **Having Conditions**: Filter groups after aggregation\n",
    "- **Performance**: Wide transformations that may require data shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f8fb301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Aggregations & Grouping Operations\n",
      "=====================================\n",
      "1ï¸âƒ£ Basic Aggregations\n",
      "ðŸ“ˆ Overall Sales Statistics:\n",
      "   â€¢ Total Transactions: 1,000\n",
      "   â€¢ Total Revenue: $2,116,080.84\n",
      "   â€¢ Average Transaction: $2116.08\n",
      "   â€¢ Min Transaction: $29.99\n",
      "   â€¢ Max Transaction: $12999.90\n",
      "   â€¢ Standard Deviation: $2621.50\n",
      "\n",
      "2ï¸âƒ£ GroupBy Operations\n",
      "ðŸ“Š Sales by Category:\n",
      "ðŸ“Š Sales by Category:\n",
      "+-----------+-----------------+------------------+------------------+----------+\n",
      "|category   |transaction_count|total_revenue     |avg_amount        |max_amount|\n",
      "+-----------+-----------------+------------------+------------------+----------+\n",
      "|Electronics|526              |1597600.7100000002|3037.263707224335 |12999.9   |\n",
      "|Furniture  |275              |398261.0          |1448.2218181818182|4499.9    |\n",
      "|Appliances |199              |120219.12999999998|604.1162311557788 |1299.9    |\n",
      "+-----------+-----------------+------------------+------------------+----------+\n",
      "\n",
      "ðŸŒ Sales by Region and Category:\n",
      "+-----------+-----------------+------------------+------------------+----------+\n",
      "|category   |transaction_count|total_revenue     |avg_amount        |max_amount|\n",
      "+-----------+-----------------+------------------+------------------+----------+\n",
      "|Electronics|526              |1597600.7100000002|3037.263707224335 |12999.9   |\n",
      "|Furniture  |275              |398261.0          |1448.2218181818182|4499.9    |\n",
      "|Appliances |199              |120219.12999999998|604.1162311557788 |1299.9    |\n",
      "+-----------+-----------------+------------------+------------------+----------+\n",
      "\n",
      "ðŸŒ Sales by Region and Category:\n",
      "+-------+-----------+-----+------------------+------------------+\n",
      "|region |category   |count|revenue           |avg_rating        |\n",
      "+-------+-----------+-----+------------------+------------------+\n",
      "|Central|Appliances |40   |26027.65          |3.0075000000000003|\n",
      "|Central|Electronics|125  |398953.14         |3.0224000000000006|\n",
      "|Central|Furniture  |51   |75293.18000000001 |2.966666666666667 |\n",
      "|East   |Appliances |36   |21377.980000000003|2.980555555555555 |\n",
      "|East   |Electronics|93   |274295.20000000007|3.01505376344086  |\n",
      "|East   |Furniture  |46   |76669.20999999999 |3.178260869565217 |\n",
      "|North  |Appliances |41   |22257.98          |3.024390243902439 |\n",
      "|North  |Electronics|96   |279894.17000000004|3.013541666666667 |\n",
      "|North  |Furniture  |57   |73621.31          |2.770175438596491 |\n",
      "|South  |Appliances |39   |24757.840000000004|3.0205128205128204|\n",
      "+-------+-----------+-----+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "3ï¸âƒ£ Advanced Aggregations\n",
      "ðŸŒŽ Advanced Regional Statistics:\n",
      "+-------+-----------+-----+------------------+------------------+\n",
      "|region |category   |count|revenue           |avg_rating        |\n",
      "+-------+-----------+-----+------------------+------------------+\n",
      "|Central|Appliances |40   |26027.65          |3.0075000000000003|\n",
      "|Central|Electronics|125  |398953.14         |3.0224000000000006|\n",
      "|Central|Furniture  |51   |75293.18000000001 |2.966666666666667 |\n",
      "|East   |Appliances |36   |21377.980000000003|2.980555555555555 |\n",
      "|East   |Electronics|93   |274295.20000000007|3.01505376344086  |\n",
      "|East   |Furniture  |46   |76669.20999999999 |3.178260869565217 |\n",
      "|North  |Appliances |41   |22257.98          |3.024390243902439 |\n",
      "|North  |Electronics|96   |279894.17000000004|3.013541666666667 |\n",
      "|North  |Furniture  |57   |73621.31          |2.770175438596491 |\n",
      "|South  |Appliances |39   |24757.840000000004|3.0205128205128204|\n",
      "+-------+-----------+-----+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "3ï¸âƒ£ Advanced Aggregations\n",
      "ðŸŒŽ Advanced Regional Statistics:\n",
      "+-------+------------+------------------+------------------+----------+----------+---------------+-----------+------------------+\n",
      "|region |transactions|revenue           |avg_amount        |min_amount|max_amount|unique_products|unique_reps|avg_rating        |\n",
      "+-------+------------+------------------+------------------+----------+----------+---------------+-----------+------------------+\n",
      "|Central|216         |500273.97         |2316.0831944444444|29.99     |12999.9   |10             |5          |3.0064814814814813|\n",
      "|South  |214         |465918.32000000007|2177.1884112149537|29.99     |12999.9   |10             |5          |3.0378504672897195|\n",
      "|West   |201         |401772.70000000007|1998.8691542288561|29.99     |11699.91  |10             |5          |2.770149253731343 |\n",
      "|North  |194         |375773.4600000001 |1936.9765979381448|29.99     |12999.9   |10             |5          |2.9443298969072167|\n",
      "|East   |175         |372342.3900000001 |2127.6708000000003|29.99     |12999.9   |10             |5          |3.050857142857143 |\n",
      "+-------+------------+------------------+------------------+----------+----------+---------------+-----------+------------------+\n",
      "\n",
      "\n",
      "4ï¸âƒ£ Conditional Aggregations\n",
      "ðŸŽ¯ Conditional Aggregations by Category:\n",
      "+-------+------------+------------------+------------------+----------+----------+---------------+-----------+------------------+\n",
      "|region |transactions|revenue           |avg_amount        |min_amount|max_amount|unique_products|unique_reps|avg_rating        |\n",
      "+-------+------------+------------------+------------------+----------+----------+---------------+-----------+------------------+\n",
      "|Central|216         |500273.97         |2316.0831944444444|29.99     |12999.9   |10             |5          |3.0064814814814813|\n",
      "|South  |214         |465918.32000000007|2177.1884112149537|29.99     |12999.9   |10             |5          |3.0378504672897195|\n",
      "|West   |201         |401772.70000000007|1998.8691542288561|29.99     |11699.91  |10             |5          |2.770149253731343 |\n",
      "|North  |194         |375773.4600000001 |1936.9765979381448|29.99     |12999.9   |10             |5          |2.9443298969072167|\n",
      "|East   |175         |372342.3900000001 |2127.6708000000003|29.99     |12999.9   |10             |5          |3.050857142857143 |\n",
      "+-------+------------+------------------+------------------+----------+----------+---------------+-----------+------------------+\n",
      "\n",
      "\n",
      "4ï¸âƒ£ Conditional Aggregations\n",
      "ðŸŽ¯ Conditional Aggregations by Category:\n",
      "+-----------+-----------+----------------+------------------+------------------+------------+-------------+\n",
      "|category   |total_sales|high_value_sales|high_value_revenue|avg_high_rating   |online_sales|offline_sales|\n",
      "+-----------+-----------+----------------+------------------+------------------+------------+-------------+\n",
      "|Furniture  |275        |165             |368840.2200000001 |4.391549295774648 |127         |121          |\n",
      "|Electronics|526        |362             |1564838.35        |4.456818181818182 |226         |241          |\n",
      "|Appliances |199        |115             |97591.32          |4.4423076923076925|91          |90           |\n",
      "+-----------+-----------+----------------+------------------+------------------+------------+-------------+\n",
      "\n",
      "\n",
      "5ï¸âƒ£ Date-based Aggregations\n",
      "ðŸ“… Monthly Sales Trends:\n",
      "+-----------+-----------+----------------+------------------+------------------+------------+-------------+\n",
      "|category   |total_sales|high_value_sales|high_value_revenue|avg_high_rating   |online_sales|offline_sales|\n",
      "+-----------+-----------+----------------+------------------+------------------+------------+-------------+\n",
      "|Furniture  |275        |165             |368840.2200000001 |4.391549295774648 |127         |121          |\n",
      "|Electronics|526        |362             |1564838.35        |4.456818181818182 |226         |241          |\n",
      "|Appliances |199        |115             |97591.32          |4.4423076923076925|91          |90           |\n",
      "+-----------+-----------+----------------+------------------+------------------+------------+-------------+\n",
      "\n",
      "\n",
      "5ï¸âƒ£ Date-based Aggregations\n",
      "ðŸ“… Monthly Sales Trends:\n",
      "+----+-----+------------+------------------+------------------+----------------+\n",
      "|year|month|transactions|           revenue|   avg_transaction|unique_customers|\n",
      "+----+-----+------------+------------------+------------------+----------------+\n",
      "|2024|    1|          86|127617.54999999997|1483.9249999999997|               0|\n",
      "|2024|    2|          88|         208426.48|2368.4827272727275|               0|\n",
      "|2024|    3|          80|         176263.45|       2203.293125|               0|\n",
      "|2024|    4|          75|         161981.72|2159.7562666666668|               0|\n",
      "|2024|    5|          80|164323.83000000002|       2054.047875|               0|\n",
      "|2024|    6|          69|         141412.54|2049.4571014492753|               0|\n",
      "|2024|    7|          69|159292.11000000002|2308.5813043478265|               0|\n",
      "|2024|    8|         100|         259618.37|         2596.1837|               0|\n",
      "|2024|    9|          88|         161291.16| 1832.854090909091|               0|\n",
      "|2024|   10|          87|         172959.12|1988.0358620689656|               0|\n",
      "|2024|   11|          74|         152749.99| 2064.189054054054|               0|\n",
      "|2024|   12|         104|230144.52000000005|2212.9280769230772|               0|\n",
      "+----+-----+------------+------------------+------------------+----------------+\n",
      "\n",
      "\n",
      "6ï¸âƒ£ Percentile and Quantile Aggregations\n",
      "ðŸ“Š Percentile Analysis by Category:\n",
      "+----+-----+------------+------------------+------------------+----------------+\n",
      "|year|month|transactions|           revenue|   avg_transaction|unique_customers|\n",
      "+----+-----+------------+------------------+------------------+----------------+\n",
      "|2024|    1|          86|127617.54999999997|1483.9249999999997|               0|\n",
      "|2024|    2|          88|         208426.48|2368.4827272727275|               0|\n",
      "|2024|    3|          80|         176263.45|       2203.293125|               0|\n",
      "|2024|    4|          75|         161981.72|2159.7562666666668|               0|\n",
      "|2024|    5|          80|164323.83000000002|       2054.047875|               0|\n",
      "|2024|    6|          69|         141412.54|2049.4571014492753|               0|\n",
      "|2024|    7|          69|159292.11000000002|2308.5813043478265|               0|\n",
      "|2024|    8|         100|         259618.37|         2596.1837|               0|\n",
      "|2024|    9|          88|         161291.16| 1832.854090909091|               0|\n",
      "|2024|   10|          87|         172959.12|1988.0358620689656|               0|\n",
      "|2024|   11|          74|         152749.99| 2064.189054054054|               0|\n",
      "|2024|   12|         104|230144.52000000005|2212.9280769230772|               0|\n",
      "+----+-----+------------+------------------+------------------+----------------+\n",
      "\n",
      "\n",
      "6ï¸âƒ£ Percentile and Quantile Aggregations\n",
      "ðŸ“Š Percentile Analysis by Category:\n",
      "+-----------+-----+------------------+-----------------+------------------+----------+\n",
      "|category   |count|q1_amount         |median_amount    |q3_amount         |p95_amount|\n",
      "+-----------+-----+------------------+-----------------+------------------+----------+\n",
      "|Furniture  |275  |367.92            |899.98           |2399.92           |4049.91   |\n",
      "|Electronics|526  |299.9             |1799.97          |4799.9400000000005|9099.93   |\n",
      "|Appliances |199  |269.96999999999997|539.9399999999999|899.9             |1169.91   |\n",
      "+-----------+-----+------------------+-----------------+------------------+----------+\n",
      "\n",
      "\n",
      "7ï¸âƒ£ Custom Aggregations\n",
      "ðŸ”§ Custom Aggregations (Coefficient of Variation):\n",
      "+-------+------------+------------------+------------------+------------------------+\n",
      "|region |transactions|avg_amount        |std_amount        |coefficient_of_variation|\n",
      "+-------+------------+------------------+------------------+------------------------+\n",
      "|East   |175         |2127.6708000000003|2526.468419661877 |1.187433892339866       |\n",
      "|North  |194         |1936.9765979381448|2640.7034875634595|1.3633120247164636      |\n",
      "|West   |201         |1998.8691542288561|2530.0986647320265|1.2657650248788364      |\n",
      "|South  |214         |2177.1884112149537|2697.5290195251955|1.2389965910299292      |\n",
      "|Central|216         |2316.0831944444444|2693.884181920481 |1.1631206462627346      |\n",
      "+-------+------------+------------------+------------------+------------------------+\n",
      "\n",
      "\n",
      "8ï¸âƒ£ Having Clauses (Post-aggregation Filtering)\n",
      "+-----------+-----+------------------+-----------------+------------------+----------+\n",
      "|category   |count|q1_amount         |median_amount    |q3_amount         |p95_amount|\n",
      "+-----------+-----+------------------+-----------------+------------------+----------+\n",
      "|Furniture  |275  |367.92            |899.98           |2399.92           |4049.91   |\n",
      "|Electronics|526  |299.9             |1799.97          |4799.9400000000005|9099.93   |\n",
      "|Appliances |199  |269.96999999999997|539.9399999999999|899.9             |1169.91   |\n",
      "+-----------+-----+------------------+-----------------+------------------+----------+\n",
      "\n",
      "\n",
      "7ï¸âƒ£ Custom Aggregations\n",
      "ðŸ”§ Custom Aggregations (Coefficient of Variation):\n",
      "+-------+------------+------------------+------------------+------------------------+\n",
      "|region |transactions|avg_amount        |std_amount        |coefficient_of_variation|\n",
      "+-------+------------+------------------+------------------+------------------------+\n",
      "|East   |175         |2127.6708000000003|2526.468419661877 |1.187433892339866       |\n",
      "|North  |194         |1936.9765979381448|2640.7034875634595|1.3633120247164636      |\n",
      "|West   |201         |1998.8691542288561|2530.0986647320265|1.2657650248788364      |\n",
      "|South  |214         |2177.1884112149537|2697.5290195251955|1.2389965910299292      |\n",
      "|Central|216         |2316.0831944444444|2693.884181920481 |1.1631206462627346      |\n",
      "+-------+------------+------------------+------------------+------------------------+\n",
      "\n",
      "\n",
      "8ï¸âƒ£ Having Clauses (Post-aggregation Filtering)\n",
      "ðŸ’° High Revenue Categories (>$10,000):\n",
      "+-----------+-----------------+------------------+\n",
      "|   category|transaction_count|     total_revenue|\n",
      "+-----------+-----------------+------------------+\n",
      "|  Furniture|              275|          398261.0|\n",
      "|Electronics|              526|1597600.7100000002|\n",
      "| Appliances|              199|120219.12999999998|\n",
      "+-----------+-----------------+------------------+\n",
      "\n",
      "ðŸ“ˆ High Volume Regions (â‰¥150 transactions):\n",
      "ðŸ’° High Revenue Categories (>$10,000):\n",
      "+-----------+-----------------+------------------+\n",
      "|   category|transaction_count|     total_revenue|\n",
      "+-----------+-----------------+------------------+\n",
      "|  Furniture|              275|          398261.0|\n",
      "|Electronics|              526|1597600.7100000002|\n",
      "| Appliances|              199|120219.12999999998|\n",
      "+-----------+-----------------+------------------+\n",
      "\n",
      "ðŸ“ˆ High Volume Regions (â‰¥150 transactions):\n",
      "+-------+-----------------+------------------+\n",
      "| region|transaction_count|       avg_revenue|\n",
      "+-------+-----------------+------------------+\n",
      "|   East|              175|2127.6708000000003|\n",
      "|  North|              194|1936.9765979381448|\n",
      "|   West|              201|1998.8691542288561|\n",
      "|  South|              214|2177.1884112149537|\n",
      "|Central|              216|2316.0831944444444|\n",
      "+-------+-----------------+------------------+\n",
      "\n",
      "\n",
      "ðŸ“Š Aggregation Best Practices:\n",
      "   â€¢ Use groupBy() for categorical analysis\n",
      "   â€¢ Apply agg() with multiple functions for efficiency\n",
      "   â€¢ Use when() for conditional aggregations\n",
      "   â€¢ Filter groups with having-style conditions after agg()\n",
      "   â€¢ Consider approximate functions (approx_*) for large datasets\n",
      "   â€¢ Combine with window functions for advanced analytics\n",
      "+-------+-----------------+------------------+\n",
      "| region|transaction_count|       avg_revenue|\n",
      "+-------+-----------------+------------------+\n",
      "|   East|              175|2127.6708000000003|\n",
      "|  North|              194|1936.9765979381448|\n",
      "|   West|              201|1998.8691542288561|\n",
      "|  South|              214|2177.1884112149537|\n",
      "|Central|              216|2316.0831944444444|\n",
      "+-------+-----------------+------------------+\n",
      "\n",
      "\n",
      "ðŸ“Š Aggregation Best Practices:\n",
      "   â€¢ Use groupBy() for categorical analysis\n",
      "   â€¢ Apply agg() with multiple functions for efficiency\n",
      "   â€¢ Use when() for conditional aggregations\n",
      "   â€¢ Filter groups with having-style conditions after agg()\n",
      "   â€¢ Consider approximate functions (approx_*) for large datasets\n",
      "   â€¢ Combine with window functions for advanced analytics\n"
     ]
    }
   ],
   "source": [
    "# Aggregations & Grouping Operations\n",
    "print(\"ðŸ“Š Aggregations & Grouping Operations\")\n",
    "print(\"=\" * 37)\n",
    "\n",
    "# 1. Basic Aggregations\n",
    "print(\"1ï¸âƒ£ Basic Aggregations\")\n",
    "\n",
    "# Simple aggregations on entire dataset\n",
    "total_sales = df_sales.agg(\n",
    "    count(\"*\").alias(\"total_transactions\"),\n",
    "    sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "    avg(\"total_amount\").alias(\"avg_transaction\"),\n",
    "    min(\"total_amount\").alias(\"min_transaction\"),\n",
    "    max(\"total_amount\").alias(\"max_transaction\"),\n",
    "    stddev(\"total_amount\").alias(\"stddev_amount\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"ðŸ“ˆ Overall Sales Statistics:\")\n",
    "print(f\"   â€¢ Total Transactions: {total_sales['total_transactions']:,}\")\n",
    "print(f\"   â€¢ Total Revenue: ${total_sales['total_revenue']:,.2f}\")\n",
    "print(f\"   â€¢ Average Transaction: ${total_sales['avg_transaction']:.2f}\")\n",
    "print(f\"   â€¢ Min Transaction: ${total_sales['min_transaction']:.2f}\")\n",
    "print(f\"   â€¢ Max Transaction: ${total_sales['max_transaction']:.2f}\")\n",
    "print(f\"   â€¢ Standard Deviation: ${total_sales['stddev_amount']:.2f}\")\n",
    "\n",
    "# 2. GroupBy Operations\n",
    "print(f\"\\n2ï¸âƒ£ GroupBy Operations\")\n",
    "\n",
    "# Group by single column\n",
    "category_stats = df_sales.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"transaction_count\"),\n",
    "    sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "    avg(\"total_amount\").alias(\"avg_amount\"),\n",
    "    max(\"total_amount\").alias(\"max_amount\")\n",
    ").orderBy(desc(\"total_revenue\"))\n",
    "\n",
    "print(f\"ðŸ“Š Sales by Category:\")\n",
    "category_stats.show(truncate=False)\n",
    "\n",
    "# Group by multiple columns\n",
    "region_category_stats = df_sales.groupBy(\"region\", \"category\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    sum(\"total_amount\").alias(\"revenue\"),\n",
    "    avg(\"rating\").alias(\"avg_rating\")\n",
    ").orderBy(\"region\", \"category\")\n",
    "\n",
    "print(f\"ðŸŒ Sales by Region and Category:\")\n",
    "region_category_stats.show(10, truncate=False)\n",
    "\n",
    "# 3. Advanced Aggregations\n",
    "print(f\"\\n3ï¸âƒ£ Advanced Aggregations\")\n",
    "\n",
    "# Multiple aggregations with different functions\n",
    "advanced_stats = df_sales.groupBy(\"region\").agg(\n",
    "    count(\"*\").alias(\"transactions\"),\n",
    "    sum(\"total_amount\").alias(\"revenue\"),\n",
    "    avg(\"total_amount\").alias(\"avg_amount\"),\n",
    "    min(\"total_amount\").alias(\"min_amount\"),\n",
    "    max(\"total_amount\").alias(\"max_amount\"),\n",
    "    count_distinct(\"product_id\").alias(\"unique_products\"),\n",
    "    count_distinct(\"sales_rep\").alias(\"unique_reps\"),\n",
    "    avg(\"rating\").alias(\"avg_rating\"),\n",
    "    collect_list(\"product_name\").alias(\"all_products\")  # Collect all product names\n",
    ").orderBy(desc(\"revenue\"))\n",
    "\n",
    "print(f\"ðŸŒŽ Advanced Regional Statistics:\")\n",
    "# Show without collect_list to avoid clutter\n",
    "advanced_stats.drop(\"all_products\").show(truncate=False)\n",
    "\n",
    "# 4. Conditional Aggregations\n",
    "print(f\"\\n4ï¸âƒ£ Conditional Aggregations\")\n",
    "\n",
    "# Aggregations with conditions using when()\n",
    "conditional_agg = df_sales.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"total_sales\"),\n",
    "    sum(when(col(\"total_amount\") >= 500, 1).otherwise(0)).alias(\"high_value_sales\"),\n",
    "    sum(when(col(\"total_amount\") >= 500, col(\"total_amount\")).otherwise(0)).alias(\"high_value_revenue\"),\n",
    "    avg(when(col(\"rating\") >= 4.0, col(\"rating\"))).alias(\"avg_high_rating\"),\n",
    "    count(when(col(\"is_online\") == True, 1)).alias(\"online_sales\"),\n",
    "    count(when(col(\"is_online\") == False, 1)).alias(\"offline_sales\")\n",
    ")\n",
    "\n",
    "print(f\"ðŸŽ¯ Conditional Aggregations by Category:\")\n",
    "conditional_agg.show(truncate=False)\n",
    "\n",
    "# 5. Date-based Aggregations\n",
    "print(f\"\\n5ï¸âƒ£ Date-based Aggregations\")\n",
    "\n",
    "# Add date components for grouping\n",
    "df_with_dates = df_sales.withColumn(\"sale_date_typed\", to_date(col(\"sale_date\"), \"yyyy-MM-dd\")) \\\n",
    "                       .withColumn(\"year\", year(col(\"sale_date_typed\"))) \\\n",
    "                       .withColumn(\"month\", month(col(\"sale_date_typed\"))) \\\n",
    "                       .withColumn(\"quarter\", quarter(col(\"sale_date_typed\")))\n",
    "\n",
    "# Monthly sales trends\n",
    "monthly_trends = df_with_dates.groupBy(\"year\", \"month\").agg(\n",
    "    count(\"*\").alias(\"transactions\"),\n",
    "    sum(\"total_amount\").alias(\"revenue\"),\n",
    "    avg(\"total_amount\").alias(\"avg_transaction\"),\n",
    "    lit(0).alias(\"unique_customers\")  # Simplified since customer_id not in sales data\n",
    ").orderBy(\"year\", \"month\")\n",
    "\n",
    "print(f\"ðŸ“… Monthly Sales Trends:\")\n",
    "monthly_trends.show(12)\n",
    "\n",
    "# 6. Percentile and Quantile Aggregations\n",
    "print(f\"\\n6ï¸âƒ£ Percentile and Quantile Aggregations\")\n",
    "\n",
    "# Calculate percentiles\n",
    "percentile_stats = df_sales.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    expr(\"percentile_approx(total_amount, 0.25)\").alias(\"q1_amount\"),\n",
    "    expr(\"percentile_approx(total_amount, 0.5)\").alias(\"median_amount\"),\n",
    "    expr(\"percentile_approx(total_amount, 0.75)\").alias(\"q3_amount\"),\n",
    "    expr(\"percentile_approx(total_amount, 0.95)\").alias(\"p95_amount\")\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“Š Percentile Analysis by Category:\")\n",
    "percentile_stats.show(truncate=False)\n",
    "\n",
    "# 7. Custom Aggregations\n",
    "print(f\"\\n7ï¸âƒ£ Custom Aggregations\")\n",
    "\n",
    "# User-defined aggregation functions\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Define a custom function for coefficient of variation\n",
    "def coefficient_of_variation(values):\n",
    "    if len(values) <= 1:\n",
    "        return 0.0\n",
    "    mean_val = sum(values) / len(values)\n",
    "    variance = sum((x - mean_val) ** 2 for x in values) / (len(values) - 1)\n",
    "    std_dev = variance ** 0.5\n",
    "    return std_dev / mean_val if mean_val != 0 else 0.0\n",
    "\n",
    "# Register UDF\n",
    "cv_udf = udf(coefficient_of_variation, DoubleType())\n",
    "\n",
    "# Custom aggregation using collect_list and UDF\n",
    "custom_agg = df_sales.groupBy(\"region\").agg(\n",
    "    count(\"*\").alias(\"transactions\"),\n",
    "    avg(\"total_amount\").alias(\"avg_amount\"),\n",
    "    stddev(\"total_amount\").alias(\"std_amount\"),\n",
    "    (stddev(\"total_amount\") / avg(\"total_amount\")).alias(\"coefficient_of_variation\")\n",
    ")\n",
    "\n",
    "print(f\"ðŸ”§ Custom Aggregations (Coefficient of Variation):\")\n",
    "custom_agg.show(truncate=False)\n",
    "\n",
    "# 8. Having Clauses (Post-aggregation Filtering)\n",
    "print(f\"\\n8ï¸âƒ£ Having Clauses (Post-aggregation Filtering)\")\n",
    "\n",
    "# Filter groups after aggregation\n",
    "high_revenue_categories = df_sales.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"transaction_count\"),\n",
    "    sum(\"total_amount\").alias(\"total_revenue\")\n",
    ").filter(col(\"total_revenue\") > 10000)  # Having clause equivalent\n",
    "\n",
    "high_volume_regions = df_sales.groupBy(\"region\").agg(\n",
    "    count(\"*\").alias(\"transaction_count\"),\n",
    "    avg(\"total_amount\").alias(\"avg_revenue\")\n",
    ").filter(col(\"transaction_count\") >= 150)\n",
    "\n",
    "print(f\"ðŸ’° High Revenue Categories (>$10,000):\")\n",
    "high_revenue_categories.show()\n",
    "\n",
    "print(f\"ðŸ“ˆ High Volume Regions (â‰¥150 transactions):\")\n",
    "high_volume_regions.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Aggregation Best Practices:\")\n",
    "print(f\"   â€¢ Use groupBy() for categorical analysis\")\n",
    "print(f\"   â€¢ Apply agg() with multiple functions for efficiency\")\n",
    "print(f\"   â€¢ Use when() for conditional aggregations\")\n",
    "print(f\"   â€¢ Filter groups with having-style conditions after agg()\")\n",
    "print(f\"   â€¢ Consider approximate functions (approx_*) for large datasets\")\n",
    "print(f\"   â€¢ Combine with window functions for advanced analytics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84b09e2",
   "metadata": {},
   "source": [
    "## 3.6 Join Operations & Module Summary\n",
    "\n",
    "Join operations combine data from multiple DataFrames based on common keys. This section demonstrates various join types and concludes Module 3 with key takeaways.\n",
    "\n",
    "**Join Types in PySpark:**\n",
    "- **Inner Join**: Returns only matching records from both DataFrames\n",
    "- **Left Join**: Returns all records from left DataFrame, matching from right\n",
    "- **Right Join**: Returns all records from right DataFrame, matching from left\n",
    "- **Full Outer Join**: Returns all records from both DataFrames\n",
    "- **Cross Join**: Cartesian product of both DataFrames\n",
    "- **Anti Join**: Returns records from left that don't match right\n",
    "- **Semi Join**: Returns records from left that have matches in right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50007011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Join Operations & Module Summary\n",
      "===================================\n",
      "1ï¸âƒ£ Creating Additional Datasets for Joins\n",
      "âœ… Created orders dataset: 6 records\n",
      "\n",
      "2ï¸âƒ£ Inner Join (Only Matching Records)\n",
      "ðŸ“Š Inner Join Result: 5 records\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|customer_id|first_name|last_name|order_id|order_amount|\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|C001       |John      |Doe      |T000001 |1299.99     |\n",
      "|C002       |Jane      |Smith    |T000002 |29.99       |\n",
      "|C001       |John      |Doe      |T000003 |299.99      |\n",
      "|C003       |Bob       |Johnson  |T000004 |89.99       |\n",
      "|C004       |Alice     |Brown    |T000005 |799.99      |\n",
      "+-----------+----------+---------+--------+------------+\n",
      "\n",
      "\n",
      "3ï¸âƒ£ Left Join (All Customers, Matching Orders)\n",
      "ðŸ“Š Inner Join Result: 5 records\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|customer_id|first_name|last_name|order_id|order_amount|\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|C001       |John      |Doe      |T000001 |1299.99     |\n",
      "|C002       |Jane      |Smith    |T000002 |29.99       |\n",
      "|C001       |John      |Doe      |T000003 |299.99      |\n",
      "|C003       |Bob       |Johnson  |T000004 |89.99       |\n",
      "|C004       |Alice     |Brown    |T000005 |799.99      |\n",
      "+-----------+----------+---------+--------+------------+\n",
      "\n",
      "\n",
      "3ï¸âƒ£ Left Join (All Customers, Matching Orders)\n",
      "ðŸ“Š Left Join Result: 11 records\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|customer_id|first_name|last_name|order_id|order_amount|\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|C001       |John      |Doe      |T000003 |299.99      |\n",
      "|C001       |John      |Doe      |T000001 |1299.99     |\n",
      "|C002       |Jane      |Smith    |T000002 |29.99       |\n",
      "|C003       |Bob       |Johnson  |T000004 |89.99       |\n",
      "|C004       |Alice     |Brown    |T000005 |799.99      |\n",
      "|C005       |Charlie   |Wilson   |NULL    |NULL        |\n",
      "|C006       |Diana     |Davis    |NULL    |NULL        |\n",
      "|C007       |Eve       |Miller   |NULL    |NULL        |\n",
      "|C008       |Frank     |Garcia   |NULL    |NULL        |\n",
      "|C009       |Grace     |Lee      |NULL    |NULL        |\n",
      "|C010       |Henry     |Taylor   |NULL    |NULL        |\n",
      "+-----------+----------+---------+--------+------------+\n",
      "\n",
      "\n",
      "4ï¸âƒ£ Right Join (All Orders, Matching Customers)\n",
      "ðŸ“Š Left Join Result: 11 records\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|customer_id|first_name|last_name|order_id|order_amount|\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|C001       |John      |Doe      |T000003 |299.99      |\n",
      "|C001       |John      |Doe      |T000001 |1299.99     |\n",
      "|C002       |Jane      |Smith    |T000002 |29.99       |\n",
      "|C003       |Bob       |Johnson  |T000004 |89.99       |\n",
      "|C004       |Alice     |Brown    |T000005 |799.99      |\n",
      "|C005       |Charlie   |Wilson   |NULL    |NULL        |\n",
      "|C006       |Diana     |Davis    |NULL    |NULL        |\n",
      "|C007       |Eve       |Miller   |NULL    |NULL        |\n",
      "|C008       |Frank     |Garcia   |NULL    |NULL        |\n",
      "|C009       |Grace     |Lee      |NULL    |NULL        |\n",
      "|C010       |Henry     |Taylor   |NULL    |NULL        |\n",
      "+-----------+----------+---------+--------+------------+\n",
      "\n",
      "\n",
      "4ï¸âƒ£ Right Join (All Orders, Matching Customers)\n",
      "ðŸ“Š Right Join Result: 6 records\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|customer_id|first_name|last_name|order_id|order_amount|\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|C001       |John      |Doe      |T000001 |1299.99     |\n",
      "|C002       |Jane      |Smith    |T000002 |29.99       |\n",
      "|C001       |John      |Doe      |T000003 |299.99      |\n",
      "|C003       |Bob       |Johnson  |T000004 |89.99       |\n",
      "|C004       |Alice     |Brown    |T000005 |799.99      |\n",
      "|C999       |NULL      |NULL     |T000006 |45.99       |\n",
      "+-----------+----------+---------+--------+------------+\n",
      "\n",
      "\n",
      "5ï¸âƒ£ Full Outer Join (All Records from Both)\n",
      "ðŸ“Š Right Join Result: 6 records\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|customer_id|first_name|last_name|order_id|order_amount|\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|C001       |John      |Doe      |T000001 |1299.99     |\n",
      "|C002       |Jane      |Smith    |T000002 |29.99       |\n",
      "|C001       |John      |Doe      |T000003 |299.99      |\n",
      "|C003       |Bob       |Johnson  |T000004 |89.99       |\n",
      "|C004       |Alice     |Brown    |T000005 |799.99      |\n",
      "|C999       |NULL      |NULL     |T000006 |45.99       |\n",
      "+-----------+----------+---------+--------+------------+\n",
      "\n",
      "\n",
      "5ï¸âƒ£ Full Outer Join (All Records from Both)\n",
      "ðŸ“Š Full Outer Join Result: 12 records\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|customer_id|first_name|last_name|order_id|order_amount|\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|C001       |John      |Doe      |T000001 |1299.99     |\n",
      "|C001       |John      |Doe      |T000003 |299.99      |\n",
      "|C002       |Jane      |Smith    |T000002 |29.99       |\n",
      "|C003       |Bob       |Johnson  |T000004 |89.99       |\n",
      "|C004       |Alice     |Brown    |T000005 |799.99      |\n",
      "|C005       |Charlie   |Wilson   |NULL    |NULL        |\n",
      "|C006       |Diana     |Davis    |NULL    |NULL        |\n",
      "|C007       |Eve       |Miller   |NULL    |NULL        |\n",
      "|C008       |Frank     |Garcia   |NULL    |NULL        |\n",
      "|C009       |Grace     |Lee      |NULL    |NULL        |\n",
      "|C010       |Henry     |Taylor   |NULL    |NULL        |\n",
      "|C999       |NULL      |NULL     |T000006 |45.99       |\n",
      "+-----------+----------+---------+--------+------------+\n",
      "\n",
      "\n",
      "6ï¸âƒ£ Anti Join (Customers Without Orders)\n",
      "ðŸ“Š Full Outer Join Result: 12 records\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|customer_id|first_name|last_name|order_id|order_amount|\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|C001       |John      |Doe      |T000001 |1299.99     |\n",
      "|C001       |John      |Doe      |T000003 |299.99      |\n",
      "|C002       |Jane      |Smith    |T000002 |29.99       |\n",
      "|C003       |Bob       |Johnson  |T000004 |89.99       |\n",
      "|C004       |Alice     |Brown    |T000005 |799.99      |\n",
      "|C005       |Charlie   |Wilson   |NULL    |NULL        |\n",
      "|C006       |Diana     |Davis    |NULL    |NULL        |\n",
      "|C007       |Eve       |Miller   |NULL    |NULL        |\n",
      "|C008       |Frank     |Garcia   |NULL    |NULL        |\n",
      "|C009       |Grace     |Lee      |NULL    |NULL        |\n",
      "|C010       |Henry     |Taylor   |NULL    |NULL        |\n",
      "|C999       |NULL      |NULL     |T000006 |45.99       |\n",
      "+-----------+----------+---------+--------+------------+\n",
      "\n",
      "\n",
      "6ï¸âƒ£ Anti Join (Customers Without Orders)\n",
      "ðŸ“Š Anti Join Result: 6 records\n",
      "+-----------+----------+---------+------------------------+\n",
      "|customer_id|first_name|last_name|email                   |\n",
      "+-----------+----------+---------+------------------------+\n",
      "|C005       |Charlie   |Wilson   |charlie.wilson@email.com|\n",
      "|C006       |Diana     |Davis    |                        |\n",
      "|C007       |Eve       |Miller   |eve.miller@email.com    |\n",
      "|C008       |Frank     |Garcia   |frank.garcia@email.com  |\n",
      "|C009       |Grace     |Lee      |grace.lee@email.com     |\n",
      "|C010       |Henry     |Taylor   |henry.taylor@email.com  |\n",
      "+-----------+----------+---------+------------------------+\n",
      "\n",
      "\n",
      "7ï¸âƒ£ Semi Join (Customers With Orders)\n",
      "ðŸ“Š Anti Join Result: 6 records\n",
      "+-----------+----------+---------+------------------------+\n",
      "|customer_id|first_name|last_name|email                   |\n",
      "+-----------+----------+---------+------------------------+\n",
      "|C005       |Charlie   |Wilson   |charlie.wilson@email.com|\n",
      "|C006       |Diana     |Davis    |                        |\n",
      "|C007       |Eve       |Miller   |eve.miller@email.com    |\n",
      "|C008       |Frank     |Garcia   |frank.garcia@email.com  |\n",
      "|C009       |Grace     |Lee      |grace.lee@email.com     |\n",
      "|C010       |Henry     |Taylor   |henry.taylor@email.com  |\n",
      "+-----------+----------+---------+------------------------+\n",
      "\n",
      "\n",
      "7ï¸âƒ£ Semi Join (Customers With Orders)\n",
      "ðŸ“Š Semi Join Result: 4 records\n",
      "+-----------+----------+---------+---------------------+\n",
      "|customer_id|first_name|last_name|email                |\n",
      "+-----------+----------+---------+---------------------+\n",
      "|C001       |John      |Doe      |john.doe@email.com   |\n",
      "|C002       |Jane      |Smith    |jane.smith@email.com |\n",
      "|C003       |Bob       |Johnson  |bob.johnson@email.com|\n",
      "|C004       |Alice     |Brown    |alice.brown@email.com|\n",
      "+-----------+----------+---------+---------------------+\n",
      "\n",
      "\n",
      "8ï¸âƒ£ Complex Join with Multiple Conditions\n",
      "ðŸ“Š Semi Join Result: 4 records\n",
      "+-----------+----------+---------+---------------------+\n",
      "|customer_id|first_name|last_name|email                |\n",
      "+-----------+----------+---------+---------------------+\n",
      "|C001       |John      |Doe      |john.doe@email.com   |\n",
      "|C002       |Jane      |Smith    |jane.smith@email.com |\n",
      "|C003       |Bob       |Johnson  |bob.johnson@email.com|\n",
      "|C004       |Alice     |Brown    |alice.brown@email.com|\n",
      "+-----------+----------+---------+---------------------+\n",
      "\n",
      "\n",
      "8ï¸âƒ£ Complex Join with Multiple Conditions\n",
      "ðŸ“Š Sales-Products Join: 906 records\n",
      "+--------------+------------------+--------------------+------------------+-------------+-----------+--------------+\n",
      "|transaction_id|sales_product_name|catalog_product_name|total_amount      |catalog_price|category   |stock_quantity|\n",
      "+--------------+------------------+--------------------+------------------+-------------+-----------+--------------+\n",
      "|T000998       |Laptop Pro        |Laptop Pro          |7799.9400000000005|1299.99      |Electronics|50            |\n",
      "|T000989       |Laptop Pro        |Laptop Pro          |12999.9           |1299.99      |Electronics|50            |\n",
      "|T000978       |Laptop Pro        |Laptop Pro          |6499.95           |1299.99      |Electronics|50            |\n",
      "|T000968       |Laptop Pro        |Laptop Pro          |12999.9           |1299.99      |Electronics|50            |\n",
      "|T000960       |Laptop Pro        |Laptop Pro          |2599.98           |1299.99      |Electronics|50            |\n",
      "+--------------+------------------+--------------------+------------------+-------------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "9ï¸âƒ£ Join Performance Analysis\n",
      "ðŸ“Š Sales-Products Join: 906 records\n",
      "+--------------+------------------+--------------------+------------------+-------------+-----------+--------------+\n",
      "|transaction_id|sales_product_name|catalog_product_name|total_amount      |catalog_price|category   |stock_quantity|\n",
      "+--------------+------------------+--------------------+------------------+-------------+-----------+--------------+\n",
      "|T000998       |Laptop Pro        |Laptop Pro          |7799.9400000000005|1299.99      |Electronics|50            |\n",
      "|T000989       |Laptop Pro        |Laptop Pro          |12999.9           |1299.99      |Electronics|50            |\n",
      "|T000978       |Laptop Pro        |Laptop Pro          |6499.95           |1299.99      |Electronics|50            |\n",
      "|T000968       |Laptop Pro        |Laptop Pro          |12999.9           |1299.99      |Electronics|50            |\n",
      "|T000960       |Laptop Pro        |Laptop Pro          |2599.98           |1299.99      |Electronics|50            |\n",
      "+--------------+------------------+--------------------+------------------+-------------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "9ï¸âƒ£ Join Performance Analysis\n",
      "âš¡ Join Performance:\n",
      "   â€¢ Join operation time: 0.1444s\n",
      "   â€¢ Records processed: 5\n",
      "   â€¢ Consider broadcast joins for small tables\n",
      "   â€¢ Use proper partitioning for large datasets\n",
      "\n",
      "ðŸŽ¯ Module 3 Complete: Data Transformations Mastery!\n",
      "=======================================================\n",
      "ðŸ“š What You've Learned:\n",
      "   âœ… Basic Column Operations\n",
      "      â€¢ Selection, creation, modification, casting\n",
      "      â€¢ String, date, and conditional operations\n",
      "   âœ… Filtering & Conditional Logic\n",
      "      â€¢ Row filtering with complex conditions\n",
      "      â€¢ Null handling and string pattern matching\n",
      "   âœ… Aggregations & Grouping\n",
      "      â€¢ GroupBy operations and aggregate functions\n",
      "      â€¢ Conditional aggregations and percentiles\n",
      "   âœ… Join Operations\n",
      "      â€¢ All join types: inner, left, right, full, anti, semi\n",
      "      â€¢ Complex join conditions and performance\n",
      "\n",
      "ðŸ’¡ Key Performance Insights:\n",
      "   â€¢ Narrow transformations (select, filter) are fast\n",
      "   â€¢ Wide transformations (groupBy, join) may shuffle data\n",
      "   â€¢ Use column references col() for optimization\n",
      "   â€¢ Apply filters early in transformation pipeline\n",
      "   â€¢ Leverage Catalyst optimizer for automatic optimization\n",
      "\n",
      "ðŸš€ Production Best Practices:\n",
      "   â€¢ Chain transformations for lazy evaluation\n",
      "   â€¢ Cache intermediate results when reused\n",
      "   â€¢ Use explicit schemas for better performance\n",
      "   â€¢ Monitor Spark UI for optimization opportunities\n",
      "   â€¢ Consider partitioning strategy for large datasets\n",
      "\n",
      "ðŸ“ˆ Next Steps:\n",
      "   â€¢ Module 4: SQL & DataFrame API Advanced Patterns\n",
      "   â€¢ Module 5: Performance Optimization & Tuning\n",
      "   â€¢ Module 6: Machine Learning with MLlib\n",
      "   â€¢ Module 7: Streaming Data Processing\n",
      "\n",
      "ðŸ“Š Module 3 Statistics:\n",
      "   â€¢ Transformation sections: 6\n",
      "   â€¢ Concepts demonstrated: 25+\n",
      "   â€¢ Sample datasets: 3 (sales, customers, products)\n",
      "   â€¢ Real-world patterns: Production-ready examples\n",
      "\n",
      "ðŸŽ‰ Congratulations! You've mastered PySpark Data Transformations!\n",
      "Ready to tackle complex data processing challenges! ðŸš€\n",
      "âš¡ Join Performance:\n",
      "   â€¢ Join operation time: 0.1444s\n",
      "   â€¢ Records processed: 5\n",
      "   â€¢ Consider broadcast joins for small tables\n",
      "   â€¢ Use proper partitioning for large datasets\n",
      "\n",
      "ðŸŽ¯ Module 3 Complete: Data Transformations Mastery!\n",
      "=======================================================\n",
      "ðŸ“š What You've Learned:\n",
      "   âœ… Basic Column Operations\n",
      "      â€¢ Selection, creation, modification, casting\n",
      "      â€¢ String, date, and conditional operations\n",
      "   âœ… Filtering & Conditional Logic\n",
      "      â€¢ Row filtering with complex conditions\n",
      "      â€¢ Null handling and string pattern matching\n",
      "   âœ… Aggregations & Grouping\n",
      "      â€¢ GroupBy operations and aggregate functions\n",
      "      â€¢ Conditional aggregations and percentiles\n",
      "   âœ… Join Operations\n",
      "      â€¢ All join types: inner, left, right, full, anti, semi\n",
      "      â€¢ Complex join conditions and performance\n",
      "\n",
      "ðŸ’¡ Key Performance Insights:\n",
      "   â€¢ Narrow transformations (select, filter) are fast\n",
      "   â€¢ Wide transformations (groupBy, join) may shuffle data\n",
      "   â€¢ Use column references col() for optimization\n",
      "   â€¢ Apply filters early in transformation pipeline\n",
      "   â€¢ Leverage Catalyst optimizer for automatic optimization\n",
      "\n",
      "ðŸš€ Production Best Practices:\n",
      "   â€¢ Chain transformations for lazy evaluation\n",
      "   â€¢ Cache intermediate results when reused\n",
      "   â€¢ Use explicit schemas for better performance\n",
      "   â€¢ Monitor Spark UI for optimization opportunities\n",
      "   â€¢ Consider partitioning strategy for large datasets\n",
      "\n",
      "ðŸ“ˆ Next Steps:\n",
      "   â€¢ Module 4: SQL & DataFrame API Advanced Patterns\n",
      "   â€¢ Module 5: Performance Optimization & Tuning\n",
      "   â€¢ Module 6: Machine Learning with MLlib\n",
      "   â€¢ Module 7: Streaming Data Processing\n",
      "\n",
      "ðŸ“Š Module 3 Statistics:\n",
      "   â€¢ Transformation sections: 6\n",
      "   â€¢ Concepts demonstrated: 25+\n",
      "   â€¢ Sample datasets: 3 (sales, customers, products)\n",
      "   â€¢ Real-world patterns: Production-ready examples\n",
      "\n",
      "ðŸŽ‰ Congratulations! You've mastered PySpark Data Transformations!\n",
      "Ready to tackle complex data processing challenges! ðŸš€\n"
     ]
    }
   ],
   "source": [
    "# Join Operations & Module Completion\n",
    "print(\"ðŸ”— Join Operations & Module Summary\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Create additional datasets for join demonstrations\n",
    "print(\"1ï¸âƒ£ Creating Additional Datasets for Joins\")\n",
    "\n",
    "# Create a customer orders dataset\n",
    "customer_orders = [\n",
    "    (\"T000001\", \"C001\", \"2024-01-15\", 1299.99),\n",
    "    (\"T000002\", \"C002\", \"2024-01-16\", 29.99),\n",
    "    (\"T000003\", \"C001\", \"2024-01-17\", 299.99),\n",
    "    (\"T000004\", \"C003\", \"2024-01-18\", 89.99),\n",
    "    (\"T000005\", \"C004\", \"2024-01-19\", 799.99),\n",
    "    (\"T000006\", \"C999\", \"2024-01-20\", 45.99),  # Customer not in customer table\n",
    "]\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"order_amount\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df_orders = spark.createDataFrame(customer_orders, orders_schema)\n",
    "print(f\"âœ… Created orders dataset: {df_orders.count()} records\")\n",
    "\n",
    "# 2. Inner Join\n",
    "print(f\"\\n2ï¸âƒ£ Inner Join (Only Matching Records)\")\n",
    "\n",
    "inner_join = df_customers.join(df_orders, \"customer_id\", \"inner\")\n",
    "print(f\"ðŸ“Š Inner Join Result: {inner_join.count()} records\")\n",
    "inner_join.select(\"customer_id\", \"first_name\", \"last_name\", \"order_id\", \"order_amount\").show(truncate=False)\n",
    "\n",
    "# 3. Left Join\n",
    "print(f\"\\n3ï¸âƒ£ Left Join (All Customers, Matching Orders)\")\n",
    "\n",
    "left_join = df_customers.join(df_orders, \"customer_id\", \"left\")\n",
    "print(f\"ðŸ“Š Left Join Result: {left_join.count()} records\")\n",
    "left_join.select(\"customer_id\", \"first_name\", \"last_name\", \"order_id\", \"order_amount\").show(truncate=False)\n",
    "\n",
    "# 4. Right Join\n",
    "print(f\"\\n4ï¸âƒ£ Right Join (All Orders, Matching Customers)\")\n",
    "\n",
    "right_join = df_customers.join(df_orders, \"customer_id\", \"right\")\n",
    "print(f\"ðŸ“Š Right Join Result: {right_join.count()} records\")\n",
    "right_join.select(\"customer_id\", \"first_name\", \"last_name\", \"order_id\", \"order_amount\").show(truncate=False)\n",
    "\n",
    "# 5. Full Outer Join\n",
    "print(f\"\\n5ï¸âƒ£ Full Outer Join (All Records from Both)\")\n",
    "\n",
    "full_join = df_customers.join(df_orders, \"customer_id\", \"full_outer\")\n",
    "print(f\"ðŸ“Š Full Outer Join Result: {full_join.count()} records\")\n",
    "full_join.select(\"customer_id\", \"first_name\", \"last_name\", \"order_id\", \"order_amount\").show(truncate=False)\n",
    "\n",
    "# 6. Anti Join (Customers without Orders)\n",
    "print(f\"\\n6ï¸âƒ£ Anti Join (Customers Without Orders)\")\n",
    "\n",
    "anti_join = df_customers.join(df_orders, \"customer_id\", \"left_anti\")\n",
    "print(f\"ðŸ“Š Anti Join Result: {anti_join.count()} records\")\n",
    "anti_join.select(\"customer_id\", \"first_name\", \"last_name\", \"email\").show(truncate=False)\n",
    "\n",
    "# 7. Semi Join (Customers with Orders)\n",
    "print(f\"\\n7ï¸âƒ£ Semi Join (Customers With Orders)\")\n",
    "\n",
    "semi_join = df_customers.join(df_orders, \"customer_id\", \"left_semi\")\n",
    "print(f\"ðŸ“Š Semi Join Result: {semi_join.count()} records\")\n",
    "semi_join.select(\"customer_id\", \"first_name\", \"last_name\", \"email\").show(truncate=False)\n",
    "\n",
    "# 8. Complex Join with Multiple Conditions\n",
    "print(f\"\\n8ï¸âƒ£ Complex Join with Multiple Conditions\")\n",
    "\n",
    "# Join sales with products for detailed analysis\n",
    "sales_products_join = df_sales.join(\n",
    "    df_products, \n",
    "    (df_sales.product_id == df_products.product_id) & (df_products.is_active == True),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    df_sales.transaction_id,\n",
    "    df_sales.product_name.alias(\"sales_product_name\"),\n",
    "    df_products.product_name.alias(\"catalog_product_name\"),\n",
    "    df_sales.total_amount,\n",
    "    df_products.price.alias(\"catalog_price\"),\n",
    "    df_products.category,\n",
    "    df_products.stock_quantity\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“Š Sales-Products Join: {sales_products_join.count()} records\")\n",
    "sales_products_join.show(5, truncate=False)\n",
    "\n",
    "# 9. Performance Considerations\n",
    "print(f\"\\n9ï¸âƒ£ Join Performance Analysis\")\n",
    "\n",
    "# Measure join performance\n",
    "start_time = time.time()\n",
    "result_count = df_customers.join(df_orders, \"customer_id\", \"inner\").count()\n",
    "join_time = time.time() - start_time\n",
    "\n",
    "print(f\"âš¡ Join Performance:\")\n",
    "print(f\"   â€¢ Join operation time: {join_time:.4f}s\")\n",
    "print(f\"   â€¢ Records processed: {result_count}\")\n",
    "print(f\"   â€¢ Consider broadcast joins for small tables\")\n",
    "print(f\"   â€¢ Use proper partitioning for large datasets\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Module 3 Complete: Data Transformations Mastery!\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "print(f\"ðŸ“š What You've Learned:\")\n",
    "print(f\"   âœ… Basic Column Operations\")\n",
    "print(f\"      â€¢ Selection, creation, modification, casting\")\n",
    "print(f\"      â€¢ String, date, and conditional operations\")\n",
    "print(f\"   âœ… Filtering & Conditional Logic\")\n",
    "print(f\"      â€¢ Row filtering with complex conditions\")\n",
    "print(f\"      â€¢ Null handling and string pattern matching\")\n",
    "print(f\"   âœ… Aggregations & Grouping\")\n",
    "print(f\"      â€¢ GroupBy operations and aggregate functions\")\n",
    "print(f\"      â€¢ Conditional aggregations and percentiles\")\n",
    "print(f\"   âœ… Join Operations\")\n",
    "print(f\"      â€¢ All join types: inner, left, right, full, anti, semi\")\n",
    "print(f\"      â€¢ Complex join conditions and performance\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Key Performance Insights:\")\n",
    "print(f\"   â€¢ Narrow transformations (select, filter) are fast\")\n",
    "print(f\"   â€¢ Wide transformations (groupBy, join) may shuffle data\")\n",
    "print(f\"   â€¢ Use column references col() for optimization\")\n",
    "print(f\"   â€¢ Apply filters early in transformation pipeline\")\n",
    "print(f\"   â€¢ Leverage Catalyst optimizer for automatic optimization\")\n",
    "\n",
    "print(f\"\\nðŸš€ Production Best Practices:\")\n",
    "print(f\"   â€¢ Chain transformations for lazy evaluation\")\n",
    "print(f\"   â€¢ Cache intermediate results when reused\")\n",
    "print(f\"   â€¢ Use explicit schemas for better performance\")\n",
    "print(f\"   â€¢ Monitor Spark UI for optimization opportunities\")\n",
    "print(f\"   â€¢ Consider partitioning strategy for large datasets\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Next Steps:\")\n",
    "print(f\"   â€¢ Module 4: SQL & DataFrame API Advanced Patterns\")\n",
    "print(f\"   â€¢ Module 5: Performance Optimization & Tuning\")\n",
    "print(f\"   â€¢ Module 6: Machine Learning with MLlib\")\n",
    "print(f\"   â€¢ Module 7: Streaming Data Processing\")\n",
    "\n",
    "# Final statistics\n",
    "total_transformations = 6  # Sections covered\n",
    "total_concepts = 25  # Approximate concepts demonstrated\n",
    "\n",
    "print(f\"\\nðŸ“Š Module 3 Statistics:\")\n",
    "print(f\"   â€¢ Transformation sections: {total_transformations}\")\n",
    "print(f\"   â€¢ Concepts demonstrated: {total_concepts}+\")\n",
    "print(f\"   â€¢ Sample datasets: 3 (sales, customers, products)\")\n",
    "print(f\"   â€¢ Real-world patterns: Production-ready examples\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Congratulations! You've mastered PySpark Data Transformations!\")\n",
    "print(f\"Ready to tackle complex data processing challenges! ðŸš€\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
