{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86f549ec",
   "metadata": {},
   "source": [
    "# Module 3: Data Transformations & Operations\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this module, you will master:\n",
    "\n",
    "### üîÑ **Core Transformations**\n",
    "- DataFrame column operations and manipulations\n",
    "- Row filtering and conditional logic\n",
    "- Data type conversions and casting\n",
    "- String manipulations and text processing\n",
    "\n",
    "### üìä **Advanced Operations**\n",
    "- Aggregations and grouping operations\n",
    "- Window functions for analytics\n",
    "- Join operations and data combining\n",
    "- Set operations (union, intersect, except)\n",
    "\n",
    "### üßπ **Data Cleaning & Preprocessing**\n",
    "- Handling null values and missing data\n",
    "- Data deduplication and uniqueness\n",
    "- Data validation and quality checks\n",
    "- Outlier detection and handling\n",
    "\n",
    "### ‚ö° **Performance & Optimization**\n",
    "- Caching and persistence strategies\n",
    "- Partitioning for transformations\n",
    "- Broadcast variables and accumulators\n",
    "- Catalyst optimizer understanding\n",
    "\n",
    "### üõ†Ô∏è **Real-World Applications**\n",
    "- ETL pipeline patterns\n",
    "- Data quality frameworks\n",
    "- Complex business logic implementation\n",
    "- Performance monitoring and tuning\n",
    "\n",
    "**Prerequisites:** Module 1 (Foundation) and Module 2 (Data I/O)  \n",
    "**Estimated Time:** 90-120 minutes  \n",
    "**Difficulty:** Intermediate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde4b713",
   "metadata": {},
   "source": [
    "## 3.1 Overview: Data Transformations in PySpark\n",
    "\n",
    "Data transformations are the heart of any data processing pipeline. In PySpark, transformations are **lazy operations** that define what you want to do with your data, but don't execute until an **action** is called.\n",
    "\n",
    "### üîÑ **Transformation Types**\n",
    "\n",
    "**1. Narrow Transformations**\n",
    "- Operations where each input partition contributes to only one output partition\n",
    "- No data shuffle required across the cluster\n",
    "- Examples: `filter()`, `map()`, `select()`, `withColumn()`\n",
    "- **Performance**: Fast, highly parallelizable\n",
    "\n",
    "**2. Wide Transformations**\n",
    "- Operations that require data from multiple partitions\n",
    "- Trigger shuffle operations across the cluster\n",
    "- Examples: `groupBy()`, `join()`, `orderBy()`, `distinct()`\n",
    "- **Performance**: More expensive, but often necessary\n",
    "\n",
    "### üìä **DataFrame API vs SQL**\n",
    "\n",
    "PySpark provides two equivalent ways to transform data:\n",
    "- **DataFrame API**: Programmatic, type-safe, chainable operations\n",
    "- **SQL Interface**: Familiar SQL syntax for complex queries\n",
    "- **Interchangeable**: Can mix both approaches seamlessly\n",
    "\n",
    "### ‚ö° **Lazy Evaluation Benefits**\n",
    "\n",
    "1. **Query Optimization**: Catalyst optimizer can analyze the entire pipeline\n",
    "2. **Efficient Execution**: Combines multiple operations into optimized stages\n",
    "3. **Memory Management**: Only computes what's needed when needed\n",
    "4. **Fault Tolerance**: Can replay transformations if nodes fail\n",
    "\n",
    "### üéØ **Key Concepts We'll Explore**\n",
    "\n",
    "- **Immutability**: DataFrames are immutable; transformations create new DataFrames\n",
    "- **Lineage**: Spark tracks the transformation graph for fault tolerance\n",
    "- **Partitioning**: How data distribution affects transformation performance\n",
    "- **Caching**: When and how to persist intermediate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c684493",
   "metadata": {},
   "source": [
    "## 3.2 Environment Setup for Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e7b758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Module 3: Data Transformations & Operations\n",
      "==================================================\n",
      "üì¶ Conda Environment: pyspark_env\n",
      "üìÅ Directory ready: data/\n",
      "üìÅ Directory ready: temp/\n",
      "üìÅ Directory ready: outputs/\n",
      "‚úÖ PySpark imports successful\n",
      "‚úÖ Additional libraries imported (pandas, numpy)\n",
      "\n",
      "üéØ Environment Ready for Module 3!\n",
      "   ‚Ä¢ Project root: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/notebooks\n",
      "   ‚Ä¢ Data directory: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/notebooks/data\n",
      "   ‚Ä¢ Temp directory: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/notebooks/temp\n",
      "   ‚Ä¢ Random seeds set for reproducibility\n",
      "   ‚Ä¢ Ready for transformation operations!\n",
      "‚úÖ Additional libraries imported (pandas, numpy)\n",
      "\n",
      "üéØ Environment Ready for Module 3!\n",
      "   ‚Ä¢ Project root: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/notebooks\n",
      "   ‚Ä¢ Data directory: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/notebooks/data\n",
      "   ‚Ä¢ Temp directory: /Users/sanjeevadodlapati/Downloads/Repos/GeoSpatialAI/projects/project_pyspark_comprehensive_tutorial/notebooks/temp\n",
      "   ‚Ä¢ Random seeds set for reproducibility\n",
      "   ‚Ä¢ Ready for transformation operations!\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup and Verification for Module 3: Data Transformations\n",
    "print(\"üöÄ Module 3: Data Transformations & Operations\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Environment verification\n",
    "conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'Not in conda environment')\n",
    "print(f\"üì¶ Conda Environment: {conda_env}\")\n",
    "\n",
    "# Set up project paths\n",
    "project_root = Path.cwd()\n",
    "data_dir = project_root / \"data\"\n",
    "temp_dir = project_root / \"temp\"\n",
    "outputs_dir = project_root / \"outputs\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [data_dir, temp_dir, outputs_dir]:\n",
    "    directory.mkdir(exist_ok=True)\n",
    "    print(f\"üìÅ Directory ready: {directory.name}/\")\n",
    "\n",
    "# PySpark imports for transformations\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import *\n",
    "    from pyspark.sql.types import *\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark import SparkConf, SparkContext\n",
    "    \n",
    "    print(f\"‚úÖ PySpark imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå PySpark import error: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Additional imports for transformations\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    print(f\"‚úÖ Additional libraries imported (pandas, numpy)\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  Optional libraries not available: {e}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "if 'np' in locals():\n",
    "    np.random.seed(42)\n",
    "\n",
    "print(f\"\\nüéØ Environment Ready for Module 3!\")\n",
    "print(f\"   ‚Ä¢ Project root: {project_root}\")\n",
    "print(f\"   ‚Ä¢ Data directory: {data_dir}\")\n",
    "print(f\"   ‚Ä¢ Temp directory: {temp_dir}\")\n",
    "print(f\"   ‚Ä¢ Random seeds set for reproducibility\")\n",
    "print(f\"   ‚Ä¢ Ready for transformation operations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de5815f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Creating SparkSession for Data Transformations\n",
      "==================================================\n",
      "üÜï No existing SparkSession to stop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/25 20:28:02 WARN Utils: Your hostname, Sanjeevas-iMac.local, resolves to a loopback address: 127.0.0.1; using 192.168.12.128 instead (on interface en1)\n",
      "25/08/25 20:28:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/25 20:28:02 WARN Utils: Your hostname, Sanjeevas-iMac.local, resolves to a loopback address: 127.0.0.1; using 192.168.12.128 instead (on interface en1)\n",
      "25/08/25 20:28:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/25 20:28:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/25 20:28:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/25 20:28:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/08/25 20:28:03 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/08/25 20:28:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/08/25 20:28:03 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ SparkSession created successfully!\n",
      "üì± Application Name: PySpark-Tutorial-Module3-Transformations\n",
      "üî¢ Spark Version: 4.0.0\n",
      "üéØ Master: local[6]\n",
      "üíæ Driver Memory: 4g\n",
      "‚ö° Default Parallelism: 6\n",
      "üîÄ Shuffle Partitions: 12\n",
      "\n",
      "üîß Transformation Optimizations:\n",
      "   ‚Ä¢ Adaptive Query Execution: true\n",
      "   ‚Ä¢ Whole-Stage Code Generation: true\n",
      "   ‚Ä¢ Skew Join Optimization: true\n",
      "   ‚Ä¢ Arrow Optimization: true\n",
      "\n",
      "üåê Spark UI: http://192.168.12.128:4042\n",
      "\n",
      "üéØ Optimized for:\n",
      "   ‚Ä¢ Complex data transformations\n",
      "   ‚Ä¢ Join operations and aggregations\n",
      "   ‚Ä¢ Window functions and analytics\n",
      "   ‚Ä¢ Local development with 6 cores\n",
      "üíæ Driver Memory: 4g\n",
      "‚ö° Default Parallelism: 6\n",
      "üîÄ Shuffle Partitions: 12\n",
      "\n",
      "üîß Transformation Optimizations:\n",
      "   ‚Ä¢ Adaptive Query Execution: true\n",
      "   ‚Ä¢ Whole-Stage Code Generation: true\n",
      "   ‚Ä¢ Skew Join Optimization: true\n",
      "   ‚Ä¢ Arrow Optimization: true\n",
      "\n",
      "üåê Spark UI: http://192.168.12.128:4042\n",
      "\n",
      "üéØ Optimized for:\n",
      "   ‚Ä¢ Complex data transformations\n",
      "   ‚Ä¢ Join operations and aggregations\n",
      "   ‚Ä¢ Window functions and analytics\n",
      "   ‚Ä¢ Local development with 6 cores\n",
      "+--------------------+\n",
      "|              status|\n",
      "+--------------------+\n",
      "|SQL interface ready!|\n",
      "+--------------------+\n",
      "\n",
      "‚úÖ SQL interface verified and ready!\n",
      "+--------------------+\n",
      "|              status|\n",
      "+--------------------+\n",
      "|SQL interface ready!|\n",
      "+--------------------+\n",
      "\n",
      "‚úÖ SQL interface verified and ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create SparkSession optimized for Data Transformations\n",
    "print(\"‚ö° Creating SparkSession for Data Transformations\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Stop any existing SparkSession\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"üßπ Stopped existing SparkSession\")\n",
    "except:\n",
    "    print(\"üÜï No existing SparkSession to stop\")\n",
    "\n",
    "# Configuration optimized for transformations and analytics\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark-Tutorial-Module3-Transformations\") \\\n",
    "    .master(\"local[6]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.minPartitionSize\", \"16MB\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"12\") \\\n",
    "    .config(\"spark.sql.codegen.wholeStage\", \"true\") \\\n",
    "    .config(\"spark.sql.codegen.maxFields\", \"200\") \\\n",
    "    .config(\"spark.sql.join.preferSortMergeJoin\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"64MB\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Get SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Set log level to reduce noise\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"\\n‚úÖ SparkSession created successfully!\")\n",
    "print(f\"üì± Application Name: {spark.sparkContext.appName}\")\n",
    "print(f\"üî¢ Spark Version: {spark.version}\")\n",
    "print(f\"üéØ Master: {spark.sparkContext.master}\")\n",
    "print(f\"üíæ Driver Memory: {spark.conf.get('spark.driver.memory')}\")\n",
    "print(f\"‚ö° Default Parallelism: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"üîÄ Shuffle Partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "\n",
    "# Transformation-specific optimizations\n",
    "print(f\"\\nüîß Transformation Optimizations:\")\n",
    "print(f\"   ‚Ä¢ Adaptive Query Execution: {spark.conf.get('spark.sql.adaptive.enabled')}\")\n",
    "print(f\"   ‚Ä¢ Whole-Stage Code Generation: {spark.conf.get('spark.sql.codegen.wholeStage')}\")\n",
    "print(f\"   ‚Ä¢ Skew Join Optimization: {spark.conf.get('spark.sql.adaptive.skewJoin.enabled')}\")\n",
    "print(f\"   ‚Ä¢ Arrow Optimization: {spark.conf.get('spark.sql.execution.arrow.pyspark.enabled')}\")\n",
    "\n",
    "if spark.sparkContext.uiWebUrl:\n",
    "    print(f\"\\nüåê Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "print(f\"\\nüéØ Optimized for:\")\n",
    "print(f\"   ‚Ä¢ Complex data transformations\")\n",
    "print(f\"   ‚Ä¢ Join operations and aggregations\")\n",
    "print(f\"   ‚Ä¢ Window functions and analytics\")\n",
    "print(f\"   ‚Ä¢ Local development with 6 cores\")\n",
    "\n",
    "# Enable SQL interface\n",
    "spark.sql(\"SELECT 'SQL interface ready!' as status\").show()\n",
    "print(\"‚úÖ SQL interface verified and ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "325c23dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Creating Sample Datasets for Transformations\n",
      "================================================\n",
      "1Ô∏è‚É£ Creating Sales Transaction Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created sales dataset: 1000 records\n",
      "\n",
      "2Ô∏è‚É£ Creating Customer Dataset...\n",
      "‚úÖ Created customer dataset: 10 records\n",
      "\n",
      "3Ô∏è‚É£ Creating Product Information Dataset...\n",
      "‚úÖ Created product dataset: 10 records\n",
      "\n",
      "üìÑ Sample Data Preview:\n",
      "\n",
      "üõí Sales Data (first 3 rows):\n",
      "+--------------+----------+----------+--------------+-----------+----------+--------+------------+-------+-----------+---------+------+\n",
      "|transaction_id|sale_date |product_id|product_name  |category   |unit_price|quantity|total_amount|region |sales_rep  |is_online|rating|\n",
      "+--------------+----------+----------+--------------+-----------+----------+--------+------------+-------+-----------+---------+------+\n",
      "|T000001       |2024-01-13|P002      |Wireless Mouse|Electronics|29.99     |5       |119.96      |South  |Bob Smith  |true     |3.3   |\n",
      "|T000002       |2024-01-16|P001      |Laptop Pro    |Electronics|1299.99   |2       |5199.96     |South  |Eve Brown  |true     |3.8   |\n",
      "|T000003       |2024-08-02|P009      |Standing Desk |Furniture  |449.99    |4       |3599.92     |Central|Carol Davis|true     |4.0   |\n",
      "+--------------+----------+----------+--------------+-----------+----------+--------+------------+-------+-----------+---------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "üë• Customer Data (first 3 rows):\n",
      "‚úÖ Created product dataset: 10 records\n",
      "\n",
      "üìÑ Sample Data Preview:\n",
      "\n",
      "üõí Sales Data (first 3 rows):\n",
      "+--------------+----------+----------+--------------+-----------+----------+--------+------------+-------+-----------+---------+------+\n",
      "|transaction_id|sale_date |product_id|product_name  |category   |unit_price|quantity|total_amount|region |sales_rep  |is_online|rating|\n",
      "+--------------+----------+----------+--------------+-----------+----------+--------+------------+-------+-----------+---------+------+\n",
      "|T000001       |2024-01-13|P002      |Wireless Mouse|Electronics|29.99     |5       |119.96      |South  |Bob Smith  |true     |3.3   |\n",
      "|T000002       |2024-01-16|P001      |Laptop Pro    |Electronics|1299.99   |2       |5199.96     |South  |Eve Brown  |true     |3.8   |\n",
      "|T000003       |2024-08-02|P009      |Standing Desk |Furniture  |449.99    |4       |3599.92     |Central|Carol Davis|true     |4.0   |\n",
      "+--------------+----------+----------+--------------+-----------+----------+--------+------------+-------+-----------+---------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "üë• Customer Data (first 3 rows):\n",
      "+-----------+----------+---------+---------------------+--------+-----------+----------+------+\n",
      "|customer_id|first_name|last_name|email                |tier    |total_spent|join_date |region|\n",
      "+-----------+----------+---------+---------------------+--------+-----------+----------+------+\n",
      "|C001       |John      |Doe      |john.doe@email.com   |Premium |25000.5    |2022-01-15|North |\n",
      "|C002       |Jane      |Smith    |jane.smith@email.com |Standard|12000.75   |2022-03-22|South |\n",
      "|C003       |Bob       |Johnson  |bob.johnson@email.com|Premium |35000.0    |2021-11-08|East  |\n",
      "+-----------+----------+---------+---------------------+--------+-----------+----------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "üì¶ Product Data (first 3 rows):\n",
      "+----------+--------------+-----------+-------+--------------+-----------+---------+-------------------------------+\n",
      "|product_id|product_name  |category   |price  |stock_quantity|launch_date|is_active|tags                           |\n",
      "+----------+--------------+-----------+-------+--------------+-----------+---------+-------------------------------+\n",
      "|P001      |Laptop Pro    |Electronics|1299.99|50            |2024-01-01 |true     |[laptop, computer, electronics]|\n",
      "|P002      |Wireless Mouse|Electronics|29.99  |200           |2024-01-01 |true     |[mouse, wireless, accessories] |\n",
      "|P003      |Office Chair  |Furniture  |299.99 |30            |2024-01-01 |true     |[chair, office, furniture]     |\n",
      "+----------+--------------+-----------+-------+--------------+-----------+---------+-------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "üìä Datasets Summary:\n",
      "+-----------+----------+---------+---------------------+--------+-----------+----------+------+\n",
      "|customer_id|first_name|last_name|email                |tier    |total_spent|join_date |region|\n",
      "+-----------+----------+---------+---------------------+--------+-----------+----------+------+\n",
      "|C001       |John      |Doe      |john.doe@email.com   |Premium |25000.5    |2022-01-15|North |\n",
      "|C002       |Jane      |Smith    |jane.smith@email.com |Standard|12000.75   |2022-03-22|South |\n",
      "|C003       |Bob       |Johnson  |bob.johnson@email.com|Premium |35000.0    |2021-11-08|East  |\n",
      "+-----------+----------+---------+---------------------+--------+-----------+----------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "üì¶ Product Data (first 3 rows):\n",
      "+----------+--------------+-----------+-------+--------------+-----------+---------+-------------------------------+\n",
      "|product_id|product_name  |category   |price  |stock_quantity|launch_date|is_active|tags                           |\n",
      "+----------+--------------+-----------+-------+--------------+-----------+---------+-------------------------------+\n",
      "|P001      |Laptop Pro    |Electronics|1299.99|50            |2024-01-01 |true     |[laptop, computer, electronics]|\n",
      "|P002      |Wireless Mouse|Electronics|29.99  |200           |2024-01-01 |true     |[mouse, wireless, accessories] |\n",
      "|P003      |Office Chair  |Furniture  |299.99 |30            |2024-01-01 |true     |[chair, office, furniture]     |\n",
      "+----------+--------------+-----------+-------+--------------+-----------+---------+-------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "üìä Datasets Summary:\n",
      "   ‚Ä¢ Sales transactions: 1000 records\n",
      "   ‚Ä¢ Customers: 10 records\n",
      "   ‚Ä¢ Products: 10 records\n",
      "   ‚Ä¢ Ready for transformation demonstrations!\n",
      "   ‚Ä¢ Sales transactions: 1000 records\n",
      "   ‚Ä¢ Customers: 10 records\n",
      "   ‚Ä¢ Products: 10 records\n",
      "   ‚Ä¢ Ready for transformation demonstrations!\n"
     ]
    }
   ],
   "source": [
    "# Create Sample Datasets for Transformation Demonstrations\n",
    "print(\"üìä Creating Sample Datasets for Transformations\")\n",
    "print(\"=\" * 48)\n",
    "\n",
    "# 1. Sales Transaction Dataset\n",
    "print(\"1Ô∏è‚É£ Creating Sales Transaction Dataset...\")\n",
    "\n",
    "def generate_sales_data(num_records=1000):\n",
    "    \"\"\"Generate realistic sales transaction data\"\"\"\n",
    "    products = [\n",
    "        (\"P001\", \"Laptop Pro\", \"Electronics\", 1299.99),\n",
    "        (\"P002\", \"Wireless Mouse\", \"Electronics\", 29.99),\n",
    "        (\"P003\", \"Office Chair\", \"Furniture\", 299.99),\n",
    "        (\"P004\", \"Coffee Maker\", \"Appliances\", 89.99),\n",
    "        (\"P005\", \"Smartphone\", \"Electronics\", 799.99),\n",
    "        (\"P006\", \"Desk Lamp\", \"Furniture\", 45.99),\n",
    "        (\"P007\", \"Tablet\", \"Electronics\", 599.99),\n",
    "        (\"P008\", \"Ergonomic Keyboard\", \"Electronics\", 79.99),\n",
    "        (\"P009\", \"Standing Desk\", \"Furniture\", 449.99),\n",
    "        (\"P010\", \"Blender\", \"Appliances\", 129.99)\n",
    "    ]\n",
    "    \n",
    "    regions = [\"North\", \"South\", \"East\", \"West\", \"Central\"]\n",
    "    sales_reps = [\"Alice Johnson\", \"Bob Smith\", \"Carol Davis\", \"David Wilson\", \"Eve Brown\"]\n",
    "    \n",
    "    data = []\n",
    "    base_date = datetime(2024, 1, 1)\n",
    "    \n",
    "    for i in range(num_records):\n",
    "        product = random.choice(products)\n",
    "        sale_date = base_date + timedelta(days=random.randint(0, 365))\n",
    "        \n",
    "        data.append((\n",
    "            f\"T{i+1:06d}\",  # transaction_id\n",
    "            sale_date.strftime('%Y-%m-%d'),  # sale_date\n",
    "            product[0],  # product_id\n",
    "            product[1],  # product_name\n",
    "            product[2],  # category\n",
    "            product[3],  # unit_price\n",
    "            random.randint(1, 10),  # quantity\n",
    "            product[3] * random.randint(1, 10),  # total_amount\n",
    "            random.choice(regions),  # region\n",
    "            random.choice(sales_reps),  # sales_rep\n",
    "            random.choice([True, False]) if random.random() > 0.1 else None,  # is_online (some nulls)\n",
    "            int(random.uniform(1.0, 5.0) * 10) / 10.0  # rating\n",
    "        ))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate sales data\n",
    "sales_data = generate_sales_data(1000)\n",
    "\n",
    "# Create DataFrame\n",
    "sales_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"sale_date\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"sales_rep\", StringType(), True),\n",
    "    StructField(\"is_online\", BooleanType(), True),\n",
    "    StructField(\"rating\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df_sales = spark.createDataFrame(sales_data, sales_schema)\n",
    "\n",
    "print(f\"‚úÖ Created sales dataset: {df_sales.count()} records\")\n",
    "\n",
    "# 2. Customer Dataset\n",
    "print(\"\\n2Ô∏è‚É£ Creating Customer Dataset...\")\n",
    "\n",
    "customer_data = [\n",
    "    (\"C001\", \"John\", \"Doe\", \"john.doe@email.com\", \"Premium\", 25000.50, \"2022-01-15\", \"North\"),\n",
    "    (\"C002\", \"Jane\", \"Smith\", \"jane.smith@email.com\", \"Standard\", 12000.75, \"2022-03-22\", \"South\"),\n",
    "    (\"C003\", \"Bob\", \"Johnson\", \"bob.johnson@email.com\", \"Premium\", 35000.00, \"2021-11-08\", \"East\"),\n",
    "    (\"C004\", \"Alice\", \"Brown\", \"alice.brown@email.com\", \"Basic\", 5000.25, \"2023-02-14\", \"West\"),\n",
    "    (\"C005\", \"Charlie\", \"Wilson\", \"charlie.wilson@email.com\", \"Standard\", 18000.00, \"2022-09-05\", \"Central\"),\n",
    "    (\"C006\", \"Diana\", \"Davis\", \"\", \"Premium\", 42000.50, \"2021-07-12\", \"North\"),  # Missing email\n",
    "    (\"C007\", \"Eve\", \"Miller\", \"eve.miller@email.com\", \"Standard\", 15000.00, \"2023-01-30\", \"South\"),\n",
    "    (\"C008\", \"Frank\", \"Garcia\", \"frank.garcia@email.com\", \"Basic\", 3000.75, \"2023-05-20\", \"East\"),\n",
    "    (\"C009\", \"Grace\", \"Lee\", \"grace.lee@email.com\", \"Premium\", 55000.00, \"2020-08-17\", \"West\"),\n",
    "    (\"C010\", \"Henry\", \"Taylor\", \"henry.taylor@email.com\", \"Standard\", 22000.25, \"2022-04-25\", \"Central\")\n",
    "]\n",
    "\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"tier\", StringType(), True),\n",
    "    StructField(\"total_spent\", DoubleType(), True),\n",
    "    StructField(\"join_date\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_customers = spark.createDataFrame(customer_data, customer_schema)\n",
    "\n",
    "print(f\"‚úÖ Created customer dataset: {df_customers.count()} records\")\n",
    "\n",
    "# 3. Product Information Dataset\n",
    "print(\"\\n3Ô∏è‚É£ Creating Product Information Dataset...\")\n",
    "\n",
    "product_data = [\n",
    "    (\"P001\", \"Laptop Pro\", \"Electronics\", 1299.99, 50, \"2024-01-01\", True, [\"laptop\", \"computer\", \"electronics\"]),\n",
    "    (\"P002\", \"Wireless Mouse\", \"Electronics\", 29.99, 200, \"2024-01-01\", True, [\"mouse\", \"wireless\", \"accessories\"]),\n",
    "    (\"P003\", \"Office Chair\", \"Furniture\", 299.99, 30, \"2024-01-01\", True, [\"chair\", \"office\", \"furniture\"]),\n",
    "    (\"P004\", \"Coffee Maker\", \"Appliances\", 89.99, 75, \"2024-01-01\", True, [\"coffee\", \"appliance\", \"kitchen\"]),\n",
    "    (\"P005\", \"Smartphone\", \"Electronics\", 799.99, 120, \"2024-01-01\", True, [\"phone\", \"smartphone\", \"mobile\"]),\n",
    "    (\"P006\", \"Desk Lamp\", \"Furniture\", 45.99, 80, \"2024-01-01\", False, [\"lamp\", \"desk\", \"lighting\"]),  # Discontinued\n",
    "    (\"P007\", \"Tablet\", \"Electronics\", 599.99, 60, \"2024-01-01\", True, [\"tablet\", \"device\", \"portable\"]),\n",
    "    (\"P008\", \"Ergonomic Keyboard\", \"Electronics\", 79.99, 90, \"2024-01-01\", True, [\"keyboard\", \"ergonomic\", \"input\"]),\n",
    "    (\"P009\", \"Standing Desk\", \"Furniture\", 449.99, 25, \"2024-01-01\", True, [\"desk\", \"standing\", \"office\"]),\n",
    "    (\"P010\", \"Blender\", \"Appliances\", 129.99, 40, \"2024-01-01\", True, [\"blender\", \"kitchen\", \"appliance\"])\n",
    "]\n",
    "\n",
    "product_schema = StructType([\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"stock_quantity\", IntegerType(), True),\n",
    "    StructField(\"launch_date\", StringType(), True),\n",
    "    StructField(\"is_active\", BooleanType(), True),\n",
    "    StructField(\"tags\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "df_products = spark.createDataFrame(product_data, product_schema)\n",
    "\n",
    "print(f\"‚úÖ Created product dataset: {df_products.count()} records\")\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\nüìÑ Sample Data Preview:\")\n",
    "print(f\"\\nüõí Sales Data (first 3 rows):\")\n",
    "df_sales.show(3, truncate=False)\n",
    "\n",
    "print(f\"\\nüë• Customer Data (first 3 rows):\")\n",
    "df_customers.show(3, truncate=False)\n",
    "\n",
    "print(f\"\\nüì¶ Product Data (first 3 rows):\")\n",
    "df_products.show(3, truncate=False)\n",
    "\n",
    "print(f\"\\nüìä Datasets Summary:\")\n",
    "print(f\"   ‚Ä¢ Sales transactions: {df_sales.count()} records\")\n",
    "print(f\"   ‚Ä¢ Customers: {df_customers.count()} records\") \n",
    "print(f\"   ‚Ä¢ Products: {df_products.count()} records\")\n",
    "print(f\"   ‚Ä¢ Ready for transformation demonstrations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee508e75",
   "metadata": {},
   "source": [
    "## 3.3 Basic Column Operations\n",
    "\n",
    "Column operations are fundamental transformations in PySpark. They allow you to create new columns, modify existing ones, and perform calculations across your dataset.\n",
    "\n",
    "**Key Column Operations:**\n",
    "- **Selection**: Choose specific columns from a DataFrame\n",
    "- **Creation**: Add new columns with computed values\n",
    "- **Modification**: Transform existing column values\n",
    "- **Renaming**: Change column names for clarity\n",
    "- **Casting**: Convert between data types\n",
    "- **Conditional Logic**: Apply if-then-else logic to columns\n",
    "\n",
    "**Performance Notes:**\n",
    "- Column operations are **narrow transformations** (no shuffle required)\n",
    "- Multiple column operations can be chained efficiently\n",
    "- Use vectorized operations whenever possible for best performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d95614ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Basic Column Operations\n",
      "===========================\n",
      "1Ô∏è‚É£ Column Selection\n",
      "üìã Selected columns: ['transaction_id', 'product_name', 'total_amount', 'region']\n",
      "+--------------+--------------+------------+-------+\n",
      "|transaction_id|  product_name|total_amount| region|\n",
      "+--------------+--------------+------------+-------+\n",
      "|       T000001|Wireless Mouse|      119.96|  South|\n",
      "|       T000002|    Laptop Pro|     5199.96|  South|\n",
      "|       T000003| Standing Desk|     3599.92|Central|\n",
      "+--------------+--------------+------------+-------+\n",
      "only showing top 3 rows\n",
      "\n",
      "üìã Selected with aliases:\n",
      "+-------+--------------+------------+------------+\n",
      "|     id|  product_name|total_amount|sales_region|\n",
      "+-------+--------------+------------+------------+\n",
      "|T000001|Wireless Mouse|      119.96|       South|\n",
      "|T000002|    Laptop Pro|     5199.96|       South|\n",
      "|T000003| Standing Desk|     3599.92|     Central|\n",
      "+-------+--------------+------------+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "2Ô∏è‚É£ Adding New Columns\n",
      "üìä Added financial calculations:\n",
      "+--------------+--------------+------------+-------+\n",
      "|transaction_id|  product_name|total_amount| region|\n",
      "+--------------+--------------+------------+-------+\n",
      "|       T000001|Wireless Mouse|      119.96|  South|\n",
      "|       T000002|    Laptop Pro|     5199.96|  South|\n",
      "|       T000003| Standing Desk|     3599.92|Central|\n",
      "+--------------+--------------+------------+-------+\n",
      "only showing top 3 rows\n",
      "\n",
      "üìã Selected with aliases:\n",
      "+-------+--------------+------------+------------+\n",
      "|     id|  product_name|total_amount|sales_region|\n",
      "+-------+--------------+------------+------------+\n",
      "|T000001|Wireless Mouse|      119.96|       South|\n",
      "|T000002|    Laptop Pro|     5199.96|       South|\n",
      "|T000003| Standing Desk|     3599.92|     Central|\n",
      "+-------+--------------+------------+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "2Ô∏è‚É£ Adding New Columns\n",
      "üìä Added financial calculations:\n",
      "+--------------+------------+---------------+----------------+------------------+------------------+\n",
      "|transaction_id|total_amount|discount_amount|discounted_total|        tax_amount|      final_amount|\n",
      "+--------------+------------+---------------+----------------+------------------+------------------+\n",
      "|       T000001|      119.96|         11.996|         107.964|           8.63712|         116.60112|\n",
      "|       T000002|     5199.96|        519.996|        4679.964|374.39712000000003|        5054.36112|\n",
      "|       T000003|     3599.92|        359.992|        3239.928|         259.19424|3499.1222399999997|\n",
      "+--------------+------------+---------------+----------------+------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "3Ô∏è‚É£ String Operations\n",
      "+--------------+------------+---------------+----------------+------------------+------------------+\n",
      "|transaction_id|total_amount|discount_amount|discounted_total|        tax_amount|      final_amount|\n",
      "+--------------+------------+---------------+----------------+------------------+------------------+\n",
      "|       T000001|      119.96|         11.996|         107.964|           8.63712|         116.60112|\n",
      "|       T000002|     5199.96|        519.996|        4679.964|374.39712000000003|        5054.36112|\n",
      "|       T000003|     3599.92|        359.992|        3239.928|         259.19424|3499.1222399999997|\n",
      "+--------------+------------+---------------+----------------+------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "3Ô∏è‚É£ String Operations\n",
      "üìù String transformations:\n",
      "üìù String transformations:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-----------+------------+----------------+--------+\n",
      "|customer_id|full_name     |name_length|email_domain|first_name_upper|initials|\n",
      "+-----------+--------------+-----------+------------+----------------+--------+\n",
      "|C001       |John Doe      |8          |email.com   |JOHN            |JD      |\n",
      "|C002       |Jane Smith    |10         |email.com   |JANE            |JS      |\n",
      "|C003       |Bob Johnson   |11         |email.com   |BOB             |BJ      |\n",
      "|C004       |Alice Brown   |11         |email.com   |ALICE           |AB      |\n",
      "|C005       |Charlie Wilson|14         |email.com   |CHARLIE         |CW      |\n",
      "+-----------+--------------+-----------+------------+----------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "4Ô∏è‚É£ Date Operations\n",
      "üìÖ Date transformations:\n",
      "+--------------+----------+----+-----+-------+-----------+---------------+\n",
      "|transaction_id| sale_date|year|month|quarter|day_of_week|days_from_today|\n",
      "+--------------+----------+----+-----+-------+-----------+---------------+\n",
      "|       T000001|2024-01-13|2024|    1|      1|          7|            590|\n",
      "|       T000002|2024-01-16|2024|    1|      1|          3|            587|\n",
      "|       T000003|2024-08-02|2024|    8|      3|          6|            388|\n",
      "|       T000004|2024-12-23|2024|   12|      4|          2|            245|\n",
      "|       T000005|2024-02-19|2024|    2|      1|          2|            553|\n",
      "+--------------+----------+----+-----+-------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "5Ô∏è‚É£ Conditional Logic\n",
      "üéØ Conditional transformations:\n",
      "+--------------+------------+---------------+---------------+-----------+---------------+\n",
      "|transaction_id|total_amount|amount_category|is_weekend_sale|   category|commission_rate|\n",
      "+--------------+------------+---------------+---------------+-----------+---------------+\n",
      "|       T000001|      119.96|            Low|           true|Electronics|           0.05|\n",
      "|       T000002|     5199.96|           High|          false|Electronics|           0.05|\n",
      "|       T000003|     3599.92|           High|          false|  Furniture|           0.03|\n",
      "|       T000004|     1799.94|           High|          false|  Furniture|           0.03|\n",
      "|       T000005|     3599.94|           High|          false|Electronics|           0.05|\n",
      "+--------------+------------+---------------+---------------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "6Ô∏è‚É£ Type Casting\n",
      "üîÑ Type casting examples:\n",
      "Original types: [('transaction_id', StringType()), ('sale_date', StringType()), ('product_id', StringType())]\n",
      "Casted types: [('quantity_str', StringType()), ('rating_int', IntegerType()), ('unit_price_float', FloatType())]\n",
      "üéØ Conditional transformations:\n",
      "+--------------+------------+---------------+---------------+-----------+---------------+\n",
      "|transaction_id|total_amount|amount_category|is_weekend_sale|   category|commission_rate|\n",
      "+--------------+------------+---------------+---------------+-----------+---------------+\n",
      "|       T000001|      119.96|            Low|           true|Electronics|           0.05|\n",
      "|       T000002|     5199.96|           High|          false|Electronics|           0.05|\n",
      "|       T000003|     3599.92|           High|          false|  Furniture|           0.03|\n",
      "|       T000004|     1799.94|           High|          false|  Furniture|           0.03|\n",
      "|       T000005|     3599.94|           High|          false|Electronics|           0.05|\n",
      "+--------------+------------+---------------+---------------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "6Ô∏è‚É£ Type Casting\n",
      "üîÑ Type casting examples:\n",
      "Original types: [('transaction_id', StringType()), ('sale_date', StringType()), ('product_id', StringType())]\n",
      "Casted types: [('quantity_str', StringType()), ('rating_int', IntegerType()), ('unit_price_float', FloatType())]\n",
      "+--------+------------+------+----------+----------+----------------+\n",
      "|quantity|quantity_str|rating|rating_int|unit_price|unit_price_float|\n",
      "+--------+------------+------+----------+----------+----------------+\n",
      "|       5|           5|   3.3|         3|     29.99|           29.99|\n",
      "|       2|           2|   3.8|         3|   1299.99|         1299.99|\n",
      "|       4|           4|   4.0|         4|    449.99|          449.99|\n",
      "+--------+------------+------+----------+----------+----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "üìä Column Operations Summary:\n",
      "   ‚Ä¢ Selection: Choose specific columns from DataFrames\n",
      "   ‚Ä¢ Addition: Create new columns with withColumn()\n",
      "   ‚Ä¢ String ops: concat(), upper(), split(), substring()\n",
      "   ‚Ä¢ Date ops: year(), month(), datediff(), current_date()\n",
      "   ‚Ä¢ Conditional: when().otherwise() for if-then-else logic\n",
      "   ‚Ä¢ Casting: Change data types with cast()\n",
      "   ‚Ä¢ All operations are lazy and optimized by Catalyst!\n",
      "+--------+------------+------+----------+----------+----------------+\n",
      "|quantity|quantity_str|rating|rating_int|unit_price|unit_price_float|\n",
      "+--------+------------+------+----------+----------+----------------+\n",
      "|       5|           5|   3.3|         3|     29.99|           29.99|\n",
      "|       2|           2|   3.8|         3|   1299.99|         1299.99|\n",
      "|       4|           4|   4.0|         4|    449.99|          449.99|\n",
      "+--------+------------+------+----------+----------+----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "üìä Column Operations Summary:\n",
      "   ‚Ä¢ Selection: Choose specific columns from DataFrames\n",
      "   ‚Ä¢ Addition: Create new columns with withColumn()\n",
      "   ‚Ä¢ String ops: concat(), upper(), split(), substring()\n",
      "   ‚Ä¢ Date ops: year(), month(), datediff(), current_date()\n",
      "   ‚Ä¢ Conditional: when().otherwise() for if-then-else logic\n",
      "   ‚Ä¢ Casting: Change data types with cast()\n",
      "   ‚Ä¢ All operations are lazy and optimized by Catalyst!\n"
     ]
    }
   ],
   "source": [
    "# Basic Column Operations and Transformations\n",
    "print(\"üîß Basic Column Operations\")\n",
    "print(\"=\" * 27)\n",
    "\n",
    "# 1. Column Selection\n",
    "print(\"1Ô∏è‚É£ Column Selection\")\n",
    "\n",
    "# Select specific columns\n",
    "df_selected = df_sales.select(\"transaction_id\", \"product_name\", \"total_amount\", \"region\")\n",
    "print(f\"üìã Selected columns: {df_selected.columns}\")\n",
    "df_selected.show(3)\n",
    "\n",
    "# Select with column expressions\n",
    "df_selected_expr = df_sales.select(\n",
    "    col(\"transaction_id\").alias(\"id\"),\n",
    "    col(\"product_name\"),\n",
    "    col(\"total_amount\"),\n",
    "    col(\"region\").alias(\"sales_region\")\n",
    ")\n",
    "print(f\"\\nüìã Selected with aliases:\")\n",
    "df_selected_expr.show(3)\n",
    "\n",
    "# 2. Adding New Columns\n",
    "print(f\"\\n2Ô∏è‚É£ Adding New Columns\")\n",
    "\n",
    "# Add calculated columns\n",
    "df_with_calculations = df_sales.withColumn(\"discount_amount\", col(\"total_amount\") * 0.1) \\\n",
    "                              .withColumn(\"discounted_total\", col(\"total_amount\") - col(\"discount_amount\")) \\\n",
    "                              .withColumn(\"tax_amount\", col(\"discounted_total\") * 0.08) \\\n",
    "                              .withColumn(\"final_amount\", col(\"discounted_total\") + col(\"tax_amount\"))\n",
    "\n",
    "print(f\"üìä Added financial calculations:\")\n",
    "df_with_calculations.select(\"transaction_id\", \"total_amount\", \"discount_amount\", \n",
    "                           \"discounted_total\", \"tax_amount\", \"final_amount\").show(3)\n",
    "\n",
    "# 3. String Operations\n",
    "print(f\"\\n3Ô∏è‚É£ String Operations\")\n",
    "\n",
    "# String manipulations\n",
    "df_string_ops = df_customers.withColumn(\"full_name\", concat(col(\"first_name\"), lit(\" \"), col(\"last_name\"))) \\\n",
    "                           .withColumn(\"name_length\", length(col(\"full_name\"))) \\\n",
    "                           .withColumn(\"email_domain\", \n",
    "                                     when(col(\"email\").isNotNull() & (col(\"email\") != \"\"), \n",
    "                                          split(col(\"email\"), \"@\").getItem(1))\n",
    "                                     .otherwise(None)) \\\n",
    "                           .withColumn(\"first_name_upper\", upper(col(\"first_name\"))) \\\n",
    "                           .withColumn(\"initials\", concat(substring(col(\"first_name\"), 1, 1), \n",
    "                                                        substring(col(\"last_name\"), 1, 1)))\n",
    "\n",
    "print(f\"üìù String transformations:\")\n",
    "df_string_ops.select(\"customer_id\", \"full_name\", \"name_length\", \"email_domain\", \n",
    "                    \"first_name_upper\", \"initials\").show(5, truncate=False)\n",
    "\n",
    "# 4. Date Operations\n",
    "print(f\"\\n4Ô∏è‚É£ Date Operations\")\n",
    "\n",
    "# Convert string dates to date type and perform date calculations\n",
    "df_date_ops = df_sales.withColumn(\"sale_date_typed\", to_date(col(\"sale_date\"), \"yyyy-MM-dd\")) \\\n",
    "                     .withColumn(\"year\", year(col(\"sale_date_typed\"))) \\\n",
    "                     .withColumn(\"month\", month(col(\"sale_date_typed\"))) \\\n",
    "                     .withColumn(\"quarter\", quarter(col(\"sale_date_typed\"))) \\\n",
    "                     .withColumn(\"day_of_week\", dayofweek(col(\"sale_date_typed\"))) \\\n",
    "                     .withColumn(\"days_from_today\", datediff(current_date(), col(\"sale_date_typed\")))\n",
    "\n",
    "print(f\"üìÖ Date transformations:\")\n",
    "df_date_ops.select(\"transaction_id\", \"sale_date\", \"year\", \"month\", \"quarter\", \n",
    "                  \"day_of_week\", \"days_from_today\").show(5)\n",
    "\n",
    "# 5. Conditional Logic\n",
    "print(f\"\\n5Ô∏è‚É£ Conditional Logic\")\n",
    "\n",
    "# Using when() for conditional logic\n",
    "df_conditional = df_sales.withColumn(\"amount_category\", \n",
    "                                   when(col(\"total_amount\") >= 1000, \"High\")\n",
    "                                   .when(col(\"total_amount\") >= 500, \"Medium\")\n",
    "                                   .when(col(\"total_amount\") >= 100, \"Low\")\n",
    "                                   .otherwise(\"Very Low\")) \\\n",
    "                        .withColumn(\"is_weekend_sale\", \n",
    "                                  when(dayofweek(to_date(col(\"sale_date\"), \"yyyy-MM-dd\")).isin([1, 7]), True)\n",
    "                                  .otherwise(False)) \\\n",
    "                        .withColumn(\"commission_rate\",\n",
    "                                  when(col(\"category\") == \"Electronics\", 0.05)\n",
    "                                  .when(col(\"category\") == \"Furniture\", 0.03)\n",
    "                                  .otherwise(0.02))\n",
    "\n",
    "print(f\"üéØ Conditional transformations:\")\n",
    "df_conditional.select(\"transaction_id\", \"total_amount\", \"amount_category\", \n",
    "                     \"is_weekend_sale\", \"category\", \"commission_rate\").show(5)\n",
    "\n",
    "# 6. Type Casting\n",
    "print(f\"\\n6Ô∏è‚É£ Type Casting\")\n",
    "\n",
    "# Data type conversions\n",
    "df_cast = df_sales.withColumn(\"quantity_str\", col(\"quantity\").cast(StringType())) \\\n",
    "                 .withColumn(\"rating_int\", col(\"rating\").cast(IntegerType())) \\\n",
    "                 .withColumn(\"unit_price_float\", col(\"unit_price\").cast(FloatType()))\n",
    "\n",
    "print(f\"üîÑ Type casting examples:\")\n",
    "print(f\"Original types: {[(field.name, field.dataType) for field in df_sales.schema.fields[:3]]}\")\n",
    "print(f\"Casted types: {[(field.name, field.dataType) for field in df_cast.select('quantity_str', 'rating_int', 'unit_price_float').schema.fields]}\")\n",
    "\n",
    "df_cast.select(\"quantity\", \"quantity_str\", \"rating\", \"rating_int\", \"unit_price\", \"unit_price_float\").show(3)\n",
    "\n",
    "print(f\"\\nüìä Column Operations Summary:\")\n",
    "print(f\"   ‚Ä¢ Selection: Choose specific columns from DataFrames\")\n",
    "print(f\"   ‚Ä¢ Addition: Create new columns with withColumn()\")\n",
    "print(f\"   ‚Ä¢ String ops: concat(), upper(), split(), substring()\")\n",
    "print(f\"   ‚Ä¢ Date ops: year(), month(), datediff(), current_date()\")\n",
    "print(f\"   ‚Ä¢ Conditional: when().otherwise() for if-then-else logic\")\n",
    "print(f\"   ‚Ä¢ Casting: Change data types with cast()\")\n",
    "print(f\"   ‚Ä¢ All operations are lazy and optimized by Catalyst!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd04408c",
   "metadata": {},
   "source": [
    "## 3.4 Filtering & Conditional Operations\n",
    "\n",
    "Filtering is essential for data analysis, allowing you to work with subsets of data that meet specific criteria. PySpark provides powerful filtering capabilities with optimized predicate pushdown.\n",
    "\n",
    "**Key Filtering Concepts:**\n",
    "- **Row Filtering**: Select rows based on conditions\n",
    "- **Compound Conditions**: Combine multiple filters with AND/OR logic\n",
    "- **Null Handling**: Deal with missing values in filters\n",
    "- **Performance**: Catalyst optimizer pushes filters down to data source\n",
    "- **SQL Equivalence**: Filter operations map directly to SQL WHERE clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30ce9c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Filtering & Conditional Operations\n",
      "====================================\n",
      "1Ô∏è‚É£ Basic Filtering\n",
      "üìä Filter Results:\n",
      "   ‚Ä¢ High value sales (>$500): 642 records\n",
      "   ‚Ä¢ Electronics sales: 526 records\n",
      "   ‚Ä¢ Recent sales (since June): 591 records\n",
      "+--------------+-------------+------------+-----------+\n",
      "|transaction_id| product_name|total_amount|   category|\n",
      "+--------------+-------------+------------+-----------+\n",
      "|       T000002|   Laptop Pro|     5199.96|Electronics|\n",
      "|       T000003|Standing Desk|     3599.92|  Furniture|\n",
      "|       T000004| Office Chair|     1799.94|  Furniture|\n",
      "+--------------+-------------+------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "2Ô∏è‚É£ Compound Conditions (AND/OR Logic)\n",
      "üìä Compound Filter Results:\n",
      "   ‚Ä¢ Electronics sales: 526 records\n",
      "   ‚Ä¢ Recent sales (since June): 591 records\n",
      "+--------------+-------------+------------+-----------+\n",
      "|transaction_id| product_name|total_amount|   category|\n",
      "+--------------+-------------+------------+-----------+\n",
      "|       T000002|   Laptop Pro|     5199.96|Electronics|\n",
      "|       T000003|Standing Desk|     3599.92|  Furniture|\n",
      "|       T000004| Office Chair|     1799.94|  Furniture|\n",
      "+--------------+-------------+------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "2Ô∏è‚É£ Compound Conditions (AND/OR Logic)\n",
      "üìä Compound Filter Results:\n",
      "   ‚Ä¢ High-value Electronics (>$300, rating‚â•4.0): 95 records\n",
      "   ‚Ä¢ Furniture OR Appliances: 474 records\n",
      "   ‚Ä¢ High-value Electronics (>$300, rating‚â•4.0): 95 records\n",
      "   ‚Ä¢ Furniture OR Appliances: 474 records\n",
      "   ‚Ä¢ Complex condition: 392 records\n",
      "\n",
      "3Ô∏è‚É£ String Filtering\n",
      "üìù String Filter Results:\n",
      "   ‚Ä¢ Products containing 'Pro': 86 records\n",
      "   ‚Ä¢ Complex condition: 392 records\n",
      "\n",
      "3Ô∏è‚É£ String Filtering\n",
      "üìù String Filter Results:\n",
      "   ‚Ä¢ Products containing 'Pro': 86 records\n",
      "   ‚Ä¢ Products starting with 'Wireless': 104 records\n",
      "   ‚Ä¢ Products ending with 'Keyboard': 103 records\n",
      "   ‚Ä¢ Products matching laptop/phone pattern: 193 records\n",
      "   ‚Ä¢ Products starting with 'Wireless': 104 records\n",
      "   ‚Ä¢ Products ending with 'Keyboard': 103 records\n",
      "   ‚Ä¢ Products matching laptop/phone pattern: 193 records\n",
      "+------------+-----------+------------------+\n",
      "|product_name|category   |total_amount      |\n",
      "+------------+-----------+------------------+\n",
      "|Laptop Pro  |Electronics|2599.98           |\n",
      "|Laptop Pro  |Electronics|1299.99           |\n",
      "|Laptop Pro  |Electronics|3899.9700000000003|\n",
      "|Laptop Pro  |Electronics|9099.93           |\n",
      "|Laptop Pro  |Electronics|7799.9400000000005|\n",
      "|Laptop Pro  |Electronics|12999.9           |\n",
      "|Laptop Pro  |Electronics|11699.91          |\n",
      "|Laptop Pro  |Electronics|6499.95           |\n",
      "|Laptop Pro  |Electronics|5199.96           |\n",
      "|Laptop Pro  |Electronics|10399.92          |\n",
      "+------------+-----------+------------------+\n",
      "\n",
      "\n",
      "4Ô∏è‚É£ Null Value Filtering\n",
      "üîç Null Value Results:\n",
      "   ‚Ä¢ Sales with null is_online: 104 records\n",
      "+------------+-----------+------------------+\n",
      "|product_name|category   |total_amount      |\n",
      "+------------+-----------+------------------+\n",
      "|Laptop Pro  |Electronics|2599.98           |\n",
      "|Laptop Pro  |Electronics|1299.99           |\n",
      "|Laptop Pro  |Electronics|3899.9700000000003|\n",
      "|Laptop Pro  |Electronics|9099.93           |\n",
      "|Laptop Pro  |Electronics|7799.9400000000005|\n",
      "|Laptop Pro  |Electronics|12999.9           |\n",
      "|Laptop Pro  |Electronics|11699.91          |\n",
      "|Laptop Pro  |Electronics|6499.95           |\n",
      "|Laptop Pro  |Electronics|5199.96           |\n",
      "|Laptop Pro  |Electronics|10399.92          |\n",
      "+------------+-----------+------------------+\n",
      "\n",
      "\n",
      "4Ô∏è‚É£ Null Value Filtering\n",
      "üîç Null Value Results:\n",
      "   ‚Ä¢ Sales with null is_online: 104 records\n",
      "   ‚Ä¢ Sales with non-null is_online: 896 records\n",
      "   ‚Ä¢ Customers without email: 1 records\n",
      "   ‚Ä¢ Customers with email: 9 records\n",
      "\n",
      "5Ô∏è‚É£ Date Range Filtering\n",
      "üìÖ Date Filter Results:\n",
      "   ‚Ä¢ Sales with non-null is_online: 896 records\n",
      "   ‚Ä¢ Customers without email: 1 records\n",
      "   ‚Ä¢ Customers with email: 9 records\n",
      "\n",
      "5Ô∏è‚É£ Date Range Filtering\n",
      "üìÖ Date Filter Results:\n",
      "   ‚Ä¢ Q1 2024 sales: 254 records\n",
      "   ‚Ä¢ Recent 30 days: 0 records\n",
      "   ‚Ä¢ Weekend sales: 269 records\n",
      "\n",
      "6Ô∏è‚É£ List-based Filtering (IN/NOT IN)\n",
      "üìã List Filter Results:\n",
      "   ‚Ä¢ Q1 2024 sales: 254 records\n",
      "   ‚Ä¢ Recent 30 days: 0 records\n",
      "   ‚Ä¢ Weekend sales: 269 records\n",
      "\n",
      "6Ô∏è‚É£ List-based Filtering (IN/NOT IN)\n",
      "üìã List Filter Results:\n",
      "   ‚Ä¢ North/South regions: 408 records\n",
      "   ‚Ä¢ Specific products: 319 records\n",
      "   ‚Ä¢ Excluding Central region: 784 records\n",
      "\n",
      "7Ô∏è‚É£ Filter Performance Comparison\n",
      "   ‚Ä¢ North/South regions: 408 records\n",
      "   ‚Ä¢ Specific products: 319 records\n",
      "   ‚Ä¢ Excluding Central region: 784 records\n",
      "\n",
      "7Ô∏è‚É£ Filter Performance Comparison\n",
      "‚ö° Performance Results:\n",
      "   ‚Ä¢ filter() method: 0.1701s, 850 records\n",
      "   ‚Ä¢ where() method: 0.0657s, 850 records\n",
      "   ‚Ä¢ filter() and where() are identical (where is alias)\n",
      "\n",
      "üìä Filtering Best Practices:\n",
      "   ‚Ä¢ Apply filters early in transformation pipeline\n",
      "   ‚Ä¢ Use column references (col()) for better optimization\n",
      "   ‚Ä¢ Combine filters with & (and) and | (or) operators\n",
      "   ‚Ä¢ Handle null values explicitly in conditions\n",
      "   ‚Ä¢ Use isin() for list-based filtering\n",
      "   ‚Ä¢ Leverage predicate pushdown for better performance\n",
      "‚ö° Performance Results:\n",
      "   ‚Ä¢ filter() method: 0.1701s, 850 records\n",
      "   ‚Ä¢ where() method: 0.0657s, 850 records\n",
      "   ‚Ä¢ filter() and where() are identical (where is alias)\n",
      "\n",
      "üìä Filtering Best Practices:\n",
      "   ‚Ä¢ Apply filters early in transformation pipeline\n",
      "   ‚Ä¢ Use column references (col()) for better optimization\n",
      "   ‚Ä¢ Combine filters with & (and) and | (or) operators\n",
      "   ‚Ä¢ Handle null values explicitly in conditions\n",
      "   ‚Ä¢ Use isin() for list-based filtering\n",
      "   ‚Ä¢ Leverage predicate pushdown for better performance\n"
     ]
    }
   ],
   "source": [
    "# Filtering & Conditional Operations\n",
    "print(\"üîç Filtering & Conditional Operations\")\n",
    "print(\"=\" * 36)\n",
    "\n",
    "# 1. Basic Filtering\n",
    "print(\"1Ô∏è‚É£ Basic Filtering\")\n",
    "\n",
    "# Simple conditions\n",
    "high_value_sales = df_sales.filter(col(\"total_amount\") > 500)\n",
    "electronics = df_sales.filter(col(\"category\") == \"Electronics\")\n",
    "recent_sales = df_sales.filter(col(\"sale_date\") >= \"2024-06-01\")\n",
    "\n",
    "print(f\"üìä Filter Results:\")\n",
    "print(f\"   ‚Ä¢ High value sales (>$500): {high_value_sales.count()} records\")\n",
    "print(f\"   ‚Ä¢ Electronics sales: {electronics.count()} records\")\n",
    "print(f\"   ‚Ä¢ Recent sales (since June): {recent_sales.count()} records\")\n",
    "\n",
    "high_value_sales.select(\"transaction_id\", \"product_name\", \"total_amount\", \"category\").show(3)\n",
    "\n",
    "# 2. Compound Conditions\n",
    "print(f\"\\n2Ô∏è‚É£ Compound Conditions (AND/OR Logic)\")\n",
    "\n",
    "# Multiple AND conditions\n",
    "high_value_electronics = df_sales.filter(\n",
    "    (col(\"category\") == \"Electronics\") & \n",
    "    (col(\"total_amount\") > 300) & \n",
    "    (col(\"rating\") >= 4.0)\n",
    ")\n",
    "\n",
    "# OR conditions\n",
    "furniture_or_appliances = df_sales.filter(\n",
    "    (col(\"category\") == \"Furniture\") | \n",
    "    (col(\"category\") == \"Appliances\")\n",
    ")\n",
    "\n",
    "# Complex conditions with parentheses\n",
    "complex_filter = df_sales.filter(\n",
    "    ((col(\"category\") == \"Electronics\") & (col(\"total_amount\") > 500)) |\n",
    "    ((col(\"category\") == \"Furniture\") & (col(\"rating\") >= 4.5))\n",
    ")\n",
    "\n",
    "print(f\"üìä Compound Filter Results:\")\n",
    "print(f\"   ‚Ä¢ High-value Electronics (>$300, rating‚â•4.0): {high_value_electronics.count()} records\")\n",
    "print(f\"   ‚Ä¢ Furniture OR Appliances: {furniture_or_appliances.count()} records\")\n",
    "print(f\"   ‚Ä¢ Complex condition: {complex_filter.count()} records\")\n",
    "\n",
    "# 3. String Filtering\n",
    "print(f\"\\n3Ô∏è‚É£ String Filtering\")\n",
    "\n",
    "# String patterns and matching\n",
    "products_with_pro = df_sales.filter(col(\"product_name\").contains(\"Pro\"))\n",
    "products_starting_wireless = df_sales.filter(col(\"product_name\").startswith(\"Wireless\"))\n",
    "products_ending_keyboard = df_sales.filter(col(\"product_name\").endswith(\"Keyboard\"))\n",
    "\n",
    "# Regular expressions\n",
    "products_with_pattern = df_sales.filter(col(\"product_name\").rlike(\".*[Ll]aptop.*|.*[Pp]hone.*\"))\n",
    "\n",
    "print(f\"üìù String Filter Results:\")\n",
    "print(f\"   ‚Ä¢ Products containing 'Pro': {products_with_pro.count()} records\")\n",
    "print(f\"   ‚Ä¢ Products starting with 'Wireless': {products_starting_wireless.count()} records\")\n",
    "print(f\"   ‚Ä¢ Products ending with 'Keyboard': {products_ending_keyboard.count()} records\")\n",
    "print(f\"   ‚Ä¢ Products matching laptop/phone pattern: {products_with_pattern.count()} records\")\n",
    "\n",
    "products_with_pro.select(\"product_name\", \"category\", \"total_amount\").distinct().show(truncate=False)\n",
    "\n",
    "# 4. Null Value Filtering\n",
    "print(f\"\\n4Ô∏è‚É£ Null Value Filtering\")\n",
    "\n",
    "# Check for null values\n",
    "sales_with_nulls = df_sales.filter(col(\"is_online\").isNull())\n",
    "sales_without_nulls = df_sales.filter(col(\"is_online\").isNotNull())\n",
    "\n",
    "# Customers with missing emails\n",
    "customers_no_email = df_customers.filter((col(\"email\").isNull()) | (col(\"email\") == \"\"))\n",
    "customers_with_email = df_customers.filter(col(\"email\").isNotNull() & (col(\"email\") != \"\"))\n",
    "\n",
    "print(f\"üîç Null Value Results:\")\n",
    "print(f\"   ‚Ä¢ Sales with null is_online: {sales_with_nulls.count()} records\")\n",
    "print(f\"   ‚Ä¢ Sales with non-null is_online: {sales_without_nulls.count()} records\")\n",
    "print(f\"   ‚Ä¢ Customers without email: {customers_no_email.count()} records\")\n",
    "print(f\"   ‚Ä¢ Customers with email: {customers_with_email.count()} records\")\n",
    "\n",
    "# 5. Date Range Filtering\n",
    "print(f\"\\n5Ô∏è‚É£ Date Range Filtering\")\n",
    "\n",
    "# Convert to date type for proper comparison\n",
    "df_sales_typed = df_sales.withColumn(\"sale_date_typed\", to_date(col(\"sale_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Date range filters\n",
    "q1_2024 = df_sales_typed.filter(\n",
    "    (col(\"sale_date_typed\") >= \"2024-01-01\") & \n",
    "    (col(\"sale_date_typed\") < \"2024-04-01\")\n",
    ")\n",
    "\n",
    "recent_30_days = df_sales_typed.filter(\n",
    "    col(\"sale_date_typed\") >= date_sub(current_date(), 30)\n",
    ")\n",
    "\n",
    "weekend_sales = df_sales_typed.filter(\n",
    "    dayofweek(col(\"sale_date_typed\")).isin([1, 7])  # Sunday=1, Saturday=7\n",
    ")\n",
    "\n",
    "print(f\"üìÖ Date Filter Results:\")\n",
    "print(f\"   ‚Ä¢ Q1 2024 sales: {q1_2024.count()} records\")\n",
    "print(f\"   ‚Ä¢ Recent 30 days: {recent_30_days.count()} records\")\n",
    "print(f\"   ‚Ä¢ Weekend sales: {weekend_sales.count()} records\")\n",
    "\n",
    "# 6. List-based Filtering\n",
    "print(f\"\\n6Ô∏è‚É£ List-based Filtering (IN/NOT IN)\")\n",
    "\n",
    "# Filter by lists of values\n",
    "target_regions = [\"North\", \"South\"]\n",
    "target_products = [\"P001\", \"P005\", \"P007\"]\n",
    "\n",
    "region_filter = df_sales.filter(col(\"region\").isin(target_regions))\n",
    "product_filter = df_sales.filter(col(\"product_id\").isin(target_products))\n",
    "exclude_regions = df_sales.filter(~col(\"region\").isin([\"Central\"]))\n",
    "\n",
    "print(f\"üìã List Filter Results:\")\n",
    "print(f\"   ‚Ä¢ North/South regions: {region_filter.count()} records\")\n",
    "print(f\"   ‚Ä¢ Specific products: {product_filter.count()} records\")\n",
    "print(f\"   ‚Ä¢ Excluding Central region: {exclude_regions.count()} records\")\n",
    "\n",
    "# 7. Performance Comparison\n",
    "print(f\"\\n7Ô∏è‚É£ Filter Performance Comparison\")\n",
    "\n",
    "# Measure filter performance\n",
    "start_time = time.time()\n",
    "result1 = df_sales.filter(col(\"total_amount\") > 200).count()\n",
    "filter_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "result2 = df_sales.where(col(\"total_amount\") > 200).count()  # where() is alias for filter()\n",
    "where_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚ö° Performance Results:\")\n",
    "print(f\"   ‚Ä¢ filter() method: {filter_time:.4f}s, {result1} records\")\n",
    "print(f\"   ‚Ä¢ where() method: {where_time:.4f}s, {result2} records\")\n",
    "print(f\"   ‚Ä¢ filter() and where() are identical (where is alias)\")\n",
    "\n",
    "print(f\"\\nüìä Filtering Best Practices:\")\n",
    "print(f\"   ‚Ä¢ Apply filters early in transformation pipeline\")\n",
    "print(f\"   ‚Ä¢ Use column references (col()) for better optimization\")\n",
    "print(f\"   ‚Ä¢ Combine filters with & (and) and | (or) operators\")\n",
    "print(f\"   ‚Ä¢ Handle null values explicitly in conditions\")\n",
    "print(f\"   ‚Ä¢ Use isin() for list-based filtering\")\n",
    "print(f\"   ‚Ä¢ Leverage predicate pushdown for better performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8925132",
   "metadata": {},
   "source": [
    "## 3.5 Aggregations & Grouping Operations\n",
    "\n",
    "Aggregations allow you to summarize and analyze data by computing statistics across groups of records. These are wide transformations that may trigger shuffle operations.\n",
    "\n",
    "**Key Aggregation Concepts:**\n",
    "- **GroupBy Operations**: Group data by one or more columns\n",
    "- **Aggregate Functions**: sum, count, avg, min, max, stddev, etc.\n",
    "- **Multiple Aggregations**: Apply several aggregation functions simultaneously\n",
    "- **Having Conditions**: Filter groups after aggregation\n",
    "- **Performance**: Wide transformations that may require data shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f8fb301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Aggregations & Grouping Operations\n",
      "=====================================\n",
      "1Ô∏è‚É£ Basic Aggregations\n",
      "üìà Overall Sales Statistics:\n",
      "   ‚Ä¢ Total Transactions: 1,000\n",
      "   ‚Ä¢ Total Revenue: $2,116,080.84\n",
      "   ‚Ä¢ Average Transaction: $2116.08\n",
      "   ‚Ä¢ Min Transaction: $29.99\n",
      "   ‚Ä¢ Max Transaction: $12999.90\n",
      "   ‚Ä¢ Standard Deviation: $2621.50\n",
      "\n",
      "2Ô∏è‚É£ GroupBy Operations\n",
      "üìä Sales by Category:\n",
      "üìä Sales by Category:\n",
      "+-----------+-----------------+------------------+------------------+----------+\n",
      "|category   |transaction_count|total_revenue     |avg_amount        |max_amount|\n",
      "+-----------+-----------------+------------------+------------------+----------+\n",
      "|Electronics|526              |1597600.7100000002|3037.263707224335 |12999.9   |\n",
      "|Furniture  |275              |398261.0          |1448.2218181818182|4499.9    |\n",
      "|Appliances |199              |120219.12999999998|604.1162311557788 |1299.9    |\n",
      "+-----------+-----------------+------------------+------------------+----------+\n",
      "\n",
      "üåç Sales by Region and Category:\n",
      "+-----------+-----------------+------------------+------------------+----------+\n",
      "|category   |transaction_count|total_revenue     |avg_amount        |max_amount|\n",
      "+-----------+-----------------+------------------+------------------+----------+\n",
      "|Electronics|526              |1597600.7100000002|3037.263707224335 |12999.9   |\n",
      "|Furniture  |275              |398261.0          |1448.2218181818182|4499.9    |\n",
      "|Appliances |199              |120219.12999999998|604.1162311557788 |1299.9    |\n",
      "+-----------+-----------------+------------------+------------------+----------+\n",
      "\n",
      "üåç Sales by Region and Category:\n",
      "+-------+-----------+-----+------------------+------------------+\n",
      "|region |category   |count|revenue           |avg_rating        |\n",
      "+-------+-----------+-----+------------------+------------------+\n",
      "|Central|Appliances |40   |26027.65          |3.0075000000000003|\n",
      "|Central|Electronics|125  |398953.14         |3.0224000000000006|\n",
      "|Central|Furniture  |51   |75293.18000000001 |2.966666666666667 |\n",
      "|East   |Appliances |36   |21377.980000000003|2.980555555555555 |\n",
      "|East   |Electronics|93   |274295.20000000007|3.01505376344086  |\n",
      "|East   |Furniture  |46   |76669.20999999999 |3.178260869565217 |\n",
      "|North  |Appliances |41   |22257.98          |3.024390243902439 |\n",
      "|North  |Electronics|96   |279894.17000000004|3.013541666666667 |\n",
      "|North  |Furniture  |57   |73621.31          |2.770175438596491 |\n",
      "|South  |Appliances |39   |24757.840000000004|3.0205128205128204|\n",
      "+-------+-----------+-----+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "3Ô∏è‚É£ Advanced Aggregations\n",
      "üåé Advanced Regional Statistics:\n",
      "+-------+-----------+-----+------------------+------------------+\n",
      "|region |category   |count|revenue           |avg_rating        |\n",
      "+-------+-----------+-----+------------------+------------------+\n",
      "|Central|Appliances |40   |26027.65          |3.0075000000000003|\n",
      "|Central|Electronics|125  |398953.14         |3.0224000000000006|\n",
      "|Central|Furniture  |51   |75293.18000000001 |2.966666666666667 |\n",
      "|East   |Appliances |36   |21377.980000000003|2.980555555555555 |\n",
      "|East   |Electronics|93   |274295.20000000007|3.01505376344086  |\n",
      "|East   |Furniture  |46   |76669.20999999999 |3.178260869565217 |\n",
      "|North  |Appliances |41   |22257.98          |3.024390243902439 |\n",
      "|North  |Electronics|96   |279894.17000000004|3.013541666666667 |\n",
      "|North  |Furniture  |57   |73621.31          |2.770175438596491 |\n",
      "|South  |Appliances |39   |24757.840000000004|3.0205128205128204|\n",
      "+-------+-----------+-----+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "3Ô∏è‚É£ Advanced Aggregations\n",
      "üåé Advanced Regional Statistics:\n",
      "+-------+------------+------------------+------------------+----------+----------+---------------+-----------+------------------+\n",
      "|region |transactions|revenue           |avg_amount        |min_amount|max_amount|unique_products|unique_reps|avg_rating        |\n",
      "+-------+------------+------------------+------------------+----------+----------+---------------+-----------+------------------+\n",
      "|Central|216         |500273.97         |2316.0831944444444|29.99     |12999.9   |10             |5          |3.0064814814814813|\n",
      "|South  |214         |465918.32000000007|2177.1884112149537|29.99     |12999.9   |10             |5          |3.0378504672897195|\n",
      "|West   |201         |401772.70000000007|1998.8691542288561|29.99     |11699.91  |10             |5          |2.770149253731343 |\n",
      "|North  |194         |375773.4600000001 |1936.9765979381448|29.99     |12999.9   |10             |5          |2.9443298969072167|\n",
      "|East   |175         |372342.3900000001 |2127.6708000000003|29.99     |12999.9   |10             |5          |3.050857142857143 |\n",
      "+-------+------------+------------------+------------------+----------+----------+---------------+-----------+------------------+\n",
      "\n",
      "\n",
      "4Ô∏è‚É£ Conditional Aggregations\n",
      "üéØ Conditional Aggregations by Category:\n",
      "+-------+------------+------------------+------------------+----------+----------+---------------+-----------+------------------+\n",
      "|region |transactions|revenue           |avg_amount        |min_amount|max_amount|unique_products|unique_reps|avg_rating        |\n",
      "+-------+------------+------------------+------------------+----------+----------+---------------+-----------+------------------+\n",
      "|Central|216         |500273.97         |2316.0831944444444|29.99     |12999.9   |10             |5          |3.0064814814814813|\n",
      "|South  |214         |465918.32000000007|2177.1884112149537|29.99     |12999.9   |10             |5          |3.0378504672897195|\n",
      "|West   |201         |401772.70000000007|1998.8691542288561|29.99     |11699.91  |10             |5          |2.770149253731343 |\n",
      "|North  |194         |375773.4600000001 |1936.9765979381448|29.99     |12999.9   |10             |5          |2.9443298969072167|\n",
      "|East   |175         |372342.3900000001 |2127.6708000000003|29.99     |12999.9   |10             |5          |3.050857142857143 |\n",
      "+-------+------------+------------------+------------------+----------+----------+---------------+-----------+------------------+\n",
      "\n",
      "\n",
      "4Ô∏è‚É£ Conditional Aggregations\n",
      "üéØ Conditional Aggregations by Category:\n",
      "+-----------+-----------+----------------+------------------+------------------+------------+-------------+\n",
      "|category   |total_sales|high_value_sales|high_value_revenue|avg_high_rating   |online_sales|offline_sales|\n",
      "+-----------+-----------+----------------+------------------+------------------+------------+-------------+\n",
      "|Furniture  |275        |165             |368840.2200000001 |4.391549295774648 |127         |121          |\n",
      "|Electronics|526        |362             |1564838.35        |4.456818181818182 |226         |241          |\n",
      "|Appliances |199        |115             |97591.32          |4.4423076923076925|91          |90           |\n",
      "+-----------+-----------+----------------+------------------+------------------+------------+-------------+\n",
      "\n",
      "\n",
      "5Ô∏è‚É£ Date-based Aggregations\n",
      "üìÖ Monthly Sales Trends:\n",
      "+-----------+-----------+----------------+------------------+------------------+------------+-------------+\n",
      "|category   |total_sales|high_value_sales|high_value_revenue|avg_high_rating   |online_sales|offline_sales|\n",
      "+-----------+-----------+----------------+------------------+------------------+------------+-------------+\n",
      "|Furniture  |275        |165             |368840.2200000001 |4.391549295774648 |127         |121          |\n",
      "|Electronics|526        |362             |1564838.35        |4.456818181818182 |226         |241          |\n",
      "|Appliances |199        |115             |97591.32          |4.4423076923076925|91          |90           |\n",
      "+-----------+-----------+----------------+------------------+------------------+------------+-------------+\n",
      "\n",
      "\n",
      "5Ô∏è‚É£ Date-based Aggregations\n",
      "üìÖ Monthly Sales Trends:\n",
      "+----+-----+------------+------------------+------------------+----------------+\n",
      "|year|month|transactions|           revenue|   avg_transaction|unique_customers|\n",
      "+----+-----+------------+------------------+------------------+----------------+\n",
      "|2024|    1|          86|127617.54999999997|1483.9249999999997|               0|\n",
      "|2024|    2|          88|         208426.48|2368.4827272727275|               0|\n",
      "|2024|    3|          80|         176263.45|       2203.293125|               0|\n",
      "|2024|    4|          75|         161981.72|2159.7562666666668|               0|\n",
      "|2024|    5|          80|164323.83000000002|       2054.047875|               0|\n",
      "|2024|    6|          69|         141412.54|2049.4571014492753|               0|\n",
      "|2024|    7|          69|159292.11000000002|2308.5813043478265|               0|\n",
      "|2024|    8|         100|         259618.37|         2596.1837|               0|\n",
      "|2024|    9|          88|         161291.16| 1832.854090909091|               0|\n",
      "|2024|   10|          87|         172959.12|1988.0358620689656|               0|\n",
      "|2024|   11|          74|         152749.99| 2064.189054054054|               0|\n",
      "|2024|   12|         104|230144.52000000005|2212.9280769230772|               0|\n",
      "+----+-----+------------+------------------+------------------+----------------+\n",
      "\n",
      "\n",
      "6Ô∏è‚É£ Percentile and Quantile Aggregations\n",
      "üìä Percentile Analysis by Category:\n",
      "+----+-----+------------+------------------+------------------+----------------+\n",
      "|year|month|transactions|           revenue|   avg_transaction|unique_customers|\n",
      "+----+-----+------------+------------------+------------------+----------------+\n",
      "|2024|    1|          86|127617.54999999997|1483.9249999999997|               0|\n",
      "|2024|    2|          88|         208426.48|2368.4827272727275|               0|\n",
      "|2024|    3|          80|         176263.45|       2203.293125|               0|\n",
      "|2024|    4|          75|         161981.72|2159.7562666666668|               0|\n",
      "|2024|    5|          80|164323.83000000002|       2054.047875|               0|\n",
      "|2024|    6|          69|         141412.54|2049.4571014492753|               0|\n",
      "|2024|    7|          69|159292.11000000002|2308.5813043478265|               0|\n",
      "|2024|    8|         100|         259618.37|         2596.1837|               0|\n",
      "|2024|    9|          88|         161291.16| 1832.854090909091|               0|\n",
      "|2024|   10|          87|         172959.12|1988.0358620689656|               0|\n",
      "|2024|   11|          74|         152749.99| 2064.189054054054|               0|\n",
      "|2024|   12|         104|230144.52000000005|2212.9280769230772|               0|\n",
      "+----+-----+------------+------------------+------------------+----------------+\n",
      "\n",
      "\n",
      "6Ô∏è‚É£ Percentile and Quantile Aggregations\n",
      "üìä Percentile Analysis by Category:\n",
      "+-----------+-----+------------------+-----------------+------------------+----------+\n",
      "|category   |count|q1_amount         |median_amount    |q3_amount         |p95_amount|\n",
      "+-----------+-----+------------------+-----------------+------------------+----------+\n",
      "|Furniture  |275  |367.92            |899.98           |2399.92           |4049.91   |\n",
      "|Electronics|526  |299.9             |1799.97          |4799.9400000000005|9099.93   |\n",
      "|Appliances |199  |269.96999999999997|539.9399999999999|899.9             |1169.91   |\n",
      "+-----------+-----+------------------+-----------------+------------------+----------+\n",
      "\n",
      "\n",
      "7Ô∏è‚É£ Custom Aggregations\n",
      "üîß Custom Aggregations (Coefficient of Variation):\n",
      "+-------+------------+------------------+------------------+------------------------+\n",
      "|region |transactions|avg_amount        |std_amount        |coefficient_of_variation|\n",
      "+-------+------------+------------------+------------------+------------------------+\n",
      "|East   |175         |2127.6708000000003|2526.468419661877 |1.187433892339866       |\n",
      "|North  |194         |1936.9765979381448|2640.7034875634595|1.3633120247164636      |\n",
      "|West   |201         |1998.8691542288561|2530.0986647320265|1.2657650248788364      |\n",
      "|South  |214         |2177.1884112149537|2697.5290195251955|1.2389965910299292      |\n",
      "|Central|216         |2316.0831944444444|2693.884181920481 |1.1631206462627346      |\n",
      "+-------+------------+------------------+------------------+------------------------+\n",
      "\n",
      "\n",
      "8Ô∏è‚É£ Having Clauses (Post-aggregation Filtering)\n",
      "+-----------+-----+------------------+-----------------+------------------+----------+\n",
      "|category   |count|q1_amount         |median_amount    |q3_amount         |p95_amount|\n",
      "+-----------+-----+------------------+-----------------+------------------+----------+\n",
      "|Furniture  |275  |367.92            |899.98           |2399.92           |4049.91   |\n",
      "|Electronics|526  |299.9             |1799.97          |4799.9400000000005|9099.93   |\n",
      "|Appliances |199  |269.96999999999997|539.9399999999999|899.9             |1169.91   |\n",
      "+-----------+-----+------------------+-----------------+------------------+----------+\n",
      "\n",
      "\n",
      "7Ô∏è‚É£ Custom Aggregations\n",
      "üîß Custom Aggregations (Coefficient of Variation):\n",
      "+-------+------------+------------------+------------------+------------------------+\n",
      "|region |transactions|avg_amount        |std_amount        |coefficient_of_variation|\n",
      "+-------+------------+------------------+------------------+------------------------+\n",
      "|East   |175         |2127.6708000000003|2526.468419661877 |1.187433892339866       |\n",
      "|North  |194         |1936.9765979381448|2640.7034875634595|1.3633120247164636      |\n",
      "|West   |201         |1998.8691542288561|2530.0986647320265|1.2657650248788364      |\n",
      "|South  |214         |2177.1884112149537|2697.5290195251955|1.2389965910299292      |\n",
      "|Central|216         |2316.0831944444444|2693.884181920481 |1.1631206462627346      |\n",
      "+-------+------------+------------------+------------------+------------------------+\n",
      "\n",
      "\n",
      "8Ô∏è‚É£ Having Clauses (Post-aggregation Filtering)\n",
      "üí∞ High Revenue Categories (>$10,000):\n",
      "+-----------+-----------------+------------------+\n",
      "|   category|transaction_count|     total_revenue|\n",
      "+-----------+-----------------+------------------+\n",
      "|  Furniture|              275|          398261.0|\n",
      "|Electronics|              526|1597600.7100000002|\n",
      "| Appliances|              199|120219.12999999998|\n",
      "+-----------+-----------------+------------------+\n",
      "\n",
      "üìà High Volume Regions (‚â•150 transactions):\n",
      "üí∞ High Revenue Categories (>$10,000):\n",
      "+-----------+-----------------+------------------+\n",
      "|   category|transaction_count|     total_revenue|\n",
      "+-----------+-----------------+------------------+\n",
      "|  Furniture|              275|          398261.0|\n",
      "|Electronics|              526|1597600.7100000002|\n",
      "| Appliances|              199|120219.12999999998|\n",
      "+-----------+-----------------+------------------+\n",
      "\n",
      "üìà High Volume Regions (‚â•150 transactions):\n",
      "+-------+-----------------+------------------+\n",
      "| region|transaction_count|       avg_revenue|\n",
      "+-------+-----------------+------------------+\n",
      "|   East|              175|2127.6708000000003|\n",
      "|  North|              194|1936.9765979381448|\n",
      "|   West|              201|1998.8691542288561|\n",
      "|  South|              214|2177.1884112149537|\n",
      "|Central|              216|2316.0831944444444|\n",
      "+-------+-----------------+------------------+\n",
      "\n",
      "\n",
      "üìä Aggregation Best Practices:\n",
      "   ‚Ä¢ Use groupBy() for categorical analysis\n",
      "   ‚Ä¢ Apply agg() with multiple functions for efficiency\n",
      "   ‚Ä¢ Use when() for conditional aggregations\n",
      "   ‚Ä¢ Filter groups with having-style conditions after agg()\n",
      "   ‚Ä¢ Consider approximate functions (approx_*) for large datasets\n",
      "   ‚Ä¢ Combine with window functions for advanced analytics\n",
      "+-------+-----------------+------------------+\n",
      "| region|transaction_count|       avg_revenue|\n",
      "+-------+-----------------+------------------+\n",
      "|   East|              175|2127.6708000000003|\n",
      "|  North|              194|1936.9765979381448|\n",
      "|   West|              201|1998.8691542288561|\n",
      "|  South|              214|2177.1884112149537|\n",
      "|Central|              216|2316.0831944444444|\n",
      "+-------+-----------------+------------------+\n",
      "\n",
      "\n",
      "üìä Aggregation Best Practices:\n",
      "   ‚Ä¢ Use groupBy() for categorical analysis\n",
      "   ‚Ä¢ Apply agg() with multiple functions for efficiency\n",
      "   ‚Ä¢ Use when() for conditional aggregations\n",
      "   ‚Ä¢ Filter groups with having-style conditions after agg()\n",
      "   ‚Ä¢ Consider approximate functions (approx_*) for large datasets\n",
      "   ‚Ä¢ Combine with window functions for advanced analytics\n"
     ]
    }
   ],
   "source": [
    "# Aggregations & Grouping Operations\n",
    "print(\"üìä Aggregations & Grouping Operations\")\n",
    "print(\"=\" * 37)\n",
    "\n",
    "# 1. Basic Aggregations\n",
    "print(\"1Ô∏è‚É£ Basic Aggregations\")\n",
    "\n",
    "# Simple aggregations on entire dataset\n",
    "total_sales = df_sales.agg(\n",
    "    count(\"*\").alias(\"total_transactions\"),\n",
    "    sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "    avg(\"total_amount\").alias(\"avg_transaction\"),\n",
    "    min(\"total_amount\").alias(\"min_transaction\"),\n",
    "    max(\"total_amount\").alias(\"max_transaction\"),\n",
    "    stddev(\"total_amount\").alias(\"stddev_amount\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"üìà Overall Sales Statistics:\")\n",
    "print(f\"   ‚Ä¢ Total Transactions: {total_sales['total_transactions']:,}\")\n",
    "print(f\"   ‚Ä¢ Total Revenue: ${total_sales['total_revenue']:,.2f}\")\n",
    "print(f\"   ‚Ä¢ Average Transaction: ${total_sales['avg_transaction']:.2f}\")\n",
    "print(f\"   ‚Ä¢ Min Transaction: ${total_sales['min_transaction']:.2f}\")\n",
    "print(f\"   ‚Ä¢ Max Transaction: ${total_sales['max_transaction']:.2f}\")\n",
    "print(f\"   ‚Ä¢ Standard Deviation: ${total_sales['stddev_amount']:.2f}\")\n",
    "\n",
    "# 2. GroupBy Operations\n",
    "print(f\"\\n2Ô∏è‚É£ GroupBy Operations\")\n",
    "\n",
    "# Group by single column\n",
    "category_stats = df_sales.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"transaction_count\"),\n",
    "    sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "    avg(\"total_amount\").alias(\"avg_amount\"),\n",
    "    max(\"total_amount\").alias(\"max_amount\")\n",
    ").orderBy(desc(\"total_revenue\"))\n",
    "\n",
    "print(f\"üìä Sales by Category:\")\n",
    "category_stats.show(truncate=False)\n",
    "\n",
    "# Group by multiple columns\n",
    "region_category_stats = df_sales.groupBy(\"region\", \"category\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    sum(\"total_amount\").alias(\"revenue\"),\n",
    "    avg(\"rating\").alias(\"avg_rating\")\n",
    ").orderBy(\"region\", \"category\")\n",
    "\n",
    "print(f\"üåç Sales by Region and Category:\")\n",
    "region_category_stats.show(10, truncate=False)\n",
    "\n",
    "# 3. Advanced Aggregations\n",
    "print(f\"\\n3Ô∏è‚É£ Advanced Aggregations\")\n",
    "\n",
    "# Multiple aggregations with different functions\n",
    "advanced_stats = df_sales.groupBy(\"region\").agg(\n",
    "    count(\"*\").alias(\"transactions\"),\n",
    "    sum(\"total_amount\").alias(\"revenue\"),\n",
    "    avg(\"total_amount\").alias(\"avg_amount\"),\n",
    "    min(\"total_amount\").alias(\"min_amount\"),\n",
    "    max(\"total_amount\").alias(\"max_amount\"),\n",
    "    count_distinct(\"product_id\").alias(\"unique_products\"),\n",
    "    count_distinct(\"sales_rep\").alias(\"unique_reps\"),\n",
    "    avg(\"rating\").alias(\"avg_rating\"),\n",
    "    collect_list(\"product_name\").alias(\"all_products\")  # Collect all product names\n",
    ").orderBy(desc(\"revenue\"))\n",
    "\n",
    "print(f\"üåé Advanced Regional Statistics:\")\n",
    "# Show without collect_list to avoid clutter\n",
    "advanced_stats.drop(\"all_products\").show(truncate=False)\n",
    "\n",
    "# 4. Conditional Aggregations\n",
    "print(f\"\\n4Ô∏è‚É£ Conditional Aggregations\")\n",
    "\n",
    "# Aggregations with conditions using when()\n",
    "conditional_agg = df_sales.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"total_sales\"),\n",
    "    sum(when(col(\"total_amount\") >= 500, 1).otherwise(0)).alias(\"high_value_sales\"),\n",
    "    sum(when(col(\"total_amount\") >= 500, col(\"total_amount\")).otherwise(0)).alias(\"high_value_revenue\"),\n",
    "    avg(when(col(\"rating\") >= 4.0, col(\"rating\"))).alias(\"avg_high_rating\"),\n",
    "    count(when(col(\"is_online\") == True, 1)).alias(\"online_sales\"),\n",
    "    count(when(col(\"is_online\") == False, 1)).alias(\"offline_sales\")\n",
    ")\n",
    "\n",
    "print(f\"üéØ Conditional Aggregations by Category:\")\n",
    "conditional_agg.show(truncate=False)\n",
    "\n",
    "# 5. Date-based Aggregations\n",
    "print(f\"\\n5Ô∏è‚É£ Date-based Aggregations\")\n",
    "\n",
    "# Add date components for grouping\n",
    "df_with_dates = df_sales.withColumn(\"sale_date_typed\", to_date(col(\"sale_date\"), \"yyyy-MM-dd\")) \\\n",
    "                       .withColumn(\"year\", year(col(\"sale_date_typed\"))) \\\n",
    "                       .withColumn(\"month\", month(col(\"sale_date_typed\"))) \\\n",
    "                       .withColumn(\"quarter\", quarter(col(\"sale_date_typed\")))\n",
    "\n",
    "# Monthly sales trends\n",
    "monthly_trends = df_with_dates.groupBy(\"year\", \"month\").agg(\n",
    "    count(\"*\").alias(\"transactions\"),\n",
    "    sum(\"total_amount\").alias(\"revenue\"),\n",
    "    avg(\"total_amount\").alias(\"avg_transaction\"),\n",
    "    lit(0).alias(\"unique_customers\")  # Simplified since customer_id not in sales data\n",
    ").orderBy(\"year\", \"month\")\n",
    "\n",
    "print(f\"üìÖ Monthly Sales Trends:\")\n",
    "monthly_trends.show(12)\n",
    "\n",
    "# 6. Percentile and Quantile Aggregations\n",
    "print(f\"\\n6Ô∏è‚É£ Percentile and Quantile Aggregations\")\n",
    "\n",
    "# Calculate percentiles\n",
    "percentile_stats = df_sales.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    expr(\"percentile_approx(total_amount, 0.25)\").alias(\"q1_amount\"),\n",
    "    expr(\"percentile_approx(total_amount, 0.5)\").alias(\"median_amount\"),\n",
    "    expr(\"percentile_approx(total_amount, 0.75)\").alias(\"q3_amount\"),\n",
    "    expr(\"percentile_approx(total_amount, 0.95)\").alias(\"p95_amount\")\n",
    ")\n",
    "\n",
    "print(f\"üìä Percentile Analysis by Category:\")\n",
    "percentile_stats.show(truncate=False)\n",
    "\n",
    "# 7. Custom Aggregations\n",
    "print(f\"\\n7Ô∏è‚É£ Custom Aggregations\")\n",
    "\n",
    "# User-defined aggregation functions\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Define a custom function for coefficient of variation\n",
    "def coefficient_of_variation(values):\n",
    "    if len(values) <= 1:\n",
    "        return 0.0\n",
    "    mean_val = sum(values) / len(values)\n",
    "    variance = sum((x - mean_val) ** 2 for x in values) / (len(values) - 1)\n",
    "    std_dev = variance ** 0.5\n",
    "    return std_dev / mean_val if mean_val != 0 else 0.0\n",
    "\n",
    "# Register UDF\n",
    "cv_udf = udf(coefficient_of_variation, DoubleType())\n",
    "\n",
    "# Custom aggregation using collect_list and UDF\n",
    "custom_agg = df_sales.groupBy(\"region\").agg(\n",
    "    count(\"*\").alias(\"transactions\"),\n",
    "    avg(\"total_amount\").alias(\"avg_amount\"),\n",
    "    stddev(\"total_amount\").alias(\"std_amount\"),\n",
    "    (stddev(\"total_amount\") / avg(\"total_amount\")).alias(\"coefficient_of_variation\")\n",
    ")\n",
    "\n",
    "print(f\"üîß Custom Aggregations (Coefficient of Variation):\")\n",
    "custom_agg.show(truncate=False)\n",
    "\n",
    "# 8. Having Clauses (Post-aggregation Filtering)\n",
    "print(f\"\\n8Ô∏è‚É£ Having Clauses (Post-aggregation Filtering)\")\n",
    "\n",
    "# Filter groups after aggregation\n",
    "high_revenue_categories = df_sales.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"transaction_count\"),\n",
    "    sum(\"total_amount\").alias(\"total_revenue\")\n",
    ").filter(col(\"total_revenue\") > 10000)  # Having clause equivalent\n",
    "\n",
    "high_volume_regions = df_sales.groupBy(\"region\").agg(\n",
    "    count(\"*\").alias(\"transaction_count\"),\n",
    "    avg(\"total_amount\").alias(\"avg_revenue\")\n",
    ").filter(col(\"transaction_count\") >= 150)\n",
    "\n",
    "print(f\"üí∞ High Revenue Categories (>$10,000):\")\n",
    "high_revenue_categories.show()\n",
    "\n",
    "print(f\"üìà High Volume Regions (‚â•150 transactions):\")\n",
    "high_volume_regions.show()\n",
    "\n",
    "print(f\"\\nüìä Aggregation Best Practices:\")\n",
    "print(f\"   ‚Ä¢ Use groupBy() for categorical analysis\")\n",
    "print(f\"   ‚Ä¢ Apply agg() with multiple functions for efficiency\")\n",
    "print(f\"   ‚Ä¢ Use when() for conditional aggregations\")\n",
    "print(f\"   ‚Ä¢ Filter groups with having-style conditions after agg()\")\n",
    "print(f\"   ‚Ä¢ Consider approximate functions (approx_*) for large datasets\")\n",
    "print(f\"   ‚Ä¢ Combine with window functions for advanced analytics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84b09e2",
   "metadata": {},
   "source": [
    "## 3.6 Join Operations & Module Summary\n",
    "\n",
    "Join operations combine data from multiple DataFrames based on common keys. This section demonstrates various join types and concludes Module 3 with key takeaways.\n",
    "\n",
    "**Join Types in PySpark:**\n",
    "- **Inner Join**: Returns only matching records from both DataFrames\n",
    "- **Left Join**: Returns all records from left DataFrame, matching from right\n",
    "- **Right Join**: Returns all records from right DataFrame, matching from left\n",
    "- **Full Outer Join**: Returns all records from both DataFrames\n",
    "- **Cross Join**: Cartesian product of both DataFrames\n",
    "- **Anti Join**: Returns records from left that don't match right\n",
    "- **Semi Join**: Returns records from left that have matches in right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50007011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Join Operations & Module Summary\n",
      "===================================\n",
      "1Ô∏è‚É£ Creating Additional Datasets for Joins\n",
      "‚úÖ Created orders dataset: 6 records\n",
      "\n",
      "2Ô∏è‚É£ Inner Join (Only Matching Records)\n",
      "üìä Inner Join Result: 5 records\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|customer_id|first_name|last_name|order_id|order_amount|\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|C001       |John      |Doe      |T000001 |1299.99     |\n",
      "|C002       |Jane      |Smith    |T000002 |29.99       |\n",
      "|C001       |John      |Doe      |T000003 |299.99      |\n",
      "|C003       |Bob       |Johnson  |T000004 |89.99       |\n",
      "|C004       |Alice     |Brown    |T000005 |799.99      |\n",
      "+-----------+----------+---------+--------+------------+\n",
      "\n",
      "\n",
      "3Ô∏è‚É£ Left Join (All Customers, Matching Orders)\n",
      "üìä Inner Join Result: 5 records\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|customer_id|first_name|last_name|order_id|order_amount|\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|C001       |John      |Doe      |T000001 |1299.99     |\n",
      "|C002       |Jane      |Smith    |T000002 |29.99       |\n",
      "|C001       |John      |Doe      |T000003 |299.99      |\n",
      "|C003       |Bob       |Johnson  |T000004 |89.99       |\n",
      "|C004       |Alice     |Brown    |T000005 |799.99      |\n",
      "+-----------+----------+---------+--------+------------+\n",
      "\n",
      "\n",
      "3Ô∏è‚É£ Left Join (All Customers, Matching Orders)\n",
      "üìä Left Join Result: 11 records\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|customer_id|first_name|last_name|order_id|order_amount|\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|C001       |John      |Doe      |T000003 |299.99      |\n",
      "|C001       |John      |Doe      |T000001 |1299.99     |\n",
      "|C002       |Jane      |Smith    |T000002 |29.99       |\n",
      "|C003       |Bob       |Johnson  |T000004 |89.99       |\n",
      "|C004       |Alice     |Brown    |T000005 |799.99      |\n",
      "|C005       |Charlie   |Wilson   |NULL    |NULL        |\n",
      "|C006       |Diana     |Davis    |NULL    |NULL        |\n",
      "|C007       |Eve       |Miller   |NULL    |NULL        |\n",
      "|C008       |Frank     |Garcia   |NULL    |NULL        |\n",
      "|C009       |Grace     |Lee      |NULL    |NULL        |\n",
      "|C010       |Henry     |Taylor   |NULL    |NULL        |\n",
      "+-----------+----------+---------+--------+------------+\n",
      "\n",
      "\n",
      "4Ô∏è‚É£ Right Join (All Orders, Matching Customers)\n",
      "üìä Left Join Result: 11 records\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|customer_id|first_name|last_name|order_id|order_amount|\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|C001       |John      |Doe      |T000003 |299.99      |\n",
      "|C001       |John      |Doe      |T000001 |1299.99     |\n",
      "|C002       |Jane      |Smith    |T000002 |29.99       |\n",
      "|C003       |Bob       |Johnson  |T000004 |89.99       |\n",
      "|C004       |Alice     |Brown    |T000005 |799.99      |\n",
      "|C005       |Charlie   |Wilson   |NULL    |NULL        |\n",
      "|C006       |Diana     |Davis    |NULL    |NULL        |\n",
      "|C007       |Eve       |Miller   |NULL    |NULL        |\n",
      "|C008       |Frank     |Garcia   |NULL    |NULL        |\n",
      "|C009       |Grace     |Lee      |NULL    |NULL        |\n",
      "|C010       |Henry     |Taylor   |NULL    |NULL        |\n",
      "+-----------+----------+---------+--------+------------+\n",
      "\n",
      "\n",
      "4Ô∏è‚É£ Right Join (All Orders, Matching Customers)\n",
      "üìä Right Join Result: 6 records\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|customer_id|first_name|last_name|order_id|order_amount|\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|C001       |John      |Doe      |T000001 |1299.99     |\n",
      "|C002       |Jane      |Smith    |T000002 |29.99       |\n",
      "|C001       |John      |Doe      |T000003 |299.99      |\n",
      "|C003       |Bob       |Johnson  |T000004 |89.99       |\n",
      "|C004       |Alice     |Brown    |T000005 |799.99      |\n",
      "|C999       |NULL      |NULL     |T000006 |45.99       |\n",
      "+-----------+----------+---------+--------+------------+\n",
      "\n",
      "\n",
      "5Ô∏è‚É£ Full Outer Join (All Records from Both)\n",
      "üìä Right Join Result: 6 records\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|customer_id|first_name|last_name|order_id|order_amount|\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|C001       |John      |Doe      |T000001 |1299.99     |\n",
      "|C002       |Jane      |Smith    |T000002 |29.99       |\n",
      "|C001       |John      |Doe      |T000003 |299.99      |\n",
      "|C003       |Bob       |Johnson  |T000004 |89.99       |\n",
      "|C004       |Alice     |Brown    |T000005 |799.99      |\n",
      "|C999       |NULL      |NULL     |T000006 |45.99       |\n",
      "+-----------+----------+---------+--------+------------+\n",
      "\n",
      "\n",
      "5Ô∏è‚É£ Full Outer Join (All Records from Both)\n",
      "üìä Full Outer Join Result: 12 records\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|customer_id|first_name|last_name|order_id|order_amount|\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|C001       |John      |Doe      |T000001 |1299.99     |\n",
      "|C001       |John      |Doe      |T000003 |299.99      |\n",
      "|C002       |Jane      |Smith    |T000002 |29.99       |\n",
      "|C003       |Bob       |Johnson  |T000004 |89.99       |\n",
      "|C004       |Alice     |Brown    |T000005 |799.99      |\n",
      "|C005       |Charlie   |Wilson   |NULL    |NULL        |\n",
      "|C006       |Diana     |Davis    |NULL    |NULL        |\n",
      "|C007       |Eve       |Miller   |NULL    |NULL        |\n",
      "|C008       |Frank     |Garcia   |NULL    |NULL        |\n",
      "|C009       |Grace     |Lee      |NULL    |NULL        |\n",
      "|C010       |Henry     |Taylor   |NULL    |NULL        |\n",
      "|C999       |NULL      |NULL     |T000006 |45.99       |\n",
      "+-----------+----------+---------+--------+------------+\n",
      "\n",
      "\n",
      "6Ô∏è‚É£ Anti Join (Customers Without Orders)\n",
      "üìä Full Outer Join Result: 12 records\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|customer_id|first_name|last_name|order_id|order_amount|\n",
      "+-----------+----------+---------+--------+------------+\n",
      "|C001       |John      |Doe      |T000001 |1299.99     |\n",
      "|C001       |John      |Doe      |T000003 |299.99      |\n",
      "|C002       |Jane      |Smith    |T000002 |29.99       |\n",
      "|C003       |Bob       |Johnson  |T000004 |89.99       |\n",
      "|C004       |Alice     |Brown    |T000005 |799.99      |\n",
      "|C005       |Charlie   |Wilson   |NULL    |NULL        |\n",
      "|C006       |Diana     |Davis    |NULL    |NULL        |\n",
      "|C007       |Eve       |Miller   |NULL    |NULL        |\n",
      "|C008       |Frank     |Garcia   |NULL    |NULL        |\n",
      "|C009       |Grace     |Lee      |NULL    |NULL        |\n",
      "|C010       |Henry     |Taylor   |NULL    |NULL        |\n",
      "|C999       |NULL      |NULL     |T000006 |45.99       |\n",
      "+-----------+----------+---------+--------+------------+\n",
      "\n",
      "\n",
      "6Ô∏è‚É£ Anti Join (Customers Without Orders)\n",
      "üìä Anti Join Result: 6 records\n",
      "+-----------+----------+---------+------------------------+\n",
      "|customer_id|first_name|last_name|email                   |\n",
      "+-----------+----------+---------+------------------------+\n",
      "|C005       |Charlie   |Wilson   |charlie.wilson@email.com|\n",
      "|C006       |Diana     |Davis    |                        |\n",
      "|C007       |Eve       |Miller   |eve.miller@email.com    |\n",
      "|C008       |Frank     |Garcia   |frank.garcia@email.com  |\n",
      "|C009       |Grace     |Lee      |grace.lee@email.com     |\n",
      "|C010       |Henry     |Taylor   |henry.taylor@email.com  |\n",
      "+-----------+----------+---------+------------------------+\n",
      "\n",
      "\n",
      "7Ô∏è‚É£ Semi Join (Customers With Orders)\n",
      "üìä Anti Join Result: 6 records\n",
      "+-----------+----------+---------+------------------------+\n",
      "|customer_id|first_name|last_name|email                   |\n",
      "+-----------+----------+---------+------------------------+\n",
      "|C005       |Charlie   |Wilson   |charlie.wilson@email.com|\n",
      "|C006       |Diana     |Davis    |                        |\n",
      "|C007       |Eve       |Miller   |eve.miller@email.com    |\n",
      "|C008       |Frank     |Garcia   |frank.garcia@email.com  |\n",
      "|C009       |Grace     |Lee      |grace.lee@email.com     |\n",
      "|C010       |Henry     |Taylor   |henry.taylor@email.com  |\n",
      "+-----------+----------+---------+------------------------+\n",
      "\n",
      "\n",
      "7Ô∏è‚É£ Semi Join (Customers With Orders)\n",
      "üìä Semi Join Result: 4 records\n",
      "+-----------+----------+---------+---------------------+\n",
      "|customer_id|first_name|last_name|email                |\n",
      "+-----------+----------+---------+---------------------+\n",
      "|C001       |John      |Doe      |john.doe@email.com   |\n",
      "|C002       |Jane      |Smith    |jane.smith@email.com |\n",
      "|C003       |Bob       |Johnson  |bob.johnson@email.com|\n",
      "|C004       |Alice     |Brown    |alice.brown@email.com|\n",
      "+-----------+----------+---------+---------------------+\n",
      "\n",
      "\n",
      "8Ô∏è‚É£ Complex Join with Multiple Conditions\n",
      "üìä Semi Join Result: 4 records\n",
      "+-----------+----------+---------+---------------------+\n",
      "|customer_id|first_name|last_name|email                |\n",
      "+-----------+----------+---------+---------------------+\n",
      "|C001       |John      |Doe      |john.doe@email.com   |\n",
      "|C002       |Jane      |Smith    |jane.smith@email.com |\n",
      "|C003       |Bob       |Johnson  |bob.johnson@email.com|\n",
      "|C004       |Alice     |Brown    |alice.brown@email.com|\n",
      "+-----------+----------+---------+---------------------+\n",
      "\n",
      "\n",
      "8Ô∏è‚É£ Complex Join with Multiple Conditions\n",
      "üìä Sales-Products Join: 906 records\n",
      "+--------------+------------------+--------------------+------------------+-------------+-----------+--------------+\n",
      "|transaction_id|sales_product_name|catalog_product_name|total_amount      |catalog_price|category   |stock_quantity|\n",
      "+--------------+------------------+--------------------+------------------+-------------+-----------+--------------+\n",
      "|T000998       |Laptop Pro        |Laptop Pro          |7799.9400000000005|1299.99      |Electronics|50            |\n",
      "|T000989       |Laptop Pro        |Laptop Pro          |12999.9           |1299.99      |Electronics|50            |\n",
      "|T000978       |Laptop Pro        |Laptop Pro          |6499.95           |1299.99      |Electronics|50            |\n",
      "|T000968       |Laptop Pro        |Laptop Pro          |12999.9           |1299.99      |Electronics|50            |\n",
      "|T000960       |Laptop Pro        |Laptop Pro          |2599.98           |1299.99      |Electronics|50            |\n",
      "+--------------+------------------+--------------------+------------------+-------------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "9Ô∏è‚É£ Join Performance Analysis\n",
      "üìä Sales-Products Join: 906 records\n",
      "+--------------+------------------+--------------------+------------------+-------------+-----------+--------------+\n",
      "|transaction_id|sales_product_name|catalog_product_name|total_amount      |catalog_price|category   |stock_quantity|\n",
      "+--------------+------------------+--------------------+------------------+-------------+-----------+--------------+\n",
      "|T000998       |Laptop Pro        |Laptop Pro          |7799.9400000000005|1299.99      |Electronics|50            |\n",
      "|T000989       |Laptop Pro        |Laptop Pro          |12999.9           |1299.99      |Electronics|50            |\n",
      "|T000978       |Laptop Pro        |Laptop Pro          |6499.95           |1299.99      |Electronics|50            |\n",
      "|T000968       |Laptop Pro        |Laptop Pro          |12999.9           |1299.99      |Electronics|50            |\n",
      "|T000960       |Laptop Pro        |Laptop Pro          |2599.98           |1299.99      |Electronics|50            |\n",
      "+--------------+------------------+--------------------+------------------+-------------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "9Ô∏è‚É£ Join Performance Analysis\n",
      "‚ö° Join Performance:\n",
      "   ‚Ä¢ Join operation time: 0.1444s\n",
      "   ‚Ä¢ Records processed: 5\n",
      "   ‚Ä¢ Consider broadcast joins for small tables\n",
      "   ‚Ä¢ Use proper partitioning for large datasets\n",
      "\n",
      "üéØ Module 3 Complete: Data Transformations Mastery!\n",
      "=======================================================\n",
      "üìö What You've Learned:\n",
      "   ‚úÖ Basic Column Operations\n",
      "      ‚Ä¢ Selection, creation, modification, casting\n",
      "      ‚Ä¢ String, date, and conditional operations\n",
      "   ‚úÖ Filtering & Conditional Logic\n",
      "      ‚Ä¢ Row filtering with complex conditions\n",
      "      ‚Ä¢ Null handling and string pattern matching\n",
      "   ‚úÖ Aggregations & Grouping\n",
      "      ‚Ä¢ GroupBy operations and aggregate functions\n",
      "      ‚Ä¢ Conditional aggregations and percentiles\n",
      "   ‚úÖ Join Operations\n",
      "      ‚Ä¢ All join types: inner, left, right, full, anti, semi\n",
      "      ‚Ä¢ Complex join conditions and performance\n",
      "\n",
      "üí° Key Performance Insights:\n",
      "   ‚Ä¢ Narrow transformations (select, filter) are fast\n",
      "   ‚Ä¢ Wide transformations (groupBy, join) may shuffle data\n",
      "   ‚Ä¢ Use column references col() for optimization\n",
      "   ‚Ä¢ Apply filters early in transformation pipeline\n",
      "   ‚Ä¢ Leverage Catalyst optimizer for automatic optimization\n",
      "\n",
      "üöÄ Production Best Practices:\n",
      "   ‚Ä¢ Chain transformations for lazy evaluation\n",
      "   ‚Ä¢ Cache intermediate results when reused\n",
      "   ‚Ä¢ Use explicit schemas for better performance\n",
      "   ‚Ä¢ Monitor Spark UI for optimization opportunities\n",
      "   ‚Ä¢ Consider partitioning strategy for large datasets\n",
      "\n",
      "üìà Next Steps:\n",
      "   ‚Ä¢ Module 4: SQL & DataFrame API Advanced Patterns\n",
      "   ‚Ä¢ Module 5: Performance Optimization & Tuning\n",
      "   ‚Ä¢ Module 6: Machine Learning with MLlib\n",
      "   ‚Ä¢ Module 7: Streaming Data Processing\n",
      "\n",
      "üìä Module 3 Statistics:\n",
      "   ‚Ä¢ Transformation sections: 6\n",
      "   ‚Ä¢ Concepts demonstrated: 25+\n",
      "   ‚Ä¢ Sample datasets: 3 (sales, customers, products)\n",
      "   ‚Ä¢ Real-world patterns: Production-ready examples\n",
      "\n",
      "üéâ Congratulations! You've mastered PySpark Data Transformations!\n",
      "Ready to tackle complex data processing challenges! üöÄ\n",
      "‚ö° Join Performance:\n",
      "   ‚Ä¢ Join operation time: 0.1444s\n",
      "   ‚Ä¢ Records processed: 5\n",
      "   ‚Ä¢ Consider broadcast joins for small tables\n",
      "   ‚Ä¢ Use proper partitioning for large datasets\n",
      "\n",
      "üéØ Module 3 Complete: Data Transformations Mastery!\n",
      "=======================================================\n",
      "üìö What You've Learned:\n",
      "   ‚úÖ Basic Column Operations\n",
      "      ‚Ä¢ Selection, creation, modification, casting\n",
      "      ‚Ä¢ String, date, and conditional operations\n",
      "   ‚úÖ Filtering & Conditional Logic\n",
      "      ‚Ä¢ Row filtering with complex conditions\n",
      "      ‚Ä¢ Null handling and string pattern matching\n",
      "   ‚úÖ Aggregations & Grouping\n",
      "      ‚Ä¢ GroupBy operations and aggregate functions\n",
      "      ‚Ä¢ Conditional aggregations and percentiles\n",
      "   ‚úÖ Join Operations\n",
      "      ‚Ä¢ All join types: inner, left, right, full, anti, semi\n",
      "      ‚Ä¢ Complex join conditions and performance\n",
      "\n",
      "üí° Key Performance Insights:\n",
      "   ‚Ä¢ Narrow transformations (select, filter) are fast\n",
      "   ‚Ä¢ Wide transformations (groupBy, join) may shuffle data\n",
      "   ‚Ä¢ Use column references col() for optimization\n",
      "   ‚Ä¢ Apply filters early in transformation pipeline\n",
      "   ‚Ä¢ Leverage Catalyst optimizer for automatic optimization\n",
      "\n",
      "üöÄ Production Best Practices:\n",
      "   ‚Ä¢ Chain transformations for lazy evaluation\n",
      "   ‚Ä¢ Cache intermediate results when reused\n",
      "   ‚Ä¢ Use explicit schemas for better performance\n",
      "   ‚Ä¢ Monitor Spark UI for optimization opportunities\n",
      "   ‚Ä¢ Consider partitioning strategy for large datasets\n",
      "\n",
      "üìà Next Steps:\n",
      "   ‚Ä¢ Module 4: SQL & DataFrame API Advanced Patterns\n",
      "   ‚Ä¢ Module 5: Performance Optimization & Tuning\n",
      "   ‚Ä¢ Module 6: Machine Learning with MLlib\n",
      "   ‚Ä¢ Module 7: Streaming Data Processing\n",
      "\n",
      "üìä Module 3 Statistics:\n",
      "   ‚Ä¢ Transformation sections: 6\n",
      "   ‚Ä¢ Concepts demonstrated: 25+\n",
      "   ‚Ä¢ Sample datasets: 3 (sales, customers, products)\n",
      "   ‚Ä¢ Real-world patterns: Production-ready examples\n",
      "\n",
      "üéâ Congratulations! You've mastered PySpark Data Transformations!\n",
      "Ready to tackle complex data processing challenges! üöÄ\n"
     ]
    }
   ],
   "source": [
    "# Join Operations & Module Completion\n",
    "print(\"üîó Join Operations & Module Summary\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Create additional datasets for join demonstrations\n",
    "print(\"1Ô∏è‚É£ Creating Additional Datasets for Joins\")\n",
    "\n",
    "# Create a customer orders dataset\n",
    "customer_orders = [\n",
    "    (\"T000001\", \"C001\", \"2024-01-15\", 1299.99),\n",
    "    (\"T000002\", \"C002\", \"2024-01-16\", 29.99),\n",
    "    (\"T000003\", \"C001\", \"2024-01-17\", 299.99),\n",
    "    (\"T000004\", \"C003\", \"2024-01-18\", 89.99),\n",
    "    (\"T000005\", \"C004\", \"2024-01-19\", 799.99),\n",
    "    (\"T000006\", \"C999\", \"2024-01-20\", 45.99),  # Customer not in customer table\n",
    "]\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"order_amount\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df_orders = spark.createDataFrame(customer_orders, orders_schema)\n",
    "print(f\"‚úÖ Created orders dataset: {df_orders.count()} records\")\n",
    "\n",
    "# 2. Inner Join\n",
    "print(f\"\\n2Ô∏è‚É£ Inner Join (Only Matching Records)\")\n",
    "\n",
    "inner_join = df_customers.join(df_orders, \"customer_id\", \"inner\")\n",
    "print(f\"üìä Inner Join Result: {inner_join.count()} records\")\n",
    "inner_join.select(\"customer_id\", \"first_name\", \"last_name\", \"order_id\", \"order_amount\").show(truncate=False)\n",
    "\n",
    "# 3. Left Join\n",
    "print(f\"\\n3Ô∏è‚É£ Left Join (All Customers, Matching Orders)\")\n",
    "\n",
    "left_join = df_customers.join(df_orders, \"customer_id\", \"left\")\n",
    "print(f\"üìä Left Join Result: {left_join.count()} records\")\n",
    "left_join.select(\"customer_id\", \"first_name\", \"last_name\", \"order_id\", \"order_amount\").show(truncate=False)\n",
    "\n",
    "# 4. Right Join\n",
    "print(f\"\\n4Ô∏è‚É£ Right Join (All Orders, Matching Customers)\")\n",
    "\n",
    "right_join = df_customers.join(df_orders, \"customer_id\", \"right\")\n",
    "print(f\"üìä Right Join Result: {right_join.count()} records\")\n",
    "right_join.select(\"customer_id\", \"first_name\", \"last_name\", \"order_id\", \"order_amount\").show(truncate=False)\n",
    "\n",
    "# 5. Full Outer Join\n",
    "print(f\"\\n5Ô∏è‚É£ Full Outer Join (All Records from Both)\")\n",
    "\n",
    "full_join = df_customers.join(df_orders, \"customer_id\", \"full_outer\")\n",
    "print(f\"üìä Full Outer Join Result: {full_join.count()} records\")\n",
    "full_join.select(\"customer_id\", \"first_name\", \"last_name\", \"order_id\", \"order_amount\").show(truncate=False)\n",
    "\n",
    "# 6. Anti Join (Customers without Orders)\n",
    "print(f\"\\n6Ô∏è‚É£ Anti Join (Customers Without Orders)\")\n",
    "\n",
    "anti_join = df_customers.join(df_orders, \"customer_id\", \"left_anti\")\n",
    "print(f\"üìä Anti Join Result: {anti_join.count()} records\")\n",
    "anti_join.select(\"customer_id\", \"first_name\", \"last_name\", \"email\").show(truncate=False)\n",
    "\n",
    "# 7. Semi Join (Customers with Orders)\n",
    "print(f\"\\n7Ô∏è‚É£ Semi Join (Customers With Orders)\")\n",
    "\n",
    "semi_join = df_customers.join(df_orders, \"customer_id\", \"left_semi\")\n",
    "print(f\"üìä Semi Join Result: {semi_join.count()} records\")\n",
    "semi_join.select(\"customer_id\", \"first_name\", \"last_name\", \"email\").show(truncate=False)\n",
    "\n",
    "# 8. Complex Join with Multiple Conditions\n",
    "print(f\"\\n8Ô∏è‚É£ Complex Join with Multiple Conditions\")\n",
    "\n",
    "# Join sales with products for detailed analysis\n",
    "sales_products_join = df_sales.join(\n",
    "    df_products, \n",
    "    (df_sales.product_id == df_products.product_id) & (df_products.is_active == True),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    df_sales.transaction_id,\n",
    "    df_sales.product_name.alias(\"sales_product_name\"),\n",
    "    df_products.product_name.alias(\"catalog_product_name\"),\n",
    "    df_sales.total_amount,\n",
    "    df_products.price.alias(\"catalog_price\"),\n",
    "    df_products.category,\n",
    "    df_products.stock_quantity\n",
    ")\n",
    "\n",
    "print(f\"üìä Sales-Products Join: {sales_products_join.count()} records\")\n",
    "sales_products_join.show(5, truncate=False)\n",
    "\n",
    "# 9. Performance Considerations\n",
    "print(f\"\\n9Ô∏è‚É£ Join Performance Analysis\")\n",
    "\n",
    "# Measure join performance\n",
    "start_time = time.time()\n",
    "result_count = df_customers.join(df_orders, \"customer_id\", \"inner\").count()\n",
    "join_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚ö° Join Performance:\")\n",
    "print(f\"   ‚Ä¢ Join operation time: {join_time:.4f}s\")\n",
    "print(f\"   ‚Ä¢ Records processed: {result_count}\")\n",
    "print(f\"   ‚Ä¢ Consider broadcast joins for small tables\")\n",
    "print(f\"   ‚Ä¢ Use proper partitioning for large datasets\")\n",
    "\n",
    "print(f\"\\nüéØ Module 3 Complete: Data Transformations Mastery!\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "print(f\"üìö What You've Learned:\")\n",
    "print(f\"   ‚úÖ Basic Column Operations\")\n",
    "print(f\"      ‚Ä¢ Selection, creation, modification, casting\")\n",
    "print(f\"      ‚Ä¢ String, date, and conditional operations\")\n",
    "print(f\"   ‚úÖ Filtering & Conditional Logic\")\n",
    "print(f\"      ‚Ä¢ Row filtering with complex conditions\")\n",
    "print(f\"      ‚Ä¢ Null handling and string pattern matching\")\n",
    "print(f\"   ‚úÖ Aggregations & Grouping\")\n",
    "print(f\"      ‚Ä¢ GroupBy operations and aggregate functions\")\n",
    "print(f\"      ‚Ä¢ Conditional aggregations and percentiles\")\n",
    "print(f\"   ‚úÖ Join Operations\")\n",
    "print(f\"      ‚Ä¢ All join types: inner, left, right, full, anti, semi\")\n",
    "print(f\"      ‚Ä¢ Complex join conditions and performance\")\n",
    "\n",
    "print(f\"\\nüí° Key Performance Insights:\")\n",
    "print(f\"   ‚Ä¢ Narrow transformations (select, filter) are fast\")\n",
    "print(f\"   ‚Ä¢ Wide transformations (groupBy, join) may shuffle data\")\n",
    "print(f\"   ‚Ä¢ Use column references col() for optimization\")\n",
    "print(f\"   ‚Ä¢ Apply filters early in transformation pipeline\")\n",
    "print(f\"   ‚Ä¢ Leverage Catalyst optimizer for automatic optimization\")\n",
    "\n",
    "print(f\"\\nüöÄ Production Best Practices:\")\n",
    "print(f\"   ‚Ä¢ Chain transformations for lazy evaluation\")\n",
    "print(f\"   ‚Ä¢ Cache intermediate results when reused\")\n",
    "print(f\"   ‚Ä¢ Use explicit schemas for better performance\")\n",
    "print(f\"   ‚Ä¢ Monitor Spark UI for optimization opportunities\")\n",
    "print(f\"   ‚Ä¢ Consider partitioning strategy for large datasets\")\n",
    "\n",
    "print(f\"\\nüìà Next Steps:\")\n",
    "print(f\"   ‚Ä¢ Module 4: SQL & DataFrame API Advanced Patterns\")\n",
    "print(f\"   ‚Ä¢ Module 5: Performance Optimization & Tuning\")\n",
    "print(f\"   ‚Ä¢ Module 6: Machine Learning with MLlib\")\n",
    "print(f\"   ‚Ä¢ Module 7: Streaming Data Processing\")\n",
    "\n",
    "# Final statistics\n",
    "total_transformations = 6  # Sections covered\n",
    "total_concepts = 25  # Approximate concepts demonstrated\n",
    "\n",
    "print(f\"\\nüìä Module 3 Statistics:\")\n",
    "print(f\"   ‚Ä¢ Transformation sections: {total_transformations}\")\n",
    "print(f\"   ‚Ä¢ Concepts demonstrated: {total_concepts}+\")\n",
    "print(f\"   ‚Ä¢ Sample datasets: 3 (sales, customers, products)\")\n",
    "print(f\"   ‚Ä¢ Real-world patterns: Production-ready examples\")\n",
    "\n",
    "print(f\"\\nüéâ Congratulations! You've mastered PySpark Data Transformations!\")\n",
    "print(f\"Ready to tackle complex data processing challenges! üöÄ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
