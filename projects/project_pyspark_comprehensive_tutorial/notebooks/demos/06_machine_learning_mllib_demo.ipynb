{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae113fed",
   "metadata": {},
   "source": [
    "# Module 6: PySpark MLlib - Working Demo\n",
    "\n",
    "This is a simplified working demonstration of the Module 6 machine learning concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a317e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup PySpark MLlib Environment\n",
    "import time\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml.clustering import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.tuning import *\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark-MLlib-Demo\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f\"âœ… Spark MLlib ready! Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca644402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Demo Datasets\n",
    "print(\"Creating demo datasets...\")\n",
    "\n",
    "# Customer dataset for classification\n",
    "customer_df = spark.range(1, 1001) \\\n",
    "    .withColumnRenamed(\"id\", \"customer_id\") \\\n",
    "    .withColumn(\"age\", (rand(42) * 50 + 18).cast(\"int\")) \\\n",
    "    .withColumn(\"monthly_charges\", (rand(44) * 80 + 20).cast(\"decimal(8,2)\")) \\\n",
    "    .withColumn(\"contract_type\", when(rand(46) < 0.5, \"Month-to-month\").otherwise(\"One year\")) \\\n",
    "    .withColumn(\"churn\", when(rand(55) < 0.3, 1).otherwise(0))\n",
    "\n",
    "# Sales dataset for regression\n",
    "sales_df = spark.range(1, 1001) \\\n",
    "    .withColumnRenamed(\"id\", \"sale_id\") \\\n",
    "    .withColumn(\"store_id\", (rand(56) * 10 + 1).cast(\"int\")) \\\n",
    "    .withColumn(\"temperature\", (rand(59) * 40 + 30).cast(\"decimal(5,2)\")) \\\n",
    "    .withColumn(\"sales_amount\", (rand(66) * 1000 + 500).cast(\"decimal(10,2)\"))\n",
    "\n",
    "# Product dataset for clustering\n",
    "product_df = spark.range(1, 501) \\\n",
    "    .withColumnRenamed(\"id\", \"product_id\") \\\n",
    "    .withColumn(\"price\", (rand(67) * 500 + 10).cast(\"decimal(8,2)\")) \\\n",
    "    .withColumn(\"rating\", (rand(68) * 4 + 1).cast(\"decimal(3,2)\"))\n",
    "\n",
    "print(f\"âœ… Customer dataset: {customer_df.count()} records\")\n",
    "print(f\"âœ… Sales dataset: {sales_df.count()} records\")\n",
    "print(f\"âœ… Product dataset: {product_df.count()} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e62f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "print(\"Creating ML-ready feature datasets...\")\n",
    "\n",
    "# Customer features for classification\n",
    "customer_assembler = VectorAssembler(inputCols=[\"age\", \"monthly_charges\"], outputCol=\"features\")\n",
    "customers_features = customer_assembler.transform(customer_df).withColumnRenamed(\"churn\", \"churn_label\")\n",
    "customers_features.cache()\n",
    "\n",
    "# Sales features for regression\n",
    "sales_assembler = VectorAssembler(inputCols=[\"store_id\", \"temperature\"], outputCol=\"features\")\n",
    "sales_features = sales_assembler.transform(sales_df).withColumnRenamed(\"sales_amount\", \"total_amount\")\n",
    "sales_features.cache()\n",
    "\n",
    "# Product features for clustering\n",
    "products_assembler = VectorAssembler(inputCols=[\"price\", \"rating\"], outputCol=\"features\")\n",
    "products_features = products_assembler.transform(product_df)\n",
    "products_features.cache()\n",
    "\n",
    "print(f\"âœ… Customer features: {customers_features.count()} records\")\n",
    "print(f\"âœ… Sales features: {sales_features.count()} records\")\n",
    "print(f\"âœ… Product features: {products_features.count()} records\")\n",
    "print(\"\\nâœ… All ML datasets ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a03e36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Algorithms Demo\n",
    "print(\"ðŸ”¥ CLASSIFICATION ALGORITHMS DEMO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Prepare data\n",
    "train_data, test_data = customers_features.randomSplit([0.8, 0.2], seed=42)\n",
    "train_data.cache()\n",
    "test_data.cache()\n",
    "\n",
    "print(f\"Training set: {train_data.count()} records\")\n",
    "print(f\"Test set: {test_data.count()} records\")\n",
    "\n",
    "# Test multiple algorithms\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(featuresCol=\"features\", labelCol=\"churn_label\", maxIter=10),\n",
    "    \"Random Forest\": RandomForestClassifier(featuresCol=\"features\", labelCol=\"churn_label\", numTrees=10),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"churn_label\", maxDepth=5)\n",
    "}\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"churn_label\", rawPredictionCol=\"rawPrediction\")\n",
    "results = []\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    print(f\"\\nðŸŽ¯ Testing {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = classifier.fit(train_data)\n",
    "    predictions = model.transform(test_data)\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    results.append({\n",
    "        \"Algorithm\": name,\n",
    "        \"AUC\": round(auc, 4),\n",
    "        \"Training Time\": round(training_time, 2)\n",
    "    })\n",
    "    \n",
    "    print(f\"   AUC: {auc:.4f}\")\n",
    "    print(f\"   Training Time: {training_time:.2f}s\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\nðŸ“Š CLASSIFICATION RESULTS:\")\n",
    "results_df = spark.createDataFrame(results)\n",
    "results_df.show(truncate=False)\n",
    "\n",
    "best = max(results, key=lambda x: x[\"AUC\"])\n",
    "print(f\"ðŸ† Best performer: {best['Algorithm']} (AUC: {best['AUC']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f29def1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Algorithms Demo\n",
    "print(\"\\nðŸ”¥ REGRESSION ALGORITHMS DEMO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Prepare data\n",
    "sales_train, sales_test = sales_features.randomSplit([0.8, 0.2], seed=42)\n",
    "sales_train.cache()\n",
    "sales_test.cache()\n",
    "\n",
    "print(f\"Training set: {sales_train.count()} records\")\n",
    "print(f\"Test set: {sales_test.count()} records\")\n",
    "\n",
    "# Test multiple algorithms\n",
    "regressors = {\n",
    "    \"Linear Regression\": LinearRegression(featuresCol=\"features\", labelCol=\"total_amount\", maxIter=10),\n",
    "    \"Random Forest\": RandomForestRegressor(featuresCol=\"features\", labelCol=\"total_amount\", numTrees=10),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"total_amount\", maxDepth=5)\n",
    "}\n",
    "\n",
    "reg_evaluator = RegressionEvaluator(labelCol=\"total_amount\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "reg_results = []\n",
    "\n",
    "for name, regressor in regressors.items():\n",
    "    print(f\"\\nðŸŽ¯ Testing {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = regressor.fit(sales_train)\n",
    "    predictions = model.transform(sales_test)\n",
    "    rmse = reg_evaluator.evaluate(predictions)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    reg_results.append({\n",
    "        \"Algorithm\": name,\n",
    "        \"RMSE\": round(rmse, 2),\n",
    "        \"Training Time\": round(training_time, 2)\n",
    "    })\n",
    "    \n",
    "    print(f\"   RMSE: {rmse:.2f}\")\n",
    "    print(f\"   Training Time: {training_time:.2f}s\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\nðŸ“Š REGRESSION RESULTS:\")\n",
    "reg_results_df = spark.createDataFrame(reg_results)\n",
    "reg_results_df.show(truncate=False)\n",
    "\n",
    "best_reg = min(reg_results, key=lambda x: x[\"RMSE\"])\n",
    "print(f\"ðŸ† Best performer: {best_reg['Algorithm']} (RMSE: {best_reg['RMSE']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f109fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering Algorithms Demo\n",
    "print(\"\\nðŸ”¥ CLUSTERING ALGORITHMS DEMO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test multiple algorithms\n",
    "clusterers = {\n",
    "    \"K-Means\": KMeans(featuresCol=\"features\", k=3, seed=42),\n",
    "    \"Gaussian Mixture\": GaussianMixture(featuresCol=\"features\", k=3, seed=42)\n",
    "}\n",
    "\n",
    "cluster_evaluator = ClusteringEvaluator(predictionCol=\"prediction\", featuresCol=\"features\")\n",
    "cluster_results = []\n",
    "\n",
    "for name, clusterer in clusterers.items():\n",
    "    print(f\"\\nðŸŽ¯ Testing {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = clusterer.fit(products_features)\n",
    "    predictions = model.transform(products_features)\n",
    "    silhouette = cluster_evaluator.evaluate(predictions)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    cluster_results.append({\n",
    "        \"Algorithm\": name,\n",
    "        \"Silhouette Score\": round(silhouette, 4),\n",
    "        \"Training Time\": round(training_time, 2)\n",
    "    })\n",
    "    \n",
    "    print(f\"   Silhouette Score: {silhouette:.4f}\")\n",
    "    print(f\"   Training Time: {training_time:.2f}s\")\n",
    "    \n",
    "    # Show cluster distribution\n",
    "    cluster_counts = predictions.groupBy(\"prediction\").count().orderBy(\"prediction\")\n",
    "    print(\"   Cluster Distribution:\")\n",
    "    cluster_counts.show()\n",
    "\n",
    "# Summary\n",
    "print(\"\\nðŸ“Š CLUSTERING RESULTS:\")\n",
    "cluster_results_df = spark.createDataFrame(cluster_results)\n",
    "cluster_results_df.show(truncate=False)\n",
    "\n",
    "best_cluster = max(cluster_results, key=lambda x: x[\"Silhouette Score\"])\n",
    "print(f\"ðŸ† Best performer: {best_cluster['Algorithm']} (Silhouette: {best_cluster['Silhouette Score']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d1a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Pipeline Demo\n",
    "print(\"\\nðŸ”¥ ML PIPELINE DEMO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a complete pipeline\n",
    "indexer = StringIndexer(inputCol=\"contract_type\", outputCol=\"contract_indexed\")\n",
    "encoder = OneHotEncoder(inputCol=\"contract_indexed\", outputCol=\"contract_encoded\")\n",
    "assembler = VectorAssembler(inputCols=[\"age\", \"monthly_charges\", \"contract_encoded\"], outputCol=\"pipeline_features\")\n",
    "classifier = LogisticRegression(featuresCol=\"pipeline_features\", labelCol=\"churn\", maxIter=10)\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer, encoder, assembler, classifier])\n",
    "\n",
    "print(\"Pipeline stages:\")\n",
    "for i, stage in enumerate(pipeline.getStages()):\n",
    "    print(f\"  {i+1}. {type(stage).__name__}\")\n",
    "\n",
    "# Train pipeline\n",
    "pipeline_train, pipeline_test = customer_df.randomSplit([0.8, 0.2], seed=42)\n",
    "pipeline_model = pipeline.fit(pipeline_train)\n",
    "pipeline_predictions = pipeline_model.transform(pipeline_test)\n",
    "\n",
    "# Evaluate pipeline\n",
    "pipeline_evaluator = BinaryClassificationEvaluator(labelCol=\"churn\", rawPredictionCol=\"rawPrediction\")\n",
    "pipeline_auc = pipeline_evaluator.evaluate(pipeline_predictions)\n",
    "\n",
    "print(f\"\\nâœ… Pipeline AUC: {pipeline_auc:.4f}\")\n",
    "print(\"\\nðŸ“‹ Sample predictions:\")\n",
    "pipeline_predictions.select(\"customer_id\", \"age\", \"contract_type\", \"churn\", \"prediction\", \"probability\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59466307",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Module 6 Complete!\n",
    "\n",
    "### âœ… What We Demonstrated:\n",
    "\n",
    "1. **Feature Engineering**: Vector assembly and data preparation\n",
    "2. **Classification**: Logistic Regression, Random Forest, Decision Tree\n",
    "3. **Regression**: Linear Regression, Random Forest, Decision Tree\n",
    "4. **Clustering**: K-Means, Gaussian Mixture Model\n",
    "5. **ML Pipelines**: End-to-end workflow automation\n",
    "\n",
    "### ðŸš€ Key Takeaways:\n",
    "\n",
    "- **PySpark MLlib** provides comprehensive machine learning capabilities\n",
    "- **Feature engineering** is crucial for model performance\n",
    "- **Multiple algorithms** can be compared easily\n",
    "- **Pipelines** enable production-ready ML workflows\n",
    "- **Evaluation metrics** help select the best models\n",
    "\n",
    "### ðŸ“ˆ Next Steps:\n",
    "\n",
    "- **Model Tuning**: Hyperparameter optimization\n",
    "- **Feature Selection**: Advanced feature engineering\n",
    "- **Model Persistence**: Saving and loading models\n",
    "- **Production Deployment**: Real-world deployment strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
