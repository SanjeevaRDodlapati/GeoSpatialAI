# Spark Configuration for PySpark Tutorial
# This file contains common Spark configurations for the tutorial

# Application Settings
spark.app.name=PySpark-Comprehensive-Tutorial
spark.master=local[*]

# Memory Settings
spark.driver.memory=4g
spark.driver.maxResultSize=2g
spark.executor.memory=2g
spark.executor.memoryFraction=0.8

# Performance Settings
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.skewJoin.enabled=true
spark.sql.adaptive.localShuffleReader.enabled=true

# Arrow Settings (for pandas integration)
spark.sql.execution.arrow.pyspark.enabled=true
spark.sql.execution.arrow.maxRecordsPerBatch=10000

# Serialization
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.unsafe=true

# Networking
spark.network.timeout=800s
spark.sql.broadcastTimeout=36000

# Dynamic Allocation (for cluster mode)
spark.dynamicAllocation.enabled=false
spark.dynamicAllocation.minExecutors=1
spark.dynamicAllocation.maxExecutors=10

# Logging
spark.sql.adaptive.logLevel=WARN
spark.eventLog.enabled=false

# UI Settings
spark.ui.showConsoleProgress=true
spark.ui.port=4040

# Checkpointing
spark.sql.streaming.checkpointLocation=/tmp/spark-checkpoints

# Optimization
spark.sql.optimizer.dynamicPartitionPruning.enabled=true
spark.sql.cbo.enabled=true
spark.sql.cbo.joinReorder.enabled=true
