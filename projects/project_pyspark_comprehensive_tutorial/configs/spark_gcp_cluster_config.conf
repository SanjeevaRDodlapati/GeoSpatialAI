# Spark Configuration for Google Cloud HPC Cluster
# Optimized for large datasets (10GB+) and distributed processing

# Application Settings
spark.app.name=PySpark-Tutorial-GCP-Cluster

# Cluster Resource Settings (adjust based on your cluster)
spark.executor.instances=10
spark.executor.cores=4
spark.executor.memory=8g
spark.driver.memory=8g
spark.driver.maxResultSize=4g

# Memory Management
spark.executor.memoryFraction=0.8
spark.storage.memoryFraction=0.3
spark.shuffle.memoryFraction=0.2

# Performance Optimizations for Large Data
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.skewJoin.enabled=true
spark.sql.adaptive.localShuffleReader.enabled=true
spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes=256MB

# Dynamic Allocation
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=2
spark.dynamicAllocation.maxExecutors=20
spark.dynamicAllocation.initialExecutors=5

# Shuffle Optimization
spark.shuffle.service.enabled=true
spark.shuffle.compress=true
spark.shuffle.spill.compress=true
spark.sql.shuffle.partitions=400  # Increase for large datasets

# Network and I/O
spark.network.timeout=800s
spark.sql.broadcastTimeout=36000
spark.rpc.askTimeout=600s

# Google Cloud Specific
spark.hadoop.google.cloud.auth.service.account.enable=true
spark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem
spark.hadoop.fs.AbstractFileSystem.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS

# Checkpointing for fault tolerance
spark.sql.streaming.checkpointLocation=gs://your-bucket/checkpoints
spark.sql.adaptive.localShuffleReader.enabled=true

# Serialization
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.unsafe=true
spark.kryo.registrator=org.apache.spark.serializer.KryoRegistrator

# Monitoring and Logging
spark.eventLog.enabled=true
spark.eventLog.dir=gs://your-bucket/spark-events
spark.history.fs.logDirectory=gs://your-bucket/spark-events
