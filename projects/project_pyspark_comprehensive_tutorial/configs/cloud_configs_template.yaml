# Cloud Storage Configurations for PySpark Tutorial
# Copy this file to cloud_configs.yaml and update with your credentials

aws:
  access_key_id: YOUR_AWS_ACCESS_KEY_ID
  secret_access_key: YOUR_AWS_SECRET_ACCESS_KEY
  region: us-east-1
  bucket: your-tutorial-bucket
  
  # S3 Configuration
  s3:
    endpoint_url: null  # Use default AWS S3
    path_style_access: false
    
  # EMR Configuration
  emr:
    cluster_id: your-cluster-id
    region: us-east-1

azure:
  account_name: yourstorageaccount
  account_key: YOUR_AZURE_STORAGE_ACCOUNT_KEY
  container: tutorial-container
  
  # Alternative: SAS token
  sas_token: YOUR_SAS_TOKEN
  
  # HDInsight Configuration
  hdinsight:
    cluster_name: your-cluster-name
    resource_group: your-resource-group

gcp:
  project_id: your-project-id
  bucket: your-tutorial-bucket
  credentials_path: /path/to/service-account-key.json
  
  # Dataproc Configuration
  dataproc:
    cluster_name: your-cluster-name
    region: us-central1
    zone: us-central1-a

# Local development settings
local:
  data_path: ../data
  output_path: ../outputs
  temp_path: /tmp/spark-tutorial

# Kafka settings (for streaming)
kafka:
  bootstrap_servers: localhost:9092
  security_protocol: PLAINTEXT
  sasl_mechanism: null
  username: null
  password: null
